{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "from glob import glob\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#TEXT PROCESSING\n",
    "import nltk\n",
    "import re\n",
    "import codecs\n",
    "import unidecode\n",
    "#pip install unidecode\n",
    "import mpld3\n",
    "# pip install mpld3\n",
    "import stop_words\n",
    "# pip install stop-words\n",
    "from nltk import SnowballStemmer, pos_tag, word_tokenize, wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#SKLEARN\n",
    "from sklearn.feature_extraction.text import *\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Lecture des données **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#download cv in a list\n",
    "def load_cv_list(nombre):\n",
    "    path = '/Users/mehdiregina/FilRouge/Maha/data/Txt/'\n",
    "    liste_paths = [path+directory for directory in os.listdir(path)]\n",
    "    liste_cv = []\n",
    "    liste_files = []\n",
    "    for path in liste_paths:\n",
    "        filenames = sorted(glob(os.path.join(path,\"*.txt\")))\n",
    "        for file in filenames[:nombre]:\n",
    "            liste_cv.append(open(file).read())\n",
    "            liste_files.append(file)\n",
    "    return liste_cv,liste_files,liste_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"INFORMATICIEN DÉVELOPPEMENT ET\\nRÉSEAUX\\n\\nDéveloppeur Intégrateur Web\\n\\nÉragny (95) - Email me on Indeed: indeed.com/r/d7e8913ed00d0384\\n\\nAujourd’hui, je suis en recherche d'une opportunité sur un poste de développeur ou d’intégrateur web afin de\\nmonter toujours plus en compétence et d’approfondir les bases solide que j’ai acquis en formation.\\n\\nEXPÉRIENCE\\n\\nINFORMATICIEN DÉVELOPPEMENT ET RÉSEAUX\\n\\nLeGrandCercle95  -  Éragny (95) -\\n\\nnovembre 2016 - juin 2017\\n\\nInformaticien de l'entreprise, mes missions était de gérer les différent problème dans l'entreprise. Mise en\\nplace d'un antivirus serveur, sauvegarde Nas... Créé et gérer les droits sur l'Active Directory. Paramétrer des\\nclients léger ainsi que du matériel informatique comme des imprimantes ou des étiqueteuse en IP fixe.\\n\\nRéajustement du code html et css sur le site grand public selon les normes w3c.\\nCréation d'une source ODBC\\nCréation d'un code en php - sql afin de récupéré des données librairie sur un site fournisseur pour les enregistré\\nen fiche xml.\\nCréation de bannières pour les différents évènements avec photoshop et formation d'une personne sur place\\nau logiciel photoshop.\\n\\nCHARGE DE CLIENTELE\\n\\nEuropcar France Commercial  -  Saint-Ouen-l'Aumône (95) -\\n\\nnovembre 2008 - novembre 2016\\n\\nMes missions principales : \\n\\n- Qualité de service :\\nAssurer l'accueil des clients et le respect de la charte en agence.\\n\\n- Gestion des clients :\\nTraiter l'ensemble des appels téléphoniques.\\nAnalyser les besoins du client.\\nAssurer le suivi de la clientèle.\\n\\n- Logistique et Administratif :\\nS'assurer de la disponibilité des véhicules.\\nGérer l'administratif de l'agence.\\nGestion de la caisse.\\n\\n- Polyvalence\\n\\n\\x0cFormation Développeur Intégrateur Web\\n\\n-\\n\\nseptembre 2015 - juin 2016\\n\\nC.I.F)\\nCentre de formation IFOCOP (8mois)\\n\\n- Création de différents sites. (portfolio)\\n- Présentation d'un site e-commerce créé de A à Z pour l'examen\\n\\nDéveloppeur Web (stage)\\n\\nLe Club des formateurs  -  Éragny (95) -\\n\\nfévrier 2016 - mai 2016\\n\\nDéveloppement d'un site sous Wordpress de A à Z.\\n\\nCommercial stagiaire\\n\\nLAMY Assurance -\\n\\nnovembre 2007 - décembre 2007\\n\\nstagiaire\\n\\nPort Marly accessoires  -  Le Port-Marly (78) -\\n\\nmai 2007 - juin 2007\\n\\nLes Galeries Lafayette Vendeur\\n\\n-\\n\\n2005 - 2006\\n\\nconfection homme) Job étudiant\\n\\nBrice Vendeur\\n\\nPAP Job -\\n\\n2004 - 2004\\n\\nétudiant\\n\\nFORMATION\\n\\nniveau II bac+3 bac+4 en Développeur intégrateur web\\n\\nifocop  -  Éragny (95)\\n\\n2015 - 2016\\n\\n\\x0c\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "liste_cv,liste_files,liste_paths = load_cv_list(200)\n",
    "liste_cv[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2200 cvs\n"
     ]
    }
   ],
   "source": [
    "print(len(liste_cv), \"cvs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Suppression des sauts de ligne et text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string,re\n",
    "regex = re.compile('[%s]' % '(\\\\n)*(\\\\x0c)*')\n",
    "def del_line_feed(s):  \n",
    "    \"\"\"Delete \\n in the text\"\"\"\n",
    "    return regex.sub(' ', s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"informaticien développement et réseaux  développeur intégrateur web  éragny  95  - email me on indeed: indeed.com/r/d7e8913ed00d0384  aujourd’hui, je suis en recherche d'une opportunité sur un poste de développeur ou d’intégrateur web afin de monter toujours plus en compétence et d’approfondir les bases solide que j’ai acquis en formation.  expérience  informaticien développement et réseaux  legrandcercle95  -  éragny  95  -  novembre 2016 - juin 2017  informaticien de l'entreprise, mes missions était de gérer les différent problème dans l'entreprise. mise en place d'un antivirus serveur, sauvegarde nas... créé et gérer les droits sur l'active directory. paramétrer des clients léger ainsi que du matériel informatique comme des imprimantes ou des étiqueteuse en ip fixe.  réajustement du code html et css sur le site grand public selon les normes w3c. création d'une source odbc création d'un code en php - sql afin de récupéré des données librairie sur un site fournisseur pour les enregistré en fiche xml. création de bannières pour les différents évènements avec photoshop et formation d'une personne sur place au logiciel photoshop.  charge de clientele  europcar france commercial  -  saint-ouen-l'aumône  95  -  novembre 2008 - novembre 2016  mes missions principales :   - qualité de service : assurer l'accueil des clients et le respect de la charte en agence.  - gestion des clients : traiter l'ensemble des appels téléphoniques. analyser les besoins du client. assurer le suivi de la clientèle.  - logistique et administratif : s'assurer de la disponibilité des véhicules. gérer l'administratif de l'agence. gestion de la caisse.  - polyvalence   formation développeur intégrateur web  -  septembre 2015 - juin 2016  c.i.f  centre de formation ifocop  8mois   - création de différents sites.  portfolio  - présentation d'un site e-commerce créé de a à z pour l'examen  développeur web  stage   le club des formateurs  -  éragny  95  -  février 2016 - mai 2016  développement d'un site sous wordpress de a à z.  commercial stagiaire  lamy assurance -  novembre 2007 - décembre 2007  stagiaire  port marly accessoires  -  le port-marly  78  -  mai 2007 - juin 2007  les galeries lafayette vendeur  -  2005 - 2006  confection homme  job étudiant  brice vendeur  pap job -  2004 - 2004  étudiant  formation  niveau ii bac+3 bac+4 en développeur intégrateur web  ifocop  -  éragny  95   2015 - 2016   \""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "liste_cv = [del_line_feed(text).lower() for text in liste_cv]\n",
    "liste_cv[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Suppression ponctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#le maintient de la ponctuation pertube le stop words, apostrophe gérée dans text_treatment\n",
    "regex = re.compile('[%s]' % re.escape('!\"#$%&\\()*+,-./:;<=>?@[\\\\]^_{|}~')) \n",
    "\n",
    "def del_punct(s):  \n",
    "    \"\"\"Delete punctuation in the text\"\"\"\n",
    "    return regex.sub('', s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"informaticien développement et réseaux  développeur intégrateur web  éragny  95   email me on indeed indeedcomrd7e8913ed00d0384  aujourd’hui je suis en recherche d'une opportunité sur un poste de développeur ou d’intégrateur web afin de monter toujours plus en compétence et d’approfondir les bases solide que j’ai acquis en formation  expérience  informaticien développement et réseaux  legrandcercle95    éragny  95    novembre 2016  juin 2017  informaticien de l'entreprise mes missions était de gérer les différent problème dans l'entreprise mise en place d'un antivirus serveur sauvegarde nas créé et gérer les droits sur l'active directory paramétrer des clients léger ainsi que du matériel informatique comme des imprimantes ou des étiqueteuse en ip fixe  réajustement du code html et css sur le site grand public selon les normes w3c création d'une source odbc création d'un code en php  sql afin de récupéré des données librairie sur un site fournisseur pour les enregistré en fiche xml création de bannières pour les différents évènements avec photoshop et formation d'une personne sur place au logiciel photoshop  charge de clientele  europcar france commercial    saintouenl'aumône  95    novembre 2008  novembre 2016  mes missions principales     qualité de service  assurer l'accueil des clients et le respect de la charte en agence   gestion des clients  traiter l'ensemble des appels téléphoniques analyser les besoins du client assurer le suivi de la clientèle   logistique et administratif  s'assurer de la disponibilité des véhicules gérer l'administratif de l'agence gestion de la caisse   polyvalence   formation développeur intégrateur web    septembre 2015  juin 2016  cif  centre de formation ifocop  8mois    création de différents sites  portfolio   présentation d'un site ecommerce créé de a à z pour l'examen  développeur web  stage   le club des formateurs    éragny  95    février 2016  mai 2016  développement d'un site sous wordpress de a à z  commercial stagiaire  lamy assurance   novembre 2007  décembre 2007  stagiaire  port marly accessoires    le portmarly  78    mai 2007  juin 2007  les galeries lafayette vendeur    2005  2006  confection homme  job étudiant  brice vendeur  pap job   2004  2004  étudiant  formation  niveau ii bac3 bac4 en développeur intégrateur web  ifocop    éragny  95   2015  2016   \""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test \n",
    "liste_cv_no_punc = [del_punct(text) for text in liste_cv]\n",
    "liste_cv_no_punc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reconnaissance langage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _calculate_languages_ratios(text):\n",
    "    \"\"\"\n",
    "    Calculate probability of given text to be written in several languages and\n",
    "    return a dictionary that looks like {'french': 2, 'spanish': 4, 'english': 0}\n",
    "    \"\"\"\n",
    "\n",
    "    languages_ratios = {}\n",
    "\n",
    "    '''\n",
    "    nltk.wordpunct_tokenize() splits all punctuations into separate tokens\n",
    "    \n",
    "    >>> wordpunct_tokenize(\"That's thirty minutes away. I'll be there in ten.\")\n",
    "    ['That', \"'\", 's', 'thirty', 'minutes', 'away', '.', 'I', \"'\", 'll', 'be', 'there', 'in', 'ten', '.']\n",
    "    '''\n",
    "\n",
    "    tokens = wordpunct_tokenize(text)\n",
    "    words = [word.lower() for word in tokens] #from text get list of word in minuscule\n",
    "\n",
    "    \n",
    "    for language in stopwords.fileids(): # pour chaque langue\n",
    "        stopwords_set = set(stopwords.words(language)) #je mets les stop words du langage dans un set\n",
    "        words_set = set(words) #je mets les mots de mon texte dans un set\n",
    "        #je prends l'intersection entre les mots de mon texte et les mots du stopwords dans le langage donné\n",
    "        common_elements = words_set & stopwords_set\n",
    "        \n",
    "        #je compute mon score comme le nombre d'éléments en communs dictionnaire [langage : score]\n",
    "        languages_ratios[language] = len(common_elements) # language \"score\"\n",
    "\n",
    "    return languages_ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/mehdiregina/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['arabic',\n",
       " 'azerbaijani',\n",
       " 'danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'greek',\n",
       " 'hungarian',\n",
       " 'indonesian',\n",
       " 'italian',\n",
       " 'kazakh',\n",
       " 'nepali',\n",
       " 'norwegian',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'spanish',\n",
       " 'swedish',\n",
       " 'turkish']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "stopwords.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cv_langue(liste_cv, liste_filenames,language) :\n",
    "    \"\"\"Return resume witten in the specified language in parameter\"\"\"\n",
    "    liste_2 = []\n",
    "    listeFileNames = []\n",
    "    i=0\n",
    "    for cv in liste_cv :\n",
    "        if max(_calculate_languages_ratios(cv),key =_calculate_languages_ratios(cv).get)=='french':\n",
    "            liste_2.append(cv)\n",
    "            listeFileNames.append(liste_filenames[i])\n",
    "        i+=1\n",
    "    return liste_2,listeFileNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proportion cv french : 0.730909090909091\n"
     ]
    }
   ],
   "source": [
    "liste_cv_fr, liste_fileNames_fr = get_cv_langue(liste_cv_no_punc,liste_files,'french')\n",
    "\n",
    "nb_cv = len(liste_cv_no_punc)\n",
    "nb_cv_fr = len(liste_cv_fr)\n",
    "\n",
    "print(\"proportion cv french :\",1- ((nb_cv-nb_cv_fr)/nb_cv))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nombre de CV français: 1608\n"
     ]
    }
   ],
   "source": [
    "nb_cv_fr = len(liste_cv_fr)\n",
    "\n",
    "print('nombre de CV français:', len(liste_fileNames_fr))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** text treatment **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_treatment (text):\n",
    "    text = text.lower()\n",
    "    text = text.replace(\"\\x00\", '').replace(\"\\x01\", '').replace(\"\\x02\", '').replace(\"\\x03\", '') \\\n",
    "    .replace(\"\\x04\", '').replace(\"\\x05\", '').replace(\"\\x06\", '').replace(\"\\x07\", '').replace(\"\\x08\", '') \\\n",
    "    .replace(\"\\x0e\", '').replace(\"\\x11\", '').replace(\"\\x12\", '').replace(\"\\x10\", '').replace(\"\\x19\", '') \\\n",
    "    .replace(\"\\x1b\", '').replace(\"\\x14\", '').replace(\"\\x15\", '').replace('/', ' ').replace('=', ' ').replace(\"〓\", \"\") \\\n",
    "    .replace(\"»\", \"\").replace(\"«\", \"\").replace(\"¬\", \"\").replace('`', '').replace (\" -\", \" \").replace(\"•\", \"\")\\\n",
    "    .replace(\"l'\", \"\").replace(\"l’\", \"\").replace(\"l´\", \"\").replace(\"d’\", \"\").replace(\"d'\", \"\").replace(\"d´\",\"\")\\\n",
    "    .replace(\"j’\", \"\").replace(\"j'\", \"\").replace(\"j´\",\"\").replace(\"n’\", \"\").replace(\"n'\", \"\").replace(\"n´\",\"\")\\\n",
    "    .replace(\"”\", \"\").replace(\"~\", \"\").replace(\"§\", \"\").replace(\"¨\", \"\").replace(\"©\", \"\").replace(\"›\", \"\")\\\n",
    "    .replace(\"₋\", \"\").replace(\"→\", \"\").replace(\"⇨\", \"\").replace(\"∎\", \"\").replace(\"√\", \"\").replace(\"□\", \"\")\\\n",
    "    .replace(\"*\", \"\").replace(\"&\", \"\").replace(\"►\", \"\").replace(\"◊\", \"\").replace(\"☞\", \"\").replace(\"#\", \"\")\\\n",
    "    .replace(\"%\", \"\").replace(\"❖\", \"\").replace(\"➠\", \"\").replace(\"➢\", \"\").replace(\"\", \"\").replace(\"✓\", \"\") \\\n",
    "    .replace(\"√\", \"\").replace(\"✔\", \"\").replace(\"♦\", \"\").replace(\"◦\", \"\").replace(\"●\", \"\").replace(\"▫\", \"\")\\\n",
    "    .replace(\"▪\", \"\").replace(\"…\", \"\").replace(\"þ\", \"\").replace(\"®\", \"\").replace('', '').replace(\"...\", \"\")\n",
    "    text = unidecode.unidecode(text) # remove accent\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"informaticien developpement et reseaux  developpeur integrateur web  eragny  95   email me on indeed indeedcomrd7e8913ed00d0384  aujourhui je suis en recherche une opportunite sur un poste de developpeur ou integrateur web afin de monter toujours plus en competence et approfondir les bases solide que ai acquis en formation  experience  informaticien developpement et reseaux  legrandcercle95    eragny  95    novembre 2016  juin 2017  informaticien de entreprise mes missions etait de gerer les different probleme dans entreprise mise en place un antivirus serveur sauvegarde nas cree et gerer les droits sur active directory parametrer des clients leger ainsi que du materiel informatique comme des imprimantes ou des etiqueteuse en ip fixe  reajustement du code html et css sur le site grand public selon les normes w3c creation une source odbc creation un code en php  sql afin de recupere des donnees librairie sur un site fournisseur pour les enregistre en fiche xml creation de bannieres pour les differents evenements avec photoshop et formation une personne sur place au logiciel photoshop  charge de clientele  europcar france commercial    saintouenaumone  95    novembre 2008  novembre 2016  mes missions principales     qualite de service  assurer accueil des clients et le respect de la charte en agence   gestion des clients  traiter ensemble des appels telephoniques analyser les besoins du client assurer le suivi de la clientele   logistique et administratif  s'assurer de la disponibilite des vehicules gerer administratif de agence gestion de la caisse   polyvalence   formation developpeur integrateur web    septembre 2015  juin 2016  cif  centre de formation ifocop  8mois    creation de differents sites  portfolio   presentation un site ecommerce cree de a a z pour examen  developpeur web  stage   le club des formateurs    eragny  95    fevrier 2016  mai 2016  developpement un site sous wordpress de a a z  commercial stagiaire  lamy assurance   novembre 2007  decembre 2007  stagiaire  port marly accessoires    le portmarly  78    mai 2007  juin 2007  les galeries lafayette vendeur    2005  2006  confection homme  job etudiant  brice vendeur  pap job   2004  2004  etudiant  formation  niveau ii bac3 bac4 en developpeur integrateur web  ifocop    eragny  95   2015  2016   \""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#On supprime les caractères étranges et accents\n",
    "liste_cv_treated = [text_treatment(text) for text in liste_cv_fr]\n",
    "liste_cv_treated[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gestion des stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taille stop words liste :  368\n"
     ]
    }
   ],
   "source": [
    "#generate stopwords\n",
    "stop_words_py = set(stop_words.get_stop_words('french'))\n",
    "\n",
    "# attention certains stop words pourraient être utiles par la suite\n",
    "stopwords_set_manuel = set([\"an\", \"ans\", 'les', 'moins', 'd\\'un','janvier', 'fevrier', 'février', 'mars', 'avril', \\\n",
    "                 'mai', 'juin', 'juillet', 'aout', 'août', 'septembre', 'octobre', 'novembre', 'décembre', \\\n",
    "                  'decembre', 'moins', 'mise', 'universit\\xc3\\xa9', 'université', 'universite', 'ion','sage', \\\n",
    "                  'o', 'rac', 'vers', 'via', 'p\\xc3\\xa9rim\\xc3\\xa8tre', 'périmètre','et','paris','x',\"\\x00\",\\\n",
    "                          \"\\x01\",\"\\x02\", \"\\x03\",\"\\x04\",\"\\x05\",\"\\x06\",\"\\x07\",\"\\x08\",\"\\x09\",\"\\x0e\",\"\\x0e\",\"\\x11\",\\\n",
    "                           \"\\x12\",\"\\x13\",\"\\x14\",\"\\x15\",\"\\x16\",\"\\x17\",\"\\x18\",\"\\x19\",\"transport\",\"puis\",\"lieu\",\\\n",
    "                           \"adresse\",\"entre\",'dun','dune','chez','boulognebillancourt','bt','etc','recrutement','main',\\\n",
    "                           'and', 'paie','paiement','environ','place','france','paris','mois','mobile','mobiles',\\\n",
    "                           'nanterre','source','sources','concerne','concernant','of','non','notes','rh','minimum',\\\n",
    "                           'maximum','bac','site','sites','actuellement','telephone','telephonique','telephoniques','ca','demenager',\\\n",
    "                           'demenagement','participer','participation','lycee','baccalaureat','lien','liens','in',\\\n",
    "                           'indeed','email','indeedcomrd7e8913ed00d0384','aujourhui','afin','toujours','enterprise',\\\n",
    "                           \"guide\",\"10g\",\"11g\",\"9i\",'ad','v10','v2','v3','v5','v6','v8','v9',])\n",
    "stop_words_main = stop_words_py | stopwords_set_manuel\n",
    "stop_words_main = list(stop_words_main)\n",
    "print(\"taille stop words liste : \", len(stop_words_main))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['leur',\n",
       " 'v5',\n",
       " 'lien',\n",
       " 'fussions',\n",
       " 'eussiez',\n",
       " 'telephone',\n",
       " 'eussions',\n",
       " 'fut',\n",
       " 'site',\n",
       " 'toujours',\n",
       " 'mois',\n",
       " 'étaient',\n",
       " 'était',\n",
       " 'on',\n",
       " 'fûmes',\n",
       " 'mais',\n",
       " 'quand',\n",
       " '9i',\n",
       " 'même',\n",
       " 'hors',\n",
       " 'février',\n",
       " 'votre',\n",
       " 'pour',\n",
       " 'adresse',\n",
       " 'été',\n",
       " 'v8',\n",
       " 'eûtes',\n",
       " 'si',\n",
       " 'seras',\n",
       " 'es',\n",
       " 'aie',\n",
       " 'sans',\n",
       " 'peu',\n",
       " 'v10',\n",
       " 'v6',\n",
       " 'eût',\n",
       " 'baccalaureat',\n",
       " 'droite',\n",
       " 'france',\n",
       " 'dois',\n",
       " '\\x17',\n",
       " '\\x18',\n",
       " 'vous',\n",
       " 'serait',\n",
       " 'avril',\n",
       " 'aient',\n",
       " 'eûmes',\n",
       " 'pas',\n",
       " 'universitÃ©',\n",
       " 'seraient',\n",
       " 'que',\n",
       " 'valeur',\n",
       " 'le',\n",
       " 'quel',\n",
       " 'depuis',\n",
       " 'enterprise',\n",
       " 'pourquoi',\n",
       " 'devront',\n",
       " 'd',\n",
       " 'vos',\n",
       " 'ca',\n",
       " 'tous',\n",
       " 'transport',\n",
       " 'aurais',\n",
       " 'afin',\n",
       " 'peut',\n",
       " 'personne',\n",
       " 'les',\n",
       " '\\x01',\n",
       " 'ayez',\n",
       " 'a',\n",
       " 'rac',\n",
       " 'and',\n",
       " 'fais',\n",
       " 'ta',\n",
       " 'mobile',\n",
       " 'chaque',\n",
       " '\\x16',\n",
       " 'juillet',\n",
       " 'par',\n",
       " 'personnes',\n",
       " 'étions',\n",
       " 'v9',\n",
       " 'actuellement',\n",
       " 'sources',\n",
       " 'aies',\n",
       " 'y',\n",
       " 'juin',\n",
       " 'main',\n",
       " 'nous',\n",
       " 'cette',\n",
       " 'eux',\n",
       " 'sage',\n",
       " 'of',\n",
       " 'auriez',\n",
       " 'sien',\n",
       " 'fussiez',\n",
       " '\\x12',\n",
       " 'plupart',\n",
       " 'de',\n",
       " 'ans',\n",
       " 'êtes',\n",
       " 'dos',\n",
       " 'guide',\n",
       " 'encore',\n",
       " 'devoir',\n",
       " 'juste',\n",
       " 'place',\n",
       " 'il',\n",
       " 'eues',\n",
       " 'ne',\n",
       " 'donc',\n",
       " 'bac',\n",
       " 'novembre',\n",
       " 'serions',\n",
       " '\\x13',\n",
       " 'recrutement',\n",
       " 'ad',\n",
       " 'bt',\n",
       " 'eussent',\n",
       " 'furent',\n",
       " 'parole',\n",
       " 'sa',\n",
       " 'fût',\n",
       " 'seriez',\n",
       " 'aviez',\n",
       " 'eue',\n",
       " 'indeed',\n",
       " 'tandis',\n",
       " 'je',\n",
       " 'serai',\n",
       " 'faites',\n",
       " '\\x15',\n",
       " 'qui',\n",
       " 'haut',\n",
       " '\\x08',\n",
       " 'un',\n",
       " 'paie',\n",
       " 'vois',\n",
       " '\\x03',\n",
       " 'sois',\n",
       " 'nanterre',\n",
       " 'ils',\n",
       " 'suis',\n",
       " '\\x05',\n",
       " 'nommés',\n",
       " 'non',\n",
       " 'avaient',\n",
       " 'participation',\n",
       " 'quels',\n",
       " 'm',\n",
       " 'ceux',\n",
       " 'et',\n",
       " 'vu',\n",
       " '\\x11',\n",
       " 'du',\n",
       " '10g',\n",
       " 'boulognebillancourt',\n",
       " 'étant',\n",
       " 'mon',\n",
       " 'nommée',\n",
       " 'elles',\n",
       " 'où',\n",
       " 'aura',\n",
       " 'notre',\n",
       " 'dans',\n",
       " 'indeedcomrd7e8913ed00d0384',\n",
       " 'lieu',\n",
       " 'force',\n",
       " '\\x04',\n",
       " 'ou',\n",
       " 'eusses',\n",
       " 'doit',\n",
       " 'n',\n",
       " 'début',\n",
       " 'an',\n",
       " 'étés',\n",
       " 'dune',\n",
       " 'au',\n",
       " 'faire',\n",
       " 'serez',\n",
       " 'universite',\n",
       " 'mobiles',\n",
       " 'ait',\n",
       " 'soi',\n",
       " 'eus',\n",
       " 'dehors',\n",
       " 'tels',\n",
       " '\\x06',\n",
       " 'ma',\n",
       " 'fûtes',\n",
       " 'sommes',\n",
       " 'via',\n",
       " 'puis',\n",
       " 'paris',\n",
       " 'sont',\n",
       " 'faisez',\n",
       " 'ses',\n",
       " 'alors',\n",
       " 'soient',\n",
       " 'bon',\n",
       " 'des',\n",
       " 'la',\n",
       " 'minimum',\n",
       " 'aurions',\n",
       " 'avais',\n",
       " 'moi',\n",
       " '\\x0e',\n",
       " 'avons',\n",
       " 'demenager',\n",
       " 'notes',\n",
       " 'aurons',\n",
       " 'ni',\n",
       " 'ici',\n",
       " 'comment',\n",
       " 'étais',\n",
       " 'ça',\n",
       " 'v3',\n",
       " 'font',\n",
       " 'eut',\n",
       " 'quelles',\n",
       " 'une',\n",
       " 'décembre',\n",
       " 'dedans',\n",
       " '\\x14',\n",
       " 'serons',\n",
       " 'concerne',\n",
       " 'demenagement',\n",
       " 'serais',\n",
       " 'nom',\n",
       " 'devrions',\n",
       " 'seulement',\n",
       " 'sujet',\n",
       " 'aucun',\n",
       " 'paiement',\n",
       " 'etc',\n",
       " 'comme',\n",
       " 'aussi',\n",
       " 'nos',\n",
       " 'devriez',\n",
       " 'dès',\n",
       " 'fus',\n",
       " 'ai',\n",
       " 'devrait',\n",
       " 'fussent',\n",
       " 'aout',\n",
       " 'tellement',\n",
       " 'pÃ©rimÃ¨tre',\n",
       " '\\x19',\n",
       " 'concernant',\n",
       " 'quelle',\n",
       " 'sur',\n",
       " 'fusses',\n",
       " 'état',\n",
       " 'mars',\n",
       " 'ces',\n",
       " 'l',\n",
       " 'car',\n",
       " 'chez',\n",
       " 'très',\n",
       " 't',\n",
       " 'août',\n",
       " 'devrons',\n",
       " 'mai',\n",
       " 'avec',\n",
       " 'j',\n",
       " 'te',\n",
       " 'périmètre',\n",
       " 'vers',\n",
       " 'fait',\n",
       " 'auraient',\n",
       " 'autre',\n",
       " 'son',\n",
       " 'deux',\n",
       " 'ton',\n",
       " 'eurent',\n",
       " 'x',\n",
       " 'telephonique',\n",
       " 'ci',\n",
       " 'trop',\n",
       " 'septembre',\n",
       " '\\x07',\n",
       " '11g',\n",
       " 'aurez',\n",
       " 'lui',\n",
       " 'sites',\n",
       " 'email',\n",
       " 'participer',\n",
       " 'là',\n",
       " 'est',\n",
       " 'tes',\n",
       " 'entre',\n",
       " 'me',\n",
       " 'aurai',\n",
       " 'à',\n",
       " 'ce',\n",
       " 'se',\n",
       " \"d'un\",\n",
       " 'fusse',\n",
       " 'cela',\n",
       " 'parce',\n",
       " '\\x00',\n",
       " 'ont',\n",
       " 'en',\n",
       " 'mes',\n",
       " 'toi',\n",
       " 'soyons',\n",
       " 'environ',\n",
       " 'telephoniques',\n",
       " 'voit',\n",
       " 'avions',\n",
       " 'eusse',\n",
       " 'sera',\n",
       " 'janvier',\n",
       " 'auront',\n",
       " 'aujourhui',\n",
       " 'v2',\n",
       " 'qu',\n",
       " 'avant',\n",
       " 'cet',\n",
       " 'vont',\n",
       " 'université',\n",
       " 'avoir',\n",
       " 'ion',\n",
       " 'tout',\n",
       " 'octobre',\n",
       " 'sous',\n",
       " 'lycee',\n",
       " 'mot',\n",
       " 'in',\n",
       " 'moins',\n",
       " 'fois',\n",
       " 'dun',\n",
       " 'maximum',\n",
       " 'étiez',\n",
       " 'soyez',\n",
       " 'ayons',\n",
       " 'être',\n",
       " 'avait',\n",
       " 'rh',\n",
       " 'eu',\n",
       " 'nouveau',\n",
       " 'nommé',\n",
       " 'elle',\n",
       " 'aux',\n",
       " 'fevrier',\n",
       " 'source',\n",
       " 'ayant',\n",
       " 'aurait',\n",
       " 'avez',\n",
       " 'soit',\n",
       " 'o',\n",
       " 'liens',\n",
       " 'nouveaux',\n",
       " 'decembre',\n",
       " 'devrez',\n",
       " 'voient',\n",
       " 'tu',\n",
       " 'mise',\n",
       " 'dù',\n",
       " '\\x02',\n",
       " 'ceci',\n",
       " 'auras',\n",
       " '\\t',\n",
       " 'seront',\n",
       " 'leurs',\n",
       " 'maintenant',\n",
       " 'as']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#voir si utile\n",
    "def remove_stopwords(text,stopwords_list):\n",
    "    text_temp = \" \".join(text.split())+\" \"\n",
    "    for word in stopwords_list:\n",
    "        text_temp = text_temp.replace(\" \"+word+\" \", \" \")\n",
    "    return text_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"informaticien developpement reseaux developpeur integrateur web eragny 95 recherche opportunite poste developpeur integrateur web monter plus competence approfondir bases solide acquis formation experience informaticien developpement reseaux legrandcercle95 eragny 95 2016 2017 informaticien entreprise missions etait gerer different probleme entreprise antivirus serveur sauvegarde nas cree gerer droits active directory parametrer clients leger ainsi materiel informatique imprimantes etiqueteuse ip fixe reajustement code html css grand public selon normes w3c creation odbc creation code php sql recupere donnees librairie fournisseur enregistre fiche xml creation bannieres differents evenements photoshop formation logiciel photoshop charge clientele europcar commercial saintouenaumone 95 2008 2016 missions principales qualite service assurer accueil clients respect charte agence gestion clients traiter ensemble appels analyser besoins client assurer suivi clientele logistique administratif s'assurer disponibilite vehicules gerer administratif agence gestion caisse polyvalence formation developpeur integrateur web 2015 2016 cif centre formation ifocop 8mois creation differents portfolio presentation ecommerce cree a z examen developpeur web stage club formateurs eragny 95 2016 2016 developpement wordpress a z commercial stagiaire lamy assurance 2007 2007 stagiaire port marly accessoires portmarly 78 2007 2007 galeries lafayette vendeur 2005 2006 confection homme job etudiant brice vendeur pap job 2004 2004 etudiant formation niveau ii bac3 bac4 developpeur integrateur web ifocop eragny 95 2015 2016 \""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "liste_cv_no_stop = [remove_stopwords(text,stop_words_main) for text in liste_cv_treated]\n",
    "liste_cv_no_stop[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load nltk's SnowballStemmer as variabled 'stemmer'\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"french\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# here I define a tokenizer and stemmer which returns the set of stems in the text that it is passed\n",
    "\n",
    "def tokenize_and_stem(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems\n",
    "\n",
    "\n",
    "def tokenize_only(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    #print(tokens)\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        \n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "totalvocab_stemmed = []\n",
    "totalvocab_tokenized = []\n",
    "for text in liste_cv_no_stop:\n",
    "    allwords_stemmed = tokenize_and_stem(text) #for each item in 'synopses', tokenize/stem\n",
    "    totalvocab_stemmed.extend(allwords_stemmed) #extend the 'totalvocab_stemmed' list\n",
    "    allwords_tokenized = tokenize_only(text)\n",
    "    totalvocab_tokenized.extend(allwords_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "liste_cv_cleaned = [tokenize_only(text) for text in liste_cv_no_stop]\n",
    "liste_cv_no_num = [' '.join(cv) for cv in liste_cv_cleaned]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"informaticien developpement reseaux developpeur integrateur web eragny recherche opportunite poste developpeur integrateur web monter plus competence approfondir bases solide acquis formation experience informaticien developpement reseaux legrandcercle95 eragny informaticien entreprise missions etait gerer different probleme entreprise antivirus serveur sauvegarde nas cree gerer droits active directory parametrer clients leger ainsi materiel informatique imprimantes etiqueteuse ip fixe reajustement code html css grand public selon normes w3c creation odbc creation code php sql recupere donnees librairie fournisseur enregistre fiche xml creation bannieres differents evenements photoshop formation logiciel photoshop charge clientele europcar commercial saintouenaumone missions principales qualite service assurer accueil clients respect charte agence gestion clients traiter ensemble appels analyser besoins client assurer suivi clientele logistique administratif s'assurer disponibilite vehicules gerer administratif agence gestion caisse polyvalence formation developpeur integrateur web cif centre formation ifocop 8mois creation differents portfolio presentation ecommerce cree a z examen developpeur web stage club formateurs eragny developpement wordpress a z commercial stagiaire lamy assurance stagiaire port marly accessoires portmarly galeries lafayette vendeur confection homme job etudiant brice vendeur pap job etudiant formation niveau ii bac3 bac4 developpeur integrateur web ifocop eragny\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#on constate que les caracteres numériques sont biens supprimés\n",
    "liste_cv_no_num[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1608"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(liste_cv_no_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def del_word_freq(max_idf,min_idf,list_texts):\n",
    "    \n",
    "    tf_vect = TfidfVectorizer(stop_words=stop_words_main,max_df=max_idf,min_df=min_idf,\\\n",
    "                           preprocessor=text_treatment,tokenizer=tokenize_only)\n",
    "    bow_idf_indeed = tf_vect.fit_transform(list_texts)\n",
    "    vocab_liste_idf = tf_vect.get_feature_names()\n",
    "    \n",
    "    liste_texte_new =[]\n",
    "    prop = 0\n",
    "    for text in list_texts:\n",
    "        liste_mot = [mot for mot in text.split(' ') if mot in vocab_liste_idf]\n",
    "        liste_texte_new.append(' '.join(liste_mot))\n",
    "        prop += len(liste_mot)/len(text.split(' '))\n",
    "        \n",
    "    return liste_texte_new, (prop/len(list_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "liste_texte_new, prop = del_word_freq(0.80,0.05,liste_cv_no_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5074057269038234"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'informaticien developpement reseaux developpeur web recherche poste developpeur web plus competence bases formation informaticien developpement reseaux informaticien entreprise missions gerer entreprise serveur sauvegarde gerer active directory clients ainsi materiel informatique ip html css normes creation creation php sql donnees xml creation differents evenements photoshop formation logiciel photoshop charge clientele commercial missions qualite service assurer accueil clients respect agence gestion clients ensemble appels analyser besoins client assurer suivi clientele logistique gerer agence gestion caisse formation developpeur web centre formation creation differents presentation ecommerce developpeur web stage developpement commercial stagiaire assurance stagiaire formation niveau developpeur web'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "liste_texte_new[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"informaticien developpement reseaux developpeur integrateur web eragny recherche opportunite poste developpeur integrateur web monter plus competence approfondir bases solide acquis formation experience informaticien developpement reseaux legrandcercle95 eragny informaticien entreprise missions etait gerer different probleme entreprise antivirus serveur sauvegarde nas cree gerer droits active directory parametrer clients leger ainsi materiel informatique imprimantes etiqueteuse ip fixe reajustement code html css grand public selon normes w3c creation odbc creation code php sql recupere donnees librairie fournisseur enregistre fiche xml creation bannieres differents evenements photoshop formation logiciel photoshop charge clientele europcar commercial saintouenaumone missions principales qualite service assurer accueil clients respect charte agence gestion clients traiter ensemble appels analyser besoins client assurer suivi clientele logistique administratif s'assurer disponibilite vehicules gerer administratif agence gestion caisse polyvalence formation developpeur integrateur web cif centre formation ifocop 8mois creation differents portfolio presentation ecommerce cree a z examen developpeur web stage club formateurs eragny developpement wordpress a z commercial stagiaire lamy assurance stagiaire port marly accessoires portmarly galeries lafayette vendeur confection homme job etudiant brice vendeur pap job etudiant formation niveau ii bac3 bac4 developpeur integrateur web ifocop eragny\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "liste_cv_no_num[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc2Vec essaie 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pourquoi ne fonctionnerait-il pas ?\n",
    "\n",
    "-> Le contexte n'explique pas suffisemment le mot à prédire, en quelques mots on passe \"du coq à l'âne\" sur un CV. Or le word2vec émet l'hypothèse que le contexte explique le mot.\n",
    "\n",
    "Est ce qu'un prétraitement est nécessaire ? OUI il est conseillé => Tokenizing et eventuellement Stemming, stop word removing (il n'apportent rien au contexte), punctuation removing, change numeric into letters.\n",
    "\n",
    "Comment l'améliorer ? -> Faire le Word2vec sur une sous partie du CV\n",
    "-> Retirer la partie Centre d'intérets (ajoute du bruit dans la compréhension du contexte du cv)\n",
    "-> Certaines parties correspondent à un listing de mots Secteurs d'activité, Outils : intérêt de conserver ? -> Peut-il trouver un contexte pour le mots listés\n",
    "\n",
    "Attention le dernier envoie de CV de Talan (CV_3) ne contient pas de phrases, uniquement des listing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1392"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "import logging\n",
    "import re\n",
    "from gensim.models.doc2vec import TaggedDocument, Doc2Vec\n",
    "\n",
    "class Documents(object):\n",
    "    def __init__(self, documents, labels):\n",
    "        self.documents = documents\n",
    "        self.labels = labels\n",
    "\n",
    "    def __iter__(self):\n",
    "        for i, doc in enumerate(self.documents):\n",
    "            yield TaggedDocument(words = doc, tags = [self.labels[i]])\n",
    "\n",
    "def create_documents_doc2vec(liste_cv,liste_labels):\n",
    "    \n",
    "    cvdocs = liste_cv\n",
    "    count = len(cvdocs)\n",
    "    preprocessedCv = []\n",
    "    preprocessedLabels = []\n",
    "    duplicate_dict = {}\n",
    "\n",
    "    ilabel = 0\n",
    "    for t in cvdocs:\n",
    "        if t not in duplicate_dict:\n",
    "            duplicate_dict[t] = True\n",
    "            preprocessedCv.append(t)\n",
    "            preprocessedLabels.append(liste_labels[ilabel].split('/')[-1].split('.')[0]) \n",
    "        ilabel +=1\n",
    "\n",
    "    documents = Documents(preprocessedCv,preprocessedLabels)\n",
    "    return documents\n",
    "\n",
    "documents = create_documents_doc2vec(liste_texte_new,liste_fileNames_fr) \n",
    "len(documents.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ajouter un tf-idf pour trier les mots au préalable, étape de preprocessing supplémentaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_doc2vec(documents,model_name):\n",
    "    #size : dim du vector représentant les mots : représente la dim de l'hidden layer\n",
    "    #window : max distance between current word and the word 2 predict <=> taille du contexte ?\n",
    "    model = Doc2Vec(size=50, dbow_words= 1, dm=0, iter=1,  window=3, seed=1337, min_count=0, workers=4,alpha=0.025, \n",
    "                    min_alpha=0.025)\n",
    "\n",
    "    model.build_vocab(documents)\n",
    "    #training of model\n",
    "    #how to get model error on the train ?\n",
    "    for epoch in range(100):\n",
    "        print('iteration '+str(epoch+1))\n",
    "        model.train(documents,total_examples=model.corpus_count,epochs=model.iter)\n",
    "        model.save(model_name)\n",
    "        model.alpha -= 0.002\n",
    "        model.min_alpha = model.alpha\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1\n",
      "iteration 2\n",
      "iteration 3\n",
      "iteration 4\n",
      "iteration 5\n",
      "iteration 6\n",
      "iteration 7\n",
      "iteration 8\n",
      "iteration 9\n",
      "iteration 10\n",
      "iteration 11\n",
      "iteration 12\n",
      "iteration 13\n",
      "iteration 14\n",
      "iteration 15\n",
      "iteration 16\n",
      "iteration 17\n",
      "iteration 18\n",
      "iteration 19\n",
      "iteration 20\n",
      "iteration 21\n",
      "iteration 22\n",
      "iteration 23\n",
      "iteration 24\n",
      "iteration 25\n",
      "iteration 26\n",
      "iteration 27\n",
      "iteration 28\n",
      "iteration 29\n",
      "iteration 30\n",
      "iteration 31\n",
      "iteration 32\n",
      "iteration 33\n",
      "iteration 34\n",
      "iteration 35\n",
      "iteration 36\n",
      "iteration 37\n",
      "iteration 38\n",
      "iteration 39\n",
      "iteration 40\n",
      "iteration 41\n",
      "iteration 42\n",
      "iteration 43\n",
      "iteration 44\n",
      "iteration 45\n",
      "iteration 46\n",
      "iteration 47\n",
      "iteration 48\n",
      "iteration 49\n",
      "iteration 50\n",
      "iteration 51\n",
      "iteration 52\n",
      "iteration 53\n",
      "iteration 54\n",
      "iteration 55\n",
      "iteration 56\n",
      "iteration 57\n",
      "iteration 58\n",
      "iteration 59\n",
      "iteration 60\n",
      "iteration 61\n",
      "iteration 62\n",
      "iteration 63\n",
      "iteration 64\n",
      "iteration 65\n",
      "iteration 66\n",
      "iteration 67\n",
      "iteration 68\n",
      "iteration 69\n",
      "iteration 70\n",
      "iteration 71\n",
      "iteration 72\n",
      "iteration 73\n",
      "iteration 74\n",
      "iteration 75\n",
      "iteration 76\n",
      "iteration 77\n",
      "iteration 78\n",
      "iteration 79\n",
      "iteration 80\n",
      "iteration 81\n",
      "iteration 82\n",
      "iteration 83\n",
      "iteration 84\n",
      "iteration 85\n",
      "iteration 86\n",
      "iteration 87\n",
      "iteration 88\n",
      "iteration 89\n",
      "iteration 90\n",
      "iteration 91\n",
      "iteration 92\n",
      "iteration 93\n",
      "iteration 94\n",
      "iteration 95\n",
      "iteration 96\n",
      "iteration 97\n",
      "iteration 98\n",
      "iteration 99\n",
      "iteration 100\n"
     ]
    }
   ],
   "source": [
    "model = train_doc2vec(documents,'Doc2vec300.model')\n",
    "#docvec = model.docvecs['./data/Txt/business_analyst/cv_business analyst_541.txt'] #if string tag used in training\n",
    "#len(model.docvecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cv_chef_de_projet_informatique_186',\n",
       " 'cv_chef_de_projet_informatique_187',\n",
       " 'cv_chef_de_projet_informatique_188',\n",
       " 'cv_chef_de_projet_informatique_189',\n",
       " 'cv_chef_de_projet_informatique_19',\n",
       " 'cv_chef_de_projet_informatique_190',\n",
       " 'cv_chef_de_projet_informatique_191',\n",
       " 'cv_chef_de_projet_informatique_192',\n",
       " 'cv_chef_de_projet_informatique_193',\n",
       " 'cv_chef_de_projet_informatique_194',\n",
       " 'cv_chef_de_projet_informatique_195',\n",
       " 'cv_chef_de_projet_informatique_196',\n",
       " 'cv_chef_de_projet_informatique_197',\n",
       " 'cv_chef_de_projet_informatique_198',\n",
       " 'cv_chef_de_projet_informatique_199',\n",
       " 'cv_chef_de_projet_informatique_2',\n",
       " 'cv_chef_de_projet_informatique_20',\n",
       " 'cv_chef_de_projet_informatique_200',\n",
       " 'cv_chef_de_projet_informatique_201',\n",
       " 'cv_chef_de_projet_informatique_202',\n",
       " 'cv_chef_de_projet_informatique_203',\n",
       " 'cv_chef_de_projet_informatique_204',\n",
       " 'cv_chef_de_projet_informatique_205',\n",
       " 'cv_chef_de_projet_informatique_206',\n",
       " 'cv_chef_de_projet_informatique_207',\n",
       " 'cv_chef_de_projet_informatique_208',\n",
       " 'cv_chef_de_projet_informatique_209',\n",
       " 'cv_chef_de_projet_informatique_21',\n",
       " 'cv_chef_de_projet_informatique_210',\n",
       " 'cv_chef_de_projet_informatique_211',\n",
       " 'cv_chef_de_projet_informatique_212',\n",
       " 'cv_chef_de_projet_informatique_213',\n",
       " 'cv_chef_de_projet_informatique_214',\n",
       " 'cv_chef_de_projet_informatique_215',\n",
       " 'cv_chef_de_projet_informatique_216',\n",
       " 'cv_chef_de_projet_informatique_217',\n",
       " 'cv_chef_de_projet_informatique_218',\n",
       " 'cv_chef_de_projet_informatique_219',\n",
       " 'cv_chef_de_projet_informatique_22',\n",
       " 'cv_chef_de_projet_informatique_220',\n",
       " 'cv_chef_de_projet_informatique_221',\n",
       " 'cv_chef_de_projet_informatique_222',\n",
       " 'cv_chef_de_projet_informatique_223',\n",
       " 'cv_chef_de_projet_informatique_224',\n",
       " 'cv_chef_de_projet_informatique_225',\n",
       " 'cv_chef_de_projet_informatique_226',\n",
       " 'cv_chef_de_projet_informatique_227',\n",
       " 'cv_chef_de_projet_informatique_228',\n",
       " 'cv_chef_de_projet_informatique_229',\n",
       " 'cv_chef_de_projet_informatique_230',\n",
       " 'cv_chef_de_projet_informatique_231',\n",
       " 'cv_chef_de_projet_informatique_232',\n",
       " 'cv_chef_de_projet_informatique_233',\n",
       " 'cv_chef_de_projet_informatique_234',\n",
       " 'cv_chef_de_projet_informatique_235',\n",
       " 'cv_chef_de_projet_informatique_236',\n",
       " 'cv_chef_de_projet_informatique_237',\n",
       " 'cv_chef_de_projet_informatique_238',\n",
       " 'cv_chef_de_projet_informatique_239',\n",
       " 'cv_chef_de_projet_informatique_24',\n",
       " 'cv_chef_de_projet_informatique_240',\n",
       " 'cv_chef_de_projet_informatique_241',\n",
       " 'cv_chef_de_projet_informatique_242',\n",
       " 'cv_chef_de_projet_informatique_243',\n",
       " 'cv_chef_de_projet_informatique_244',\n",
       " 'cv_chef_de_projet_informatique_245',\n",
       " 'cv_chef_de_projet_informatique_246',\n",
       " 'cv_chef_de_projet_informatique_247',\n",
       " 'cv_chef_de_projet_informatique_248',\n",
       " 'cv_chef_de_projet_informatique_249',\n",
       " 'cv_chef_de_projet_informatique_25',\n",
       " 'cv_chef_de_projet_informatique_250',\n",
       " 'cv_chef_de_projet_informatique_251',\n",
       " 'cv_chef_de_projet_informatique_252',\n",
       " 'cv_chef_de_projet_informatique_253',\n",
       " 'cv_chef_de_projet_informatique_254',\n",
       " 'cv_chef_de_projet_informatique_255',\n",
       " 'cv_chef_de_projet_informatique_256',\n",
       " 'cv_chef_de_projet_informatique_257',\n",
       " 'cv_chef_de_projet_informatique_258',\n",
       " 'cv_chef_de_projet_informatique_259',\n",
       " 'cv_chef_de_projet_informatique_26',\n",
       " 'cv_chef_de_projet_informatique_260',\n",
       " 'cv_chef_de_projet_informatique_261',\n",
       " 'cv_chef_de_projet_informatique_262',\n",
       " 'cv_chef_de_projet_informatique_263',\n",
       " 'cv_chef_de_projet_informatique_264',\n",
       " 'cv_chef_de_projet_informatique_265',\n",
       " 'cv_chef_de_projet_informatique_266',\n",
       " 'cv_chef_de_projet_informatique_267',\n",
       " 'cv_chef_de_projet_informatique_268',\n",
       " 'cv_chef_de_projet_informatique_269',\n",
       " 'cv_chef_de_projet_informatique_270',\n",
       " 'cv_chef_de_projet_informatique_271',\n",
       " 'cv_chef_de_projet_informatique_272',\n",
       " 'cv_chef_de_projet_informatique_273',\n",
       " 'cv_chef_de_projet_informatique_274',\n",
       " 'cv_chef_de_projet_informatique_275',\n",
       " 'cv_chef_de_projet_informatique_276',\n",
       " 'cv_chef_de_projet_informatique_277',\n",
       " 'cv_chef_de_projet_informatique_278',\n",
       " 'cv_data scientist_0',\n",
       " 'cv_data scientist_101',\n",
       " 'cv_data scientist_102',\n",
       " 'cv_data scientist_103',\n",
       " 'cv_data scientist_104',\n",
       " 'cv_data scientist_106',\n",
       " 'cv_data scientist_108',\n",
       " 'cv_data scientist_109',\n",
       " 'cv_data scientist_110',\n",
       " 'cv_data scientist_111',\n",
       " 'cv_data scientist_112',\n",
       " 'cv_data scientist_116',\n",
       " 'cv_data scientist_117',\n",
       " 'cv_data scientist_118',\n",
       " 'cv_data scientist_120',\n",
       " 'cv_data scientist_121',\n",
       " 'cv_data scientist_122',\n",
       " 'cv_data scientist_126',\n",
       " 'cv_data scientist_127',\n",
       " 'cv_data scientist_13',\n",
       " 'cv_data scientist_130',\n",
       " 'cv_data scientist_135',\n",
       " 'cv_data scientist_136',\n",
       " 'cv_data scientist_137',\n",
       " 'cv_data scientist_139',\n",
       " 'cv_data scientist_14',\n",
       " 'cv_data scientist_140',\n",
       " 'cv_data scientist_142',\n",
       " 'cv_data scientist_143',\n",
       " 'cv_data scientist_144',\n",
       " 'cv_data scientist_146',\n",
       " 'cv_data scientist_147',\n",
       " 'cv_data scientist_148',\n",
       " 'cv_data scientist_149',\n",
       " 'cv_data scientist_15',\n",
       " 'cv_data scientist_151',\n",
       " 'cv_data scientist_152',\n",
       " 'cv_data scientist_154',\n",
       " 'cv_data scientist_158',\n",
       " 'cv_data scientist_16',\n",
       " 'cv_data scientist_161',\n",
       " 'cv_data scientist_162',\n",
       " 'cv_data scientist_163',\n",
       " 'cv_data scientist_165',\n",
       " 'cv_data scientist_166',\n",
       " 'cv_data scientist_167',\n",
       " 'cv_data scientist_168',\n",
       " 'cv_data scientist_17',\n",
       " 'cv_data scientist_18',\n",
       " 'cv_data scientist_19',\n",
       " 'cv_data scientist_20',\n",
       " 'cv_data scientist_21',\n",
       " 'cv_data scientist_216',\n",
       " 'cv_data scientist_218',\n",
       " 'cv_data scientist_220',\n",
       " 'cv_data scientist_221',\n",
       " 'cv_data scientist_222',\n",
       " 'cv_data scientist_225',\n",
       " 'cv_data scientist_227',\n",
       " 'cv_data scientist_228',\n",
       " 'cv_data scientist_229',\n",
       " 'cv_data scientist_23',\n",
       " 'cv_data scientist_230',\n",
       " 'cv_data scientist_231',\n",
       " 'cv_data scientist_232',\n",
       " 'cv_data scientist_233',\n",
       " 'cv_data scientist_234',\n",
       " 'cv_data scientist_235',\n",
       " 'cv_data scientist_236',\n",
       " 'cv_data scientist_238',\n",
       " 'cv_data scientist_239',\n",
       " 'cv_data scientist_240',\n",
       " 'cv_data scientist_241',\n",
       " 'cv_data scientist_242',\n",
       " 'cv_data scientist_243',\n",
       " 'cv_data scientist_244',\n",
       " 'cv_data scientist_245',\n",
       " 'cv_data scientist_246',\n",
       " 'cv_data scientist_247',\n",
       " 'cv_data scientist_248',\n",
       " 'cv_data scientist_249',\n",
       " 'cv_data scientist_250',\n",
       " 'cv_data scientist_251',\n",
       " 'cv_data scientist_252',\n",
       " 'cv_data scientist_253',\n",
       " 'cv_data scientist_254',\n",
       " 'cv_data scientist_255',\n",
       " 'cv_data scientist_256',\n",
       " 'cv_data scientist_257',\n",
       " 'cv_data scientist_258',\n",
       " 'cv_data scientist_259',\n",
       " 'cv_data scientist_26',\n",
       " 'cv_business analyst_101',\n",
       " 'cv_business analyst_103',\n",
       " 'cv_business analyst_104',\n",
       " 'cv_business analyst_106',\n",
       " 'cv_business analyst_109',\n",
       " 'cv_business analyst_110',\n",
       " 'cv_business analyst_112',\n",
       " 'cv_business analyst_114',\n",
       " 'cv_business analyst_12',\n",
       " 'cv_business analyst_122',\n",
       " 'cv_business analyst_123',\n",
       " 'cv_business analyst_13',\n",
       " 'cv_business analyst_130',\n",
       " 'cv_business analyst_131',\n",
       " 'cv_business analyst_132',\n",
       " 'cv_business analyst_133',\n",
       " 'cv_business analyst_14',\n",
       " 'cv_business analyst_142',\n",
       " 'cv_business analyst_143',\n",
       " 'cv_business analyst_144',\n",
       " 'cv_business analyst_146',\n",
       " 'cv_business analyst_147',\n",
       " 'cv_business analyst_148',\n",
       " 'cv_business analyst_151',\n",
       " 'cv_business analyst_153',\n",
       " 'cv_business analyst_155',\n",
       " 'cv_business analyst_156',\n",
       " 'cv_business analyst_157',\n",
       " 'cv_business analyst_160',\n",
       " 'cv_business analyst_162',\n",
       " 'cv_business analyst_165',\n",
       " 'cv_business analyst_169',\n",
       " 'cv_business analyst_17',\n",
       " 'cv_business analyst_172',\n",
       " 'cv_business analyst_174',\n",
       " 'cv_business analyst_176',\n",
       " 'cv_business analyst_177',\n",
       " 'cv_business analyst_18',\n",
       " 'cv_business analyst_181',\n",
       " 'cv_business analyst_182',\n",
       " 'cv_business analyst_183',\n",
       " 'cv_business analyst_19',\n",
       " 'cv_business analyst_192',\n",
       " 'cv_business analyst_193',\n",
       " 'cv_business analyst_194',\n",
       " 'cv_business analyst_20',\n",
       " 'cv_business analyst_21',\n",
       " 'cv_business analyst_217',\n",
       " 'cv_business analyst_218',\n",
       " 'cv_business analyst_222',\n",
       " 'cv_business analyst_23',\n",
       " 'cv_business analyst_236',\n",
       " 'cv_business analyst_24',\n",
       " 'cv_business analyst_248',\n",
       " 'cv_business analyst_25',\n",
       " 'cv_business analyst_250',\n",
       " 'cv_business analyst_253',\n",
       " 'cv_business analyst_254',\n",
       " 'cv_business analyst_256',\n",
       " 'cv_business analyst_258',\n",
       " 'cv_business analyst_261',\n",
       " 'cv_business analyst_264',\n",
       " 'cv_business analyst_266',\n",
       " 'cv_business analyst_269',\n",
       " 'cv_business analyst_27',\n",
       " 'cv_business analyst_272',\n",
       " 'cv_business analyst_273',\n",
       " 'cv_business analyst_274',\n",
       " 'cv_business analyst_277',\n",
       " 'cv_business analyst_279',\n",
       " 'cv_consultant it_10',\n",
       " 'cv_consultant it_100',\n",
       " 'cv_consultant it_104',\n",
       " 'cv_consultant it_105',\n",
       " 'cv_consultant it_106',\n",
       " 'cv_consultant it_107',\n",
       " 'cv_consultant it_108',\n",
       " 'cv_consultant it_109',\n",
       " 'cv_consultant it_11',\n",
       " 'cv_consultant it_110',\n",
       " 'cv_consultant it_111',\n",
       " 'cv_consultant it_112',\n",
       " 'cv_consultant it_115',\n",
       " 'cv_consultant it_117',\n",
       " 'cv_consultant it_119',\n",
       " 'cv_consultant it_12',\n",
       " 'cv_consultant it_120',\n",
       " 'cv_consultant it_123',\n",
       " 'cv_consultant it_126',\n",
       " 'cv_consultant it_128',\n",
       " 'cv_consultant it_133',\n",
       " 'cv_consultant it_134',\n",
       " 'cv_consultant it_135',\n",
       " 'cv_consultant it_136',\n",
       " 'cv_consultant it_137',\n",
       " 'cv_consultant it_139',\n",
       " 'cv_consultant it_14',\n",
       " 'cv_consultant it_141',\n",
       " 'cv_consultant it_146',\n",
       " 'cv_consultant it_149',\n",
       " 'cv_consultant it_157',\n",
       " 'cv_consultant it_158',\n",
       " 'cv_consultant it_160',\n",
       " 'cv_consultant it_164',\n",
       " 'cv_consultant it_165',\n",
       " 'cv_consultant it_166',\n",
       " 'cv_consultant it_167',\n",
       " 'cv_consultant it_168',\n",
       " 'cv_consultant it_169',\n",
       " 'cv_consultant it_17',\n",
       " 'cv_consultant it_170',\n",
       " 'cv_consultant it_171',\n",
       " 'cv_consultant it_172',\n",
       " 'cv_consultant it_179',\n",
       " 'cv_consultant it_18',\n",
       " 'cv_consultant it_180',\n",
       " 'cv_consultant it_182',\n",
       " 'cv_consultant it_183',\n",
       " 'cv_consultant it_184',\n",
       " 'cv_consultant it_185',\n",
       " 'cv_consultant it_188',\n",
       " 'cv_consultant it_190',\n",
       " 'cv_consultant it_191',\n",
       " 'cv_consultant it_194',\n",
       " 'cv_consultant it_195',\n",
       " 'cv_consultant it_196',\n",
       " 'cv_consultant it_198',\n",
       " 'cv_consultant it_199',\n",
       " 'cv_consultant it_2',\n",
       " 'cv_consultant it_20',\n",
       " 'cv_consultant it_202',\n",
       " 'cv_consultant it_204',\n",
       " 'cv_consultant it_205',\n",
       " 'cv_consultant it_207',\n",
       " 'cv_consultant it_208',\n",
       " 'cv_consultant it_210',\n",
       " 'cv_consultant it_211',\n",
       " 'cv_consultant it_213',\n",
       " 'cv_consultant it_214',\n",
       " 'cv_consultant it_217',\n",
       " 'cv_consultant it_218',\n",
       " 'cv_consultant it_219',\n",
       " 'cv_consultant it_22',\n",
       " 'cv_consultant it_220',\n",
       " 'cv_consultant it_221',\n",
       " 'cv_consultant it_222',\n",
       " 'cv_consultant it_223',\n",
       " 'cv_consultant it_224',\n",
       " 'cv_consultant it_225',\n",
       " 'cv_consultant it_226',\n",
       " 'cv_consultant it_228',\n",
       " 'cv_consultant it_229',\n",
       " 'cv_consultant it_23',\n",
       " 'cv_consultant it_230',\n",
       " 'cv_consultant it_232',\n",
       " 'cv_consultant it_233',\n",
       " 'cv_consultant it_234',\n",
       " 'cv_consultant it_236',\n",
       " 'cv_consultant it_237',\n",
       " 'cv_consultant it_238',\n",
       " 'cv_consultant it_241',\n",
       " 'cv_consultant it_242',\n",
       " 'cv_consultant it_245',\n",
       " 'cv_consultant it_246',\n",
       " 'cv_consultant it_247',\n",
       " 'cv_consultant it_248',\n",
       " 'cv_consultant it_249',\n",
       " 'cv_consultant it_251',\n",
       " 'cv_consultant it_252',\n",
       " 'cv_consultant it_253',\n",
       " 'cv_consultant it_255',\n",
       " 'cv_consultant it_258',\n",
       " 'cv_consultant it_259',\n",
       " 'cv_consultant it_26',\n",
       " 'cv_consultant it_260',\n",
       " 'cv_consultant it_261',\n",
       " 'cv_consultant it_262',\n",
       " 'cv_consultant it_263',\n",
       " 'cv_consultant it_264',\n",
       " 'cv_consultant it_265',\n",
       " 'cv_consultant it_266',\n",
       " 'cv_consultant it_267',\n",
       " 'cv_consultant it_27',\n",
       " 'cv_consultant it_272',\n",
       " 'cv_consultant it_278',\n",
       " 'cv_data engineer_1',\n",
       " 'cv_data engineer_10',\n",
       " 'cv_data engineer_101',\n",
       " 'cv_data engineer_110',\n",
       " 'cv_data engineer_112',\n",
       " 'cv_data engineer_116',\n",
       " 'cv_data engineer_129',\n",
       " 'cv_data engineer_134',\n",
       " 'cv_data engineer_149',\n",
       " 'cv_data engineer_16',\n",
       " 'cv_data engineer_165',\n",
       " 'cv_data engineer_169',\n",
       " 'cv_data engineer_196',\n",
       " 'cv_data engineer_20',\n",
       " 'cv_data engineer_220',\n",
       " 'cv_data engineer_227',\n",
       " 'cv_data engineer_23',\n",
       " 'cv_data engineer_240',\n",
       " 'cv_data engineer_253',\n",
       " 'cv_data engineer_256',\n",
       " 'cv_data engineer_27',\n",
       " 'cv_data engineer_276']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents.labels[-400:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cv_data analyst_116', 0.9189307689666748),\n",
       " ('cv_business analyst_181', 0.9111403822898865),\n",
       " ('cv_business intelligence_19', 0.9026414155960083),\n",
       " ('cv_DBA_145', 0.8998908996582031),\n",
       " ('cv_data analyst_105', 0.8968216776847839),\n",
       " ('cv_business intelligence_24', 0.8936462998390198),\n",
       " ('cv_data analyst_117', 0.8924216032028198),\n",
       " ('cv_informaticien_230', 0.8885915279388428),\n",
       " ('cv_data analyst_109', 0.8881703615188599),\n",
       " ('cv_data scientist_135', 0.8872749209403992)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.docvecs.most_similar('cv_data scientist_26')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#start testing\n",
    "#printing the vector of document at index 1 in docLabels\n",
    "docvec = model.docvecs[1]\n",
    "#print(docvec)\n",
    "#printing the vector of the file using its name\n",
    "#docvec = model.docvecs['1.txt'] #if string tag used in training\n",
    "#print(docvec)\n",
    "#to get most similar document with similarity scores using document-index\n",
    "similar_doc = model.docvecs.most_similar(14) \n",
    "#print(similar_doc)\n",
    "#to get most similar document with similarity scores using document- name\n",
    "#sims = model.docvecs.most_similar('1.txt')\n",
    "#print(sims)\n",
    "#to get vector of document that are not present in corpus \n",
    "#docvec = d2v_model.docvecs.infer_vector(‘war.txt’)\n",
    "#print docvec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk, math, codecs\n",
    "from gensim.models import Doc2Vec\n",
    "from nltk.cluster.kmeans import KMeansClusterer\n",
    "\n",
    "NUM_CLUSTERS = 5\n",
    "def kmeans_doc2Vec(model_name):\n",
    "    \n",
    "    model = Doc2Vec.load(model_name)\n",
    "\n",
    "    kclusterer = KMeansClusterer(NUM_CLUSTERS, distance=nltk.cluster.util.cosine_distance, repeats=25)\n",
    "    assigned_clusters = kclusterer.cluster(model.docvecs, assign_clusters=True)\n",
    "    return assigned_clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_document(text):\n",
    "    #text = preprocess(text)\n",
    "    return ''.join([x if x.isalnum() or x.isspace() else \" \" for x in text ]).split()\n",
    "\n",
    "def get_titles_by_cluster(id,clusters):\n",
    "    list = []\n",
    "    for x in range(0, len(assigned_clusters)):\n",
    "        if (clusters[x] == id):\n",
    "            list.append(preprocessedCv[x])\n",
    "    return list\n",
    "\n",
    "def get_topics(titles):\n",
    "    from collections import Counter\n",
    "    words = [preprocess_document(x) for x in titles]\n",
    "    words = [word for sublist in words for word in sublist]\n",
    "    filtered_words = [word for word in words if word not in stopwords.words('english')]\n",
    "    count = Counter(filtered_words)\n",
    "    print(count.most_common()[:20])\n",
    "\n",
    "\n",
    "def cluster_to_topics(id,clusters):\n",
    "    get_topics(get_titles_by_cluster(id,clusters))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"tag '1392' not seen in training corpus/invalid\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-634d39b5adc4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0massigned_clusters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkmeans_doc2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Doc2vec300.model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcluster\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_CLUSTERS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcluster_to_topics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcluster\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0massigned_clusters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-42-a143547c0ec5>\u001b[0m in \u001b[0;36mkmeans_doc2Vec\u001b[0;34m(model_name)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mkclusterer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKMeansClusterer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_CLUSTERS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcosine_distance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepeats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0massigned_clusters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkclusterer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocvecs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0massign_clusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0massigned_clusters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mehdiregina/anaconda/lib/python3.6/site-packages/nltk/cluster/util.py\u001b[0m in \u001b[0;36mcluster\u001b[0;34m(self, vectors, assign_clusters, trace)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;31m# call abstract method to cluster the vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster_vectorspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;31m# assign the vectors to clusters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mehdiregina/anaconda/lib/python3.6/site-packages/nltk/cluster/kmeans.py\u001b[0m in \u001b[0;36mcluster_vectorspace\u001b[0;34m(self, vectors, trace)\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'k-means trial'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_means\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtrial\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_means\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rng\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_means\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cluster_vectorspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0mmeanss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_means\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mehdiregina/anaconda/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m   1197\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors_docs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_int_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoctags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_rawint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1198\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1199\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tag '%s' not seen in training corpus/invalid\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1201\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__contains__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"tag '1392' not seen in training corpus/invalid\""
     ]
    }
   ],
   "source": [
    "  assigned_clusters = kmeans_doc2Vec(\"Doc2vec300.model\")\n",
    "for cluster in range(NUM_CLUSTERS):\n",
    "    print(cluster_to_topics(cluster,assigned_clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " model.docvecs.index_to_doctag(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TEST CV TALAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_cv_list(nombre):\n",
    "    path = './data/Txt/'\n",
    "    liste_paths = [path+directory for directory in os.listdir(path)]\n",
    "    liste_cv = []\n",
    "    liste_files = []\n",
    "    for path in liste_paths:\n",
    "        filenames = sorted(glob(os.path.join(path,\"*.txt\")))\n",
    "        \n",
    "        for file in filenames[:nombre]:\n",
    "            liste_cv.append(open(file).read())\n",
    "            liste_files.append(file)\n",
    "    return liste_cv,liste_files,liste_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "liste_cv_talan = []\n",
    "path = \"../Mehdi/data_talan\"\n",
    "filenames = sorted(glob(os.path.join(path,\"*.txt\")))\n",
    "for file in filenames:\n",
    "    liste_cv_talan.append(open(file).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "166"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#On supprime les caractères étranges et accents\n",
    "texts_treated_talan = [text_treatment(remove_stopwords(text,stop_words_main)) for text in liste_cv_talan]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "liste_cv_docvec = [tokenize_only(text) for text in texts_treated_talan ]\n",
    "liste_cv_docvec_noNum = [' '.join(cv) for cv in liste_cv_docvec]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aau consultant decisionnel junior sein cabinet talan solution son collectif permettra rejoindre facilement equipe analyse decisionnelle aau consultant junior data visualization tableau software competences outils methodes travail mode agile suivant methodologie scrum aide outil jira communication ecoute entraide travail equipe outils tableau software sql pl-sql methodes agiles scrum ||kanban r academique expertise technique analyse donnees rendu visuel clair creation modification base donnees creation diagrammes uml langages programmation sql t-sql triggers procedures stockees mdx academique html css javascript xml secteurs activite metier telecommunication industrie chimique formations certifications diplome ingenieur genie informatique option business intelligence e.i.s.t.i ecole internationale sciences traitement information cergy experiences ineos france juin projet reecriture logiciel calculs cokage vapocraqueur role mission programmation informatique contributions modification gestion base donnees point avancement regulier differentes avancees proposition idees novatrices ameliorer programme creation diagrammes uml environnement technique vb.net star uml gp2c distribution additifs carburant juillet mise production un internet hebergeur webmaster creation plusieurs maquettes site analyse et epuration contenu souhaite utilisation une api google design css html css javascript aiglon sas leader francais paraffine juillet aide plusieurs secteurs combler departs conges assistant comptabilite realisation inventaire usine realisation un plan acces visiteurs manutention usine mise jour dossiers qualite assistant pendant audit interne'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "liste_cv_docvec_noNum[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def subpart_cv(liste_cv,liste_filenames,part=0):\n",
    "    \"\"\"part can be :\n",
    "            0 - competence\n",
    "            1 - experience\"\"\"\n",
    "    assert part==0 or part==1\n",
    "    liste_sub = []\n",
    "    for i,text in enumerate(liste_cv) :\n",
    "        #si on a le mot experience au début\n",
    "        if \"experience professionnelle\" in text :\n",
    "            liste_sub.append(text.split('experience professionnelle')[part])\n",
    "        elif \"experiences\" in text :\n",
    "            liste_sub.append(text.split('experiences')[part])\n",
    "        else :\n",
    "            liste_filenames.pop(i)\n",
    "    \n",
    "    return liste_sub, liste_filenames\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "liste_competences, filenames = subpart_cv(liste_cv_docvec_noNum,filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "163"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(liste_competences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On en perd 3 => 3 CVs ne contiennent ni \"experiences\" ni experience professionnelle, à traiter..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "158"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents_talan = create_documents_doc2vec(liste_competences,filenames)\n",
    "len(documents_talan.documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On en perd 5 => 5 CVs dupliqués"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ADECV TALAN',\n",
       " 'AGNCV TALAN',\n",
       " 'AKACV TALAN',\n",
       " 'APHCV TALAN',\n",
       " 'APICV TALAN',\n",
       " 'BGHCV TALAN',\n",
       " 'BKICV TALAN',\n",
       " 'BLACV TALAN',\n",
       " 'BLPCV TALAN',\n",
       " 'BPUCV TALAN',\n",
       " 'CAPCV TALAN',\n",
       " 'CLSCV TALAN',\n",
       " 'CV - TalanSolutions - AGU',\n",
       " 'CV - TalanSolutions - CJD',\n",
       " 'CV - TalanSolutions - DPE',\n",
       " 'CV AAU_TS2017',\n",
       " 'CV HME_TS2017',\n",
       " 'CV LKH - TALAN-2017',\n",
       " 'CV NPO',\n",
       " 'CV TalanSolutions ABAH',\n",
       " 'CV TalanSolutions ACH',\n",
       " 'CV TalanSolutions AIQ',\n",
       " 'CV TalanSolutions AMO',\n",
       " 'CV TalanSolutions ANDO',\n",
       " 'CV TalanSolutions ATR',\n",
       " 'CV TalanSolutions BILI',\n",
       " 'CV TalanSolutions CPOU',\n",
       " 'CV TalanSolutions DJE',\n",
       " 'CV TalanSolutions DJS',\n",
       " 'CV TalanSolutions FGA',\n",
       " 'CV TalanSolutions ICHO',\n",
       " 'CV TalanSolutions JBOU',\n",
       " 'CV TalanSolutions LVE',\n",
       " 'CV TalanSolutions MCHI',\n",
       " 'CV TalanSolutions MSME',\n",
       " 'CV TalanSolutions NAB',\n",
       " 'CV TalanSolutions NPRI',\n",
       " 'CV TalanSolutions RGO',\n",
       " 'CV TalanSolutions RGUI',\n",
       " 'CV TalanSolutions RVA',\n",
       " 'CV TalanSolutions SEDA',\n",
       " 'CV TalanSolutions XSUN',\n",
       " 'CV TalanSolutions YGO',\n",
       " 'CV _TalanSolutions_AAG AOUT 2017',\n",
       " 'CV collaborateur_IZA',\n",
       " 'CV collaborateur_TS2017_JAU_201710',\n",
       " 'CV-TalanSolutions_VGA',\n",
       " 'CV-Talan_BIB',\n",
       " 'CV_BD_1',\n",
       " 'CV_BD_2',\n",
       " 'CV_BD_3',\n",
       " 'CV_BD_4',\n",
       " 'CV_BD_6',\n",
       " 'CV_DS_1',\n",
       " 'CV_DS_10',\n",
       " 'CV_DS_11',\n",
       " 'CV_DS_12',\n",
       " 'CV_DS_13',\n",
       " 'CV_DS_14',\n",
       " 'CV_DS_15',\n",
       " 'CV_DS_16',\n",
       " 'CV_DS_2',\n",
       " 'CV_DS_3',\n",
       " 'CV_DS_4',\n",
       " 'CV_DS_5',\n",
       " 'CV_DS_6',\n",
       " 'CV_DS_7',\n",
       " 'CV_DS_8',\n",
       " 'CV_DS_9',\n",
       " 'CV_ETL_1',\n",
       " 'CV_ETL_10',\n",
       " 'CV_ETL_11',\n",
       " 'CV_ETL_12',\n",
       " 'CV_ETL_13',\n",
       " 'CV_ETL_2',\n",
       " 'CV_ETL_3',\n",
       " 'CV_ETL_4',\n",
       " 'CV_ETL_5',\n",
       " 'CV_ETL_6',\n",
       " 'CV_ETL_7',\n",
       " 'CV_ETL_8',\n",
       " 'CV_ETL_9',\n",
       " 'CV_MDM_1',\n",
       " 'CV_MDM_10',\n",
       " 'CV_MDM_2',\n",
       " 'CV_MDM_3',\n",
       " 'CV_MDM_4',\n",
       " 'CV_MDM_5',\n",
       " 'CV_MDM_6',\n",
       " 'CV_MDM_7',\n",
       " 'CV_MDM_8',\n",
       " 'CV_TalanSolutions_AJOL',\n",
       " 'CV_TalanSolutions_FVAL',\n",
       " 'CV_TalanSolutions_JCLE',\n",
       " 'CV_TalanSolutions_JLAG',\n",
       " 'CV_TalanSolutions_MDEB',\n",
       " 'CV_TalanSolutions_WZHA',\n",
       " 'CV_YWA_TS2017',\n",
       " 'DELCV TALAN',\n",
       " 'EJI_CV_20171201',\n",
       " 'ELCCV TALAN',\n",
       " 'EXL Group_CV EXL AFI',\n",
       " 'EXL Group_CV EXL_ECH',\n",
       " 'EXL Group_CV EXL_NIT 2017',\n",
       " 'FGECV TALAN',\n",
       " 'FGICV TALAN',\n",
       " 'FQUCV TALAN',\n",
       " 'GCHCV TALAN',\n",
       " 'GLACV TALAN',\n",
       " 'HABCV TALAN',\n",
       " 'HDUCV Talan',\n",
       " 'HGZCV TALAN',\n",
       " 'HHACV TALAN',\n",
       " 'HMECV TALAN',\n",
       " 'HNOCV TALAN',\n",
       " 'IBNCV Talan',\n",
       " 'JBACV TALAN',\n",
       " 'JBECV TALAN',\n",
       " 'JCOCV TALAN',\n",
       " 'JJUCV TALAN',\n",
       " 'KMACV TALAN',\n",
       " 'LNTCV TALAN',\n",
       " 'LOUCV TALAN',\n",
       " 'MBECV TALAN',\n",
       " 'MHMCV TALAN',\n",
       " 'MRICV TALAN',\n",
       " 'NAJCV ALJANE',\n",
       " 'NGACV TALAN',\n",
       " 'NGUCV Talan',\n",
       " 'NSACV TALAN',\n",
       " 'ORICV TALAN',\n",
       " 'PVNCV TALAN',\n",
       " 'RHICV TALAN',\n",
       " 'RPICV TALAN',\n",
       " 'SBECV TALAN',\n",
       " 'SBRCV TALAN',\n",
       " 'SDECV TALAN',\n",
       " 'SDZCV TALAN',\n",
       " 'SFRCV TALAN',\n",
       " 'SMACV TALAN',\n",
       " 'SPECV TALAN',\n",
       " 'TJACV TALAN',\n",
       " 'Talan - CV ABEN',\n",
       " 'Talan - CV AFAB',\n",
       " 'Talan - CV AGIR',\n",
       " 'Talan - CV ALJN',\n",
       " 'Talan - CV CLEV',\n",
       " 'Talan - CV COUA',\n",
       " 'Talan - CV EGTA',\n",
       " 'Talan - CV IARK',\n",
       " 'Talan - CV MPRO',\n",
       " 'Talan - CV RDJE',\n",
       " 'Talan - CV SBES',\n",
       " 'Talan - CV SSOW',\n",
       " 'Talans solutions_CV Talans_MTR juillet 2017',\n",
       " 'VCACV TALAN',\n",
       " 'VLOCV TALAN',\n",
       " 'WBECV TALAN']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents_talan.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mehdiregina/anaconda/lib/python3.6/site-packages/gensim/models/doc2vec.py:362: UserWarning: The parameter `iter` is deprecated, will be removed in 4.0.0, use `epochs` instead.\n",
      "  warnings.warn(\"The parameter `iter` is deprecated, will be removed in 4.0.0, use `epochs` instead.\")\n",
      "/Users/mehdiregina/anaconda/lib/python3.6/site-packages/gensim/models/doc2vec.py:366: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n",
      "/Users/mehdiregina/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1\n",
      "iteration 2\n",
      "iteration 3\n",
      "iteration 4\n",
      "iteration 5\n",
      "iteration 6\n",
      "iteration 7\n",
      "iteration 8\n",
      "iteration 9\n",
      "iteration 10\n",
      "iteration 11\n",
      "iteration 12\n",
      "iteration 13\n",
      "iteration 14\n",
      "iteration 15\n",
      "iteration 16\n",
      "iteration 17\n",
      "iteration 18\n",
      "iteration 19\n",
      "iteration 20\n",
      "iteration 21\n",
      "iteration 22\n",
      "iteration 23\n",
      "iteration 24\n",
      "iteration 25\n",
      "iteration 26\n",
      "iteration 27\n",
      "iteration 28\n",
      "iteration 29\n",
      "iteration 30\n",
      "iteration 31\n",
      "iteration 32\n",
      "iteration 33\n",
      "iteration 34\n",
      "iteration 35\n",
      "iteration 36\n",
      "iteration 37\n",
      "iteration 38\n",
      "iteration 39\n",
      "iteration 40\n",
      "iteration 41\n",
      "iteration 42\n",
      "iteration 43\n",
      "iteration 44\n",
      "iteration 45\n",
      "iteration 46\n",
      "iteration 47\n",
      "iteration 48\n",
      "iteration 49\n",
      "iteration 50\n",
      "iteration 51\n",
      "iteration 52\n",
      "iteration 53\n",
      "iteration 54\n",
      "iteration 55\n",
      "iteration 56\n",
      "iteration 57\n",
      "iteration 58\n",
      "iteration 59\n",
      "iteration 60\n",
      "iteration 61\n",
      "iteration 62\n",
      "iteration 63\n",
      "iteration 64\n",
      "iteration 65\n",
      "iteration 66\n",
      "iteration 67\n",
      "iteration 68\n",
      "iteration 69\n",
      "iteration 70\n",
      "iteration 71\n",
      "iteration 72\n",
      "iteration 73\n",
      "iteration 74\n",
      "iteration 75\n",
      "iteration 76\n",
      "iteration 77\n",
      "iteration 78\n",
      "iteration 79\n",
      "iteration 80\n",
      "iteration 81\n",
      "iteration 82\n",
      "iteration 83\n",
      "iteration 84\n",
      "iteration 85\n",
      "iteration 86\n",
      "iteration 87\n",
      "iteration 88\n",
      "iteration 89\n",
      "iteration 90\n",
      "iteration 91\n",
      "iteration 92\n",
      "iteration 93\n",
      "iteration 94\n",
      "iteration 95\n",
      "iteration 96\n",
      "iteration 97\n",
      "iteration 98\n",
      "iteration 99\n",
      "iteration 100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<gensim.models.doc2vec.Doc2Vec at 0x1138feb70>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_talan = train_doc2vec(documents_talan,'Doc2vec300_talan.model')\n",
    "model_talan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('CV_YWA_TS2017', 0.9890708923339844), ('CV_DS_2', 0.986083984375), ('CAPCV TALAN', 0.9843279719352722), ('CLSCV TALAN', 0.9839305877685547), ('FGICV TALAN', 0.9823065996170044), ('CV_MDM_10', 0.9819942712783813), ('Talan - CV AGIR', 0.9805023074150085), ('CV_TalanSolutions_FVAL', 0.980362057685852), ('Talan - CV EGTA', 0.980359673500061), ('CV HME_TS2017', 0.978887140750885)]\n"
     ]
    }
   ],
   "source": [
    "sims = model_talan.docvecs.most_similar(\"CV_DS_1\")\n",
    "\n",
    "print(sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assigned_clusters = kmeans_doc2Vec('Doc2vec300_talan.model')\n",
    "for cluster in range(NUM_CLUSTERS):\n",
    "    print(cluster_to_topics(cluster,assigned_clusters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "non concluant"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
