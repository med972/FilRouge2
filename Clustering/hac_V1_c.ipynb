{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "from glob import glob\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "import codecs\n",
    "import unidecode\n",
    "#pip install unidecode\n",
    "import mpld3\n",
    "# pip install mpld3\n",
    "import stop_words\n",
    "# pip install stop-words\n",
    "from nltk import SnowballStemmer, pos_tag, word_tokenize, wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction.text import HashingVectorizer,TfidfTransformer,TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.decomposition import TruncatedSVD, NMF\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.metrics.pairwise import *\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import *\n",
    "from sklearn.semi_supervised import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Lecture des données **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_cv_list(nombre):\n",
    "    path = '../data_indeed/Txt/'\n",
    "    liste_paths = [path+directory for directory in os.listdir(path)]\n",
    "    liste_cv = []\n",
    "    liste_files = []\n",
    "    for path in liste_paths :\n",
    "        if \"informaticien\" and \"dba\" and \"chef_de_projet_informatique\" not in path:\n",
    "            filenames = sorted(glob(os.path.join(path,\"*.txt\")))\n",
    "           \n",
    "            for file in filenames[:nombre]:\n",
    "                liste_cv.append(open(file).read())\n",
    "                liste_files.append(file.split('/')[-1].split('.')[0])\n",
    "    return liste_cv,liste_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "liste_cv_indeed,liste_files = load_cv_list(200)\n",
    "len(liste_cv_indeed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning fonctions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Suppression des sauts de ligne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string,re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regex = re.compile('[%s]' % '(\\\\n)*(\\\\x0c)*')\n",
    "def del_line_feed(s):  \n",
    "    \"\"\"Delete \\n in the text\"\"\"\n",
    "    return regex.sub(' ', s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"informaticien développement et réseaux  développeur intégrateur web  éragny  95  - email me on indeed: indeed.com/r/d7e8913ed00d0384  aujourd’hui, je suis en recherche d'une opportunité sur un poste de développeur ou d’intégrateur web afin de monter toujours plus en compétence et d’approfondir les bases solide que j’ai acquis en formation.  expérience  informaticien développement et réseaux  legrandcercle95  -  éragny  95  -  novembre 2016 - juin 2017  informaticien de l'entreprise, mes missions était de gérer les différent problème dans l'entreprise. mise en place d'un antivirus serveur, sauvegarde nas... créé et gérer les droits sur l'active directory. paramétrer des clients léger ainsi que du matériel informatique comme des imprimantes ou des étiqueteuse en ip fixe.  réajustement du code html et css sur le site grand public selon les normes w3c. création d'une source odbc création d'un code en php - sql afin de récupéré des données librairie sur un site fournisseur pour les enregistré en fiche xml. création de bannières pour les différents évènements avec photoshop et formation d'une personne sur place au logiciel photoshop.  charge de clientele  europcar france commercial  -  saint-ouen-l'aumône  95  -  novembre 2008 - novembre 2016  mes missions principales :   - qualité de service : assurer l'accueil des clients et le respect de la charte en agence.  - gestion des clients : traiter l'ensemble des appels téléphoniques. analyser les besoins du client. assurer le suivi de la clientèle.  - logistique et administratif : s'assurer de la disponibilité des véhicules. gérer l'administratif de l'agence. gestion de la caisse.  - polyvalence   formation développeur intégrateur web  -  septembre 2015 - juin 2016  c.i.f  centre de formation ifocop  8mois   - création de différents sites.  portfolio  - présentation d'un site e-commerce créé de a à z pour l'examen  développeur web  stage   le club des formateurs  -  éragny  95  -  février 2016 - mai 2016  développement d'un site sous wordpress de a à z.  commercial stagiaire  lamy assurance -  novembre 2007 - décembre 2007  stagiaire  port marly accessoires  -  le port-marly  78  -  mai 2007 - juin 2007  les galeries lafayette vendeur  -  2005 - 2006  confection homme  job étudiant  brice vendeur  pap job -  2004 - 2004  étudiant  formation  niveau ii bac+3 bac+4 en développeur intégrateur web  ifocop  -  éragny  95   2015 - 2016   \""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "liste_cv_indeed = [del_line_feed(text).lower() for text in liste_cv_indeed]\n",
    "liste_cv_indeed[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Suppression ponctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"informaticien développement et réseaux  développeur intégrateur web  éragny  95   email me on indeed indeedcomrd7e8913ed00d0384  aujourd’hui je suis en recherche d'une opportunité sur un poste de développeur ou d’intégrateur web afin de monter toujours plus en compétence et d’approfondir les bases solide que j’ai acquis en formation  expérience  informaticien développement et réseaux  legrandcercle95    éragny  95    novembre 2016  juin 2017  informaticien de l'entreprise mes missions était de gérer les différent problème dans l'entreprise mise en place d'un antivirus serveur sauvegarde nas créé et gérer les droits sur l'active directory paramétrer des clients léger ainsi que du matériel informatique comme des imprimantes ou des étiqueteuse en ip fixe  réajustement du code html et css sur le site grand public selon les normes w3c création d'une source odbc création d'un code en php  sql afin de récupéré des données librairie sur un site fournisseur pour les enregistré en fiche xml création de bannières pour les différents évènements avec photoshop et formation d'une personne sur place au logiciel photoshop  charge de clientele  europcar france commercial    saintouenl'aumône  95    novembre 2008  novembre 2016  mes missions principales     qualité de service  assurer l'accueil des clients et le respect de la charte en agence   gestion des clients  traiter l'ensemble des appels téléphoniques analyser les besoins du client assurer le suivi de la clientèle   logistique et administratif  s'assurer de la disponibilité des véhicules gérer l'administratif de l'agence gestion de la caisse   polyvalence   formation développeur intégrateur web    septembre 2015  juin 2016  cif  centre de formation ifocop  8mois    création de différents sites  portfolio   présentation d'un site ecommerce créé de a à z pour l'examen  développeur web  stage   le club des formateurs    éragny  95    février 2016  mai 2016  développement d'un site sous wordpress de a à z  commercial stagiaire  lamy assurance   novembre 2007  décembre 2007  stagiaire  port marly accessoires    le portmarly  78    mai 2007  juin 2007  les galeries lafayette vendeur    2005  2006  confection homme  job étudiant  brice vendeur  pap job   2004  2004  étudiant  formation  niveau ii bac3 bac4 en développeur intégrateur web  ifocop    éragny  95   2015  2016   \""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#le maintient de la ponctuation pertube le stop words, apostrophe gérée dans text_treatment\n",
    "regex = re.compile('[%s]' % re.escape('!\"#$%&\\()*+,-./:;<=>?@[\\\\]^_{|}~')) \n",
    "\n",
    "def del_punct(s):  \n",
    "    \"\"\"Delete punctuation in the text\"\"\"\n",
    "    return regex.sub('', s)\n",
    "\n",
    "#test \n",
    "liste_cv_indeed_no_punc = [del_punct(text) for text in liste_cv_indeed]\n",
    "\n",
    "liste_cv_indeed_no_punc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Reconnaissance du langage du CV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _calculate_languages_ratios(text):\n",
    "    \"\"\"\n",
    "    Calculate probability of given text to be written in several languages and\n",
    "    return a dictionary that looks like {'french': 2, 'spanish': 4, 'english': 0}\n",
    "    \"\"\"\n",
    "\n",
    "    languages_ratios = {}\n",
    "\n",
    "    '''\n",
    "    nltk.wordpunct_tokenize() splits all punctuations into separate tokens\n",
    "    \n",
    "    >>> wordpunct_tokenize(\"That's thirty minutes away. I'll be there in ten.\")\n",
    "    ['That', \"'\", 's', 'thirty', 'minutes', 'away', '.', 'I', \"'\", 'll', 'be', 'there', 'in', 'ten', '.']\n",
    "    '''\n",
    "\n",
    "    tokens = wordpunct_tokenize(text)\n",
    "    words = [word.lower() for word in tokens] #from text get list of word in minuscule\n",
    "\n",
    "    \n",
    "    for language in stopwords.fileids(): # pour chaque langue\n",
    "        stopwords_set = set(stopwords.words(language)) #je mets les stop words du langage dans un set\n",
    "        words_set = set(words) #je mets les mots de mon texte dans un set\n",
    "        #je prends l'intersection entre les mots de mon texte et les mots du stopwords dans le langage donné\n",
    "        common_elements = words_set & stopwords_set\n",
    "        \n",
    "        #je compute mon score comme le nombre d'éléments en communs dictionnaire [langage : score]\n",
    "        languages_ratios[language] = len(common_elements) # language \"score\"\n",
    "\n",
    "    return languages_ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/mehdiregina/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['arabic',\n",
       " 'azerbaijani',\n",
       " 'danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'greek',\n",
       " 'hungarian',\n",
       " 'indonesian',\n",
       " 'italian',\n",
       " 'kazakh',\n",
       " 'nepali',\n",
       " 'norwegian',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'spanish',\n",
       " 'swedish',\n",
       " 'turkish']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "stopwords.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cv_langue(liste_cv, language,cv_names) :\n",
    "    \"\"\"Return resume witten in the specified language in parameter\"\"\"\n",
    "    liste_2 = []\n",
    "    french_cv_names = []\n",
    "    i=0\n",
    "    for cv in liste_cv:\n",
    "        if max(_calculate_languages_ratios(cv),key =_calculate_languages_ratios(cv).get)=='french':\n",
    "            liste_2.append(cv)\n",
    "            french_cv_names.append(cv_names[i])\n",
    "        i+=1\n",
    "    return liste_2,french_cv_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proportion cv french : 0.704\n"
     ]
    }
   ],
   "source": [
    "liste_cv_indeed_fr, liste_files_fr = get_cv_langue(liste_cv_indeed_no_punc,'french',liste_files)\n",
    "nb_cv = len(liste_cv_indeed_no_punc)\n",
    "nb_cv_fr = len(liste_cv_indeed_fr)\n",
    "\n",
    "print(\"proportion cv french :\",1- ((nb_cv-nb_cv_fr)/nb_cv))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1408"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(liste_cv_indeed_fr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Preprocessing du text **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_treatment (text):\n",
    "    text = text.lower()\n",
    "    text = text.replace(\"\\x00\", '').replace(\"\\x01\", '').replace(\"\\x02\", '').replace(\"\\x03\", '') \\\n",
    "    .replace(\"\\x04\", '').replace(\"\\x05\", '').replace(\"\\x06\", '').replace(\"\\x07\", '').replace(\"\\x08\", '') \\\n",
    "    .replace(\"\\x0e\", '').replace(\"\\x11\", '').replace(\"\\x12\", '').replace(\"\\x10\", '').replace(\"\\x19\", '') \\\n",
    "    .replace(\"\\x1b\", '').replace(\"\\x14\", '').replace(\"\\x15\", '').replace('/', '').replace('=', '').replace(\"〓\", \"\") \\\n",
    "    .replace(\"»\", \"\").replace(\"«\", \"\").replace(\"¬\", \"\").replace('`', '').replace (\" -\", \"\").replace(\"•\", \"\")\\\n",
    "    .replace(\"l'\", \"\").replace(\"l’\", \"\").replace(\"l´\", \"\").replace(\"d’\", \"\").replace(\"d'\", \"\").replace(\"d´\",\"\")\\\n",
    "    .replace(\"j’\", \"\").replace(\"j'\", \"\").replace(\"j´\",\"\").replace(\"n’\", \"\").replace(\"n'\", \"\").replace(\"n´\",\"\")\\\n",
    "    .replace(\"”\", \"\").replace(\"~\", \"\").replace(\"§\", \"\").replace(\"¨\", \"\").replace(\"©\", \"\").replace(\"›\", \"\")\\\n",
    "    .replace(\"₋\", \"\").replace(\"→\", \"\").replace(\"⇨\", \"\").replace(\"∎\", \"\").replace(\"√\", \"\").replace(\"□\", \"\")\\\n",
    "    .replace(\"*\", \"\").replace(\"&\", \"\").replace(\"►\", \"\").replace(\"◊\", \"\").replace(\"☞\", \"\").replace(\"#\", \"\")\\\n",
    "    .replace(\"%\", \"\").replace(\"❖\", \"\").replace(\"➠\", \"\").replace(\"➢\", \"\").replace(\"\", \"\").replace(\"✓\", \"\") \\\n",
    "    .replace(\"√\", \"\").replace(\"✔\", \"\").replace(\"♦\", \"\").replace(\"◦\", \"\").replace(\"●\", \"\").replace(\"▫\", \"\")\\\n",
    "    .replace(\"▪\", \"\").replace(\"…\", \"\").replace(\"þ\", \"\").replace(\"®\", \"\").replace('', '').replace(\"...\", \"\")\n",
    "    text = unidecode.unidecode(text) # remove accent\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"informaticien developpement et reseaux  developpeur integrateur web  eragny  95   email me on indeed indeedcomrd7e8913ed00d0384  aujourhui je suis en recherche une opportunite sur un poste de developpeur ou integrateur web afin de monter toujours plus en competence et approfondir les bases solide que ai acquis en formation  experience  informaticien developpement et reseaux  legrandcercle95    eragny  95    novembre 2016  juin 2017  informaticien de entreprise mes missions etait de gerer les different probleme dans entreprise mise en place un antivirus serveur sauvegarde nas cree et gerer les droits sur active directory parametrer des clients leger ainsi que du materiel informatique comme des imprimantes ou des etiqueteuse en ip fixe  reajustement du code html et css sur le site grand public selon les normes w3c creation une source odbc creation un code en php  sql afin de recupere des donnees librairie sur un site fournisseur pour les enregistre en fiche xml creation de bannieres pour les differents evenements avec photoshop et formation une personne sur place au logiciel photoshop  charge de clientele  europcar france commercial    saintouenaumone  95    novembre 2008  novembre 2016  mes missions principales     qualite de service  assurer accueil des clients et le respect de la charte en agence   gestion des clients  traiter ensemble des appels telephoniques analyser les besoins du client assurer le suivi de la clientele   logistique et administratif  s'assurer de la disponibilite des vehicules gerer administratif de agence gestion de la caisse   polyvalence   formation developpeur integrateur web    septembre 2015  juin 2016  cif  centre de formation ifocop  8mois    creation de differents sites  portfolio   presentation un site ecommerce cree de a a z pour examen  developpeur web  stage   le club des formateurs    eragny  95    fevrier 2016  mai 2016  developpement un site sous wordpress de a a z  commercial stagiaire  lamy assurance   novembre 2007  decembre 2007  stagiaire  port marly accessoires    le portmarly  78    mai 2007  juin 2007  les galeries lafayette vendeur    2005  2006  confection homme  job etudiant  brice vendeur  pap job   2004  2004  etudiant  formation  niveau ii bac3 bac4 en developpeur integrateur web  ifocop    eragny  95   2015  2016   \""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#On supprime les caractères étranges, accents et stop words\n",
    "liste_cv_indeed_treated = [text_treatment(text) for text in liste_cv_indeed_fr]\n",
    "#test\n",
    "liste_cv_indeed_treated[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Gestion des stop words **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taille stop words liste :  413\n"
     ]
    }
   ],
   "source": [
    "#generate stopwords\n",
    "stop_words_py = set(stop_words.get_stop_words('french'))\n",
    "\n",
    "# attention certains stop words pourraient être utiles par la suite\n",
    "stopwords_set_manuel = set([\"an\", \"ans\", 'les', 'moins', 'd\\'un','janvier', 'fevrier', 'février', 'mars', 'avril', \\\n",
    "                 'mai', 'juin', 'juillet', 'aout', 'août', 'septembre', 'octobre', 'novembre', 'décembre', \\\n",
    "                  'decembre', 'moins', 'mise', 'universit\\xc3\\xa9', 'université', 'universite', 'ion','sage', \\\n",
    "                  'o', 'rac', 'vers', 'via', 'p\\xc3\\xa9rim\\xc3\\xa8tre', 'périmètre','et','paris','x',\"\\x00\",\\\n",
    "                          \"\\x01\",\"\\x02\", \"\\x03\",\"\\x04\",\"\\x05\",\"\\x06\",\"\\x07\",\"\\x08\",\"\\x09\",\"\\x0e\",\"\\x0e\",\"\\x11\",\\\n",
    "                           \"\\x12\",\"\\x13\",\"\\x14\",\"\\x15\",\"\\x16\",\"\\x17\",\"\\x18\",\"\\x19\",\"transport\",\"puis\",\"lieu\",\\\n",
    "                           \"adresse\",\"entre\",'dun','dune','chez','boulognebillancourt','bt','etc','recrutement','main',\\\n",
    "                           'and', 'paie','paiement','environ','place','france','paris','mois','mobile','mobiles',\\\n",
    "                           'nanterre','source','sources','concerne','concernant','of','non','notes','rh','minimum',\\\n",
    "                           'maximum','bac','site','sites','actuellement','telephone','telephonique','telephoniques','ca','demenager',\\\n",
    "                           'demenagement','participer','participation','lycee','baccalaureat','lien','liens','in',\\\n",
    "                           'indeed','email','indeedcomrd7e8913ed00d0384','aujourhui','afin','toujours','enterprise',\\\n",
    "                           \"guide\",\"10g\",\"11g\",\"9i\",'ad','v10','v2','v3','v5','v6','v8','v9','talan','talansolutions',\\\n",
    "                           \"aide\",\"ainsi\",'aix','aupres','autour','autres','b','bonne','campagnes','cas','chaine',\\\n",
    "                           'choix','coherence','departement','differentes','differents','divers','fin','for','grandes',\\\n",
    "                           'i','ii','jour','lies','lors','lu','mettre','necessaires','national','nationale','nouvelle',\\\n",
    "                           'nouvelles','parle','partir','partie','permettant','permettte','plusieurs','reel','selon',\\\n",
    "                           'temps','toutes','v'])\n",
    "stop_words_main = stop_words_py | stopwords_set_manuel\n",
    "stop_words_main = list(stop_words_main)\n",
    "print(\"taille stop words liste : \", len(stop_words_main))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['avril',\n",
       " 'eussent',\n",
       " 'seras',\n",
       " 'été',\n",
       " 'soyons',\n",
       " 'fut',\n",
       " 'octobre',\n",
       " 'mettre',\n",
       " 'devrons',\n",
       " 'aurait',\n",
       " 'août',\n",
       " 'on',\n",
       " 'devrait',\n",
       " 'fûtes',\n",
       " 'serez',\n",
       " 'demenager',\n",
       " 'valeur',\n",
       " 'v3',\n",
       " 'cela',\n",
       " 'nommé',\n",
       " 'pour',\n",
       " 'dois',\n",
       " 'ayez',\n",
       " 'b',\n",
       " 'o',\n",
       " 'national',\n",
       " 'dun',\n",
       " 'force',\n",
       " 'as',\n",
       " 'lu',\n",
       " 'ceux',\n",
       " 'donc',\n",
       " 'ai',\n",
       " 'haut',\n",
       " 'faisez',\n",
       " 'votre',\n",
       " 'minimum',\n",
       " 'ni',\n",
       " 'j',\n",
       " '\\x0e',\n",
       " 'avant',\n",
       " 'son',\n",
       " 'doit',\n",
       " 'v9',\n",
       " '\\x19',\n",
       " 'ces',\n",
       " 'là',\n",
       " '10g',\n",
       " 'aurais',\n",
       " 'ayons',\n",
       " 'eussiez',\n",
       " 'dù',\n",
       " 'parle',\n",
       " 'fussent',\n",
       " 'sur',\n",
       " 'v6',\n",
       " '\\x15',\n",
       " 'mai',\n",
       " 'comment',\n",
       " 'eut',\n",
       " 'soit',\n",
       " 'x',\n",
       " 'font',\n",
       " 'se',\n",
       " 'differentes',\n",
       " 'quelles',\n",
       " 'est',\n",
       " 'place',\n",
       " 'elles',\n",
       " 'qui',\n",
       " 'v10',\n",
       " 'sans',\n",
       " 'te',\n",
       " 'nanterre',\n",
       " 'entre',\n",
       " 'aurai',\n",
       " '\\x16',\n",
       " 'eux',\n",
       " 'très',\n",
       " 'depuis',\n",
       " 'fin',\n",
       " 'eût',\n",
       " 'paris',\n",
       " '\\x08',\n",
       " 'quelle',\n",
       " 'mot',\n",
       " 'vont',\n",
       " 'decembre',\n",
       " 'fûmes',\n",
       " '\\x02',\n",
       " 'même',\n",
       " 'lies',\n",
       " 'and',\n",
       " 'juste',\n",
       " 'peut',\n",
       " 'email',\n",
       " 'universite',\n",
       " 'nouveau',\n",
       " 'que',\n",
       " 'comme',\n",
       " 'aide',\n",
       " 'tels',\n",
       " 'eus',\n",
       " 'nationale',\n",
       " 'telephonique',\n",
       " 'eûmes',\n",
       " 'me',\n",
       " 'i',\n",
       " 'adresse',\n",
       " 'cet',\n",
       " 'hors',\n",
       " 'serions',\n",
       " 'of',\n",
       " 'universitÃ©',\n",
       " 'selon',\n",
       " 'aupres',\n",
       " 'moi',\n",
       " 'ici',\n",
       " 'departement',\n",
       " 'notre',\n",
       " '\\x03',\n",
       " 'in',\n",
       " 'aies',\n",
       " 'a',\n",
       " 'juin',\n",
       " 'ceci',\n",
       " 'sois',\n",
       " 'vos',\n",
       " '\\x13',\n",
       " 'aurez',\n",
       " 'eu',\n",
       " 'auraient',\n",
       " 'eues',\n",
       " 'aix',\n",
       " 't',\n",
       " 'liens',\n",
       " 'trop',\n",
       " 'il',\n",
       " 'rh',\n",
       " 'leurs',\n",
       " 'septembre',\n",
       " 'serons',\n",
       " \"d'un\",\n",
       " 'vers',\n",
       " 'afin',\n",
       " 'puis',\n",
       " 'ils',\n",
       " 'avons',\n",
       " 'fussions',\n",
       " 'paiement',\n",
       " 'rac',\n",
       " 'l',\n",
       " 'participation',\n",
       " '\\x12',\n",
       " 'devriez',\n",
       " 'quand',\n",
       " 'actuellement',\n",
       " 'seulement',\n",
       " 'en',\n",
       " 'aussi',\n",
       " 'enterprise',\n",
       " 'seriez',\n",
       " 'dune',\n",
       " 'lors',\n",
       " 'voient',\n",
       " 'était',\n",
       " 'coherence',\n",
       " 'transport',\n",
       " 'deux',\n",
       " 'moins',\n",
       " 'bac',\n",
       " 'quel',\n",
       " 'concerne',\n",
       " 'dos',\n",
       " 'devoir',\n",
       " 'fevrier',\n",
       " 'necessaires',\n",
       " 'alors',\n",
       " 'ayant',\n",
       " 'du',\n",
       " 'lui',\n",
       " 'chaque',\n",
       " 'dehors',\n",
       " 'lien',\n",
       " 'cas',\n",
       " 'périmètre',\n",
       " 'eusse',\n",
       " 'leur',\n",
       " 'avoir',\n",
       " 'sage',\n",
       " 'quels',\n",
       " 'où',\n",
       " 'novembre',\n",
       " 'devront',\n",
       " 'maintenant',\n",
       " 'ion',\n",
       " 'je',\n",
       " 'dedans',\n",
       " 'pourquoi',\n",
       " 'soi',\n",
       " 'maximum',\n",
       " 'eûtes',\n",
       " 'autour',\n",
       " 'fussiez',\n",
       " 'nos',\n",
       " 'tes',\n",
       " 'furent',\n",
       " 'faire',\n",
       " 'serai',\n",
       " 'ta',\n",
       " 'ton',\n",
       " 'france',\n",
       " 'telephone',\n",
       " 'car',\n",
       " 'ne',\n",
       " 'eurent',\n",
       " 'ses',\n",
       " 'à',\n",
       " 'nommée',\n",
       " 'devrions',\n",
       " 'êtes',\n",
       " 'site',\n",
       " 'droite',\n",
       " 'partir',\n",
       " 'parole',\n",
       " 'indeedcomrd7e8913ed00d0384',\n",
       " 'nouvelles',\n",
       " 'n',\n",
       " '\\x05',\n",
       " 'mise',\n",
       " 'qu',\n",
       " 'nom',\n",
       " 'tous',\n",
       " 'état',\n",
       " 'permettte',\n",
       " 'aout',\n",
       " 'avions',\n",
       " 'y',\n",
       " 'an',\n",
       " 'université',\n",
       " 'au',\n",
       " '\\x18',\n",
       " 'sujet',\n",
       " 'par',\n",
       " 'paie',\n",
       " 'étiez',\n",
       " 'toujours',\n",
       " 'telephoniques',\n",
       " 'demenagement',\n",
       " 'aurons',\n",
       " 'étais',\n",
       " 'for',\n",
       " 'début',\n",
       " 'soient',\n",
       " 'tellement',\n",
       " 'un',\n",
       " 'ca',\n",
       " 'mois',\n",
       " 'ou',\n",
       " 'fois',\n",
       " 'soyez',\n",
       " 'concernant',\n",
       " 'sera',\n",
       " 'étaient',\n",
       " 'baccalaureat',\n",
       " 'février',\n",
       " 'mobile',\n",
       " '\\x06',\n",
       " 'serait',\n",
       " 'etc',\n",
       " 'participer',\n",
       " 'fût',\n",
       " 'ce',\n",
       " 'être',\n",
       " 'janvier',\n",
       " 'v',\n",
       " 'es',\n",
       " 'mobiles',\n",
       " 'chez',\n",
       " 'parce',\n",
       " 'fusse',\n",
       " '11g',\n",
       " 'mars',\n",
       " 'avaient',\n",
       " 'aie',\n",
       " 'avez',\n",
       " 'seraient',\n",
       " 'auras',\n",
       " 'notes',\n",
       " 'étés',\n",
       " 'tandis',\n",
       " '9i',\n",
       " 'vois',\n",
       " 'lycee',\n",
       " 'des',\n",
       " 'aux',\n",
       " 'la',\n",
       " 'bt',\n",
       " 'fais',\n",
       " 'nouveaux',\n",
       " 'vous',\n",
       " 'eussions',\n",
       " 'non',\n",
       " 'reel',\n",
       " 'main',\n",
       " 'ma',\n",
       " 'mes',\n",
       " 'si',\n",
       " 'ad',\n",
       " 'grandes',\n",
       " '\\x04',\n",
       " 'autres',\n",
       " 'partie',\n",
       " 'permettant',\n",
       " 'fait',\n",
       " 'fus',\n",
       " 'aurions',\n",
       " 'sous',\n",
       " 'juillet',\n",
       " '\\x00',\n",
       " 'boulognebillancourt',\n",
       " 'eusses',\n",
       " '\\x01',\n",
       " 'ont',\n",
       " 'chaine',\n",
       " 'décembre',\n",
       " '\\t',\n",
       " 'toutes',\n",
       " 'aient',\n",
       " 'dès',\n",
       " 'dans',\n",
       " 'nommés',\n",
       " 'ait',\n",
       " 'm',\n",
       " 'une',\n",
       " 'sommes',\n",
       " 'v5',\n",
       " '\\x14',\n",
       " 'étant',\n",
       " 'v8',\n",
       " 'environ',\n",
       " 'sien',\n",
       " 'divers',\n",
       " 'et',\n",
       " 'mon',\n",
       " 'source',\n",
       " 'seront',\n",
       " 'auriez',\n",
       " 'toi',\n",
       " 'talan',\n",
       " 'aucun',\n",
       " 'sont',\n",
       " 'd',\n",
       " 'fusses',\n",
       " 'differents',\n",
       " 'tu',\n",
       " 'jour',\n",
       " 'cette',\n",
       " 'avait',\n",
       " 'tout',\n",
       " 'nous',\n",
       " 'avec',\n",
       " 'plusieurs',\n",
       " 'bonne',\n",
       " 'serais',\n",
       " 'devrez',\n",
       " 'ça',\n",
       " 'sources',\n",
       " 'suis',\n",
       " 'aujourhui',\n",
       " 'ci',\n",
       " 'plupart',\n",
       " 'indeed',\n",
       " 'bon',\n",
       " 'recrutement',\n",
       " 'auront',\n",
       " 'aviez',\n",
       " 'le',\n",
       " 'elle',\n",
       " 'temps',\n",
       " 'vu',\n",
       " 'de',\n",
       " 'faites',\n",
       " 'personnes',\n",
       " 'voit',\n",
       " 'mais',\n",
       " 'guide',\n",
       " 'autre',\n",
       " 'ainsi',\n",
       " 'nouvelle',\n",
       " 'encore',\n",
       " 'sites',\n",
       " 'eue',\n",
       " 'avais',\n",
       " 'étions',\n",
       " 'campagnes',\n",
       " 'lieu',\n",
       " 'pÃ©rimÃ¨tre',\n",
       " '\\x11',\n",
       " 'pas',\n",
       " 'sa',\n",
       " 'choix',\n",
       " 'v2',\n",
       " 'via',\n",
       " '\\x07',\n",
       " 'ans',\n",
       " 'les',\n",
       " '\\x17',\n",
       " 'talansolutions',\n",
       " 'ii',\n",
       " 'peu',\n",
       " 'personne',\n",
       " 'aura']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#voir si utile\n",
    "def remove_stopwords(text,stopwords_list):\n",
    "    text_temp = \" \".join(text.split())+\" \"\n",
    "    for word in stopwords_list:\n",
    "        text_temp = text_temp.replace(\" \"+word+\" \", \" \")\n",
    "    return text_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"informaticien developpement reseaux developpeur integrateur web eragny 95 recherche opportunite poste developpeur integrateur web monter plus competence approfondir bases solide acquis formation experience informaticien developpement reseaux legrandcercle95 eragny 95 2016 2017 informaticien entreprise missions etait gerer different probleme entreprise antivirus serveur sauvegarde nas cree gerer droits active directory parametrer clients leger materiel informatique imprimantes etiqueteuse ip fixe reajustement code html css grand public normes w3c creation odbc creation code php sql recupere donnees librairie fournisseur enregistre fiche xml creation bannieres evenements photoshop formation logiciel photoshop charge clientele europcar commercial saintouenaumone 95 2008 2016 missions principales qualite service assurer accueil clients respect charte agence gestion clients traiter ensemble appels analyser besoins client assurer suivi clientele logistique administratif s'assurer disponibilite vehicules gerer administratif agence gestion caisse polyvalence formation developpeur integrateur web 2015 2016 cif centre formation ifocop 8mois creation portfolio presentation ecommerce cree a z examen developpeur web stage club formateurs eragny 95 2016 2016 developpement wordpress a z commercial stagiaire lamy assurance 2007 2007 stagiaire port marly accessoires portmarly 78 2007 2007 galeries lafayette vendeur 2005 2006 confection homme job etudiant brice vendeur pap job 2004 2004 etudiant formation niveau bac3 bac4 developpeur integrateur web ifocop eragny 95 2015 2016 \""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test \n",
    "liste_cv_indeed_no_stop = [remove_stopwords(text,stop_words_main) for text in liste_cv_indeed_treated]\n",
    "liste_cv_indeed_no_stop[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#SnowballStemmer \n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"french\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_and_stem(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters \n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems\n",
    "\n",
    "\n",
    "def tokenize_only(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters \n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "totalvocab_indeed_stemmed = []\n",
    "totalvocab_indeed_tokenized = []\n",
    "for text in liste_cv_indeed_no_stop:\n",
    "    allwords_stemmed = tokenize_and_stem(text) #for each item in 'synopses', tokenize/stem\n",
    "    totalvocab_indeed_stemmed.extend(allwords_stemmed) #extend the 'totalvocab_stemmed' list\n",
    "    allwords_tokenized = tokenize_only(text)\n",
    "    totalvocab_indeed_tokenized.extend(allwords_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 302629 items in vocab_frame\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>informaticien</th>\n",
       "      <td>informaticien</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>developp</th>\n",
       "      <td>developpement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reseau</th>\n",
       "      <td>reseaux</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>developpeur</th>\n",
       "      <td>developpeur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>integr</th>\n",
       "      <td>integrateur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>web</th>\n",
       "      <td>web</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eragny</th>\n",
       "      <td>eragny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recherch</th>\n",
       "      <td>recherche</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>opportunit</th>\n",
       "      <td>opportunite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>post</th>\n",
       "      <td>poste</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>developpeur</th>\n",
       "      <td>developpeur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>integr</th>\n",
       "      <td>integrateur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>web</th>\n",
       "      <td>web</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mont</th>\n",
       "      <td>monter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>plus</th>\n",
       "      <td>plus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>competent</th>\n",
       "      <td>competence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>approfond</th>\n",
       "      <td>approfondir</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bas</th>\n",
       "      <td>bases</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>solid</th>\n",
       "      <td>solide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acquis</th>\n",
       "      <td>acquis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>format</th>\n",
       "      <td>formation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>experient</th>\n",
       "      <td>experience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>informaticien</th>\n",
       "      <td>informaticien</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>developp</th>\n",
       "      <td>developpement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reseau</th>\n",
       "      <td>reseaux</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>legrandcercle95</th>\n",
       "      <td>legrandcercle95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eragny</th>\n",
       "      <td>eragny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>informaticien</th>\n",
       "      <td>informaticien</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entrepris</th>\n",
       "      <td>entreprise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mission</th>\n",
       "      <td>missions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>saintgobain</th>\n",
       "      <td>saintgobain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>glass</th>\n",
       "      <td>glass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sgg</th>\n",
       "      <td>sgg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avantvent</th>\n",
       "      <td>avantvente</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>realis</th>\n",
       "      <td>realisation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>projet</th>\n",
       "      <td>projet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b2b</th>\n",
       "      <td>b2b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>extranet</th>\n",
       "      <td>extranet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>embarqu</th>\n",
       "      <td>embarquant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>configur</th>\n",
       "      <td>configurateur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>selectic</th>\n",
       "      <td>selectica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intershopenfinity</th>\n",
       "      <td>intershopenfinity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prepar</th>\n",
       "      <td>preparation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>command</th>\n",
       "      <td>commande</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>camion</th>\n",
       "      <td>camions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>transport</th>\n",
       "      <td>transportant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>verr</th>\n",
       "      <td>verre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>distributeur</th>\n",
       "      <td>distributeurs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>projet</th>\n",
       "      <td>projet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>niveau</th>\n",
       "      <td>niveau</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>europ</th>\n",
       "      <td>europe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>interfac</th>\n",
       "      <td>interface</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sap</th>\n",
       "      <td>sap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bap</th>\n",
       "      <td>bapi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>heberg</th>\n",
       "      <td>hebergement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>linkbynet</th>\n",
       "      <td>linkbynet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>equip</th>\n",
       "      <td>equipe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ebusiness</th>\n",
       "      <td>ebusiness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sebastien</th>\n",
       "      <td>sebastien</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>poh</th>\n",
       "      <td>poher</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>302629 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               words\n",
       "informaticien          informaticien\n",
       "developp               developpement\n",
       "reseau                       reseaux\n",
       "developpeur              developpeur\n",
       "integr                   integrateur\n",
       "web                              web\n",
       "eragny                        eragny\n",
       "recherch                   recherche\n",
       "opportunit               opportunite\n",
       "post                           poste\n",
       "developpeur              developpeur\n",
       "integr                   integrateur\n",
       "web                              web\n",
       "mont                          monter\n",
       "plus                            plus\n",
       "competent                 competence\n",
       "approfond                approfondir\n",
       "bas                            bases\n",
       "solid                         solide\n",
       "acquis                        acquis\n",
       "format                     formation\n",
       "experient                 experience\n",
       "informaticien          informaticien\n",
       "developp               developpement\n",
       "reseau                       reseaux\n",
       "legrandcercle95      legrandcercle95\n",
       "eragny                        eragny\n",
       "informaticien          informaticien\n",
       "entrepris                 entreprise\n",
       "mission                     missions\n",
       "...                              ...\n",
       "saintgobain              saintgobain\n",
       "glass                          glass\n",
       "sgg                              sgg\n",
       "avantvent                 avantvente\n",
       "realis                   realisation\n",
       "projet                        projet\n",
       "b2b                              b2b\n",
       "extranet                    extranet\n",
       "embarqu                   embarquant\n",
       "configur               configurateur\n",
       "selectic                   selectica\n",
       "intershopenfinity  intershopenfinity\n",
       "prepar                   preparation\n",
       "command                     commande\n",
       "camion                       camions\n",
       "transport               transportant\n",
       "verr                           verre\n",
       "distributeur           distributeurs\n",
       "projet                        projet\n",
       "niveau                        niveau\n",
       "europ                         europe\n",
       "interfac                   interface\n",
       "sap                              sap\n",
       "bap                             bapi\n",
       "heberg                   hebergement\n",
       "linkbynet                  linkbynet\n",
       "equip                         equipe\n",
       "ebusiness                  ebusiness\n",
       "sebastien                  sebastien\n",
       "poh                            poher\n",
       "\n",
       "[302629 rows x 1 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_frame_indeed = pd.DataFrame({'words': totalvocab_indeed_tokenized}, index = totalvocab_indeed_stemmed)\n",
    "print('there are ' + str(vocab_frame_indeed.shape[0]) + ' items in vocab_frame')\n",
    "vocab_frame_indeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "words    ins\n",
       "Name: in, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_frame_indeed.loc['in']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HAC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag of word tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vect = TfidfVectorizer(stop_words=stop_words_main,max_df=0.8,min_df=0.05,\\\n",
    "                           preprocessor=text_treatment,tokenizer=tokenize_and_stem)\n",
    "bow_idf_indeed = tf_vect.fit_transform(liste_cv_indeed_no_stop)\n",
    "l2_norm = make_pipeline(Normalizer(copy=False))\n",
    "bow_idf_indeed = l2_norm.fit_transform(bow_idf_indeed)\n",
    "\n",
    "vocab_liste_indeed = tf_vect.get_feature_names()\n",
    "\n",
    "#Ajout une étape pour supprimer les doublons & shuffle\n",
    "buffer = pd.DataFrame(data=bow_idf_indeed.toarray())\n",
    "buffer = shuffle(buffer)\n",
    "buffer.drop_duplicates(inplace=True)\n",
    "\n",
    "new_idx = buffer.index\n",
    "bow_idf_indeed = buffer.values\n",
    "liste_files_new = []\n",
    "for idx in new_idx:\n",
    "    liste_files_new.append(liste_files_fr[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 3, ..., 1, 0, 1])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hac = AgglomerativeClustering(n_clusters=5, affinity='euclidean', compute_full_tree='auto', linkage='ward')\n",
    "hac.fit(bow_idf_indeed)\n",
    "\n",
    "hac.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Label\n",
       "0    552\n",
       "1    351\n",
       "2     74\n",
       "3     86\n",
       "4    130\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(hac.labels_, columns=[\"Label\"]).groupby([\"Label\"])[\"Label\"].size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find topics behind each label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#select randomly n doc idx in each cluster\n",
    "#for each doc return n strongest tf-idf\n",
    "dict_rand_word_label = dict()\n",
    "for label in np.unique(hac.labels_):\n",
    "    idx_label = np.where(hac.labels_==label)[0]\n",
    "    idx_rand = np.random.choice(idx_label,size=(20))\n",
    "    liste_cluster_word = []\n",
    "    for idx in idx_rand:\n",
    "        text = \" \"\n",
    "        idx_ordered = np.argsort(bow_idf_indeed[idx])[::-1]\n",
    "        for j in range(0, 4):  # nombre de motss\n",
    "            text += vocab_frame_indeed.loc[vocab_liste_indeed[idx_ordered[j]]].values.tolist()[0][0] + \" \"\n",
    "        liste_cluster_word.append(text)\n",
    "    dict_rand_word_label[\"cluster\"+str(label)] = liste_cluster_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' referentiel chargee courant suivi ',\n",
       " ' migrations audit oeuvre techniques ',\n",
       " ' configuration visual plateforme studio ',\n",
       " ' marketing animation responsabilites stage ',\n",
       " ' ecole commerce negociation telecom ',\n",
       " ' deploiements finance pays projets ',\n",
       " ' ibm developpement e administratif ',\n",
       " ' controle assistant commande gestion ',\n",
       " ' marketing data analyser strategie ',\n",
       " ' resultat optimisation organisations amelioration ',\n",
       " ' fournisseur industrie entreprise charge ',\n",
       " ' filiale suivi entites budget ',\n",
       " ' risques credit pilotage responsable ',\n",
       " ' publications affaires politique veille ',\n",
       " ' groupes politique management developpement ',\n",
       " ' entretien it profils selection ',\n",
       " ' centrale qualite activite ecole ',\n",
       " ' business assistant clients preparateur ',\n",
       " ' gestionnaires saisie personnel sociale ',\n",
       " ' definition business projets international ']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_rand_word_label[\"cluster0\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ACP X HAC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimension reduction before applying HAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance of the SVD step: 77%\n"
     ]
    }
   ],
   "source": [
    "# implement LSA\n",
    "lsa_number = 170\n",
    "svd = TruncatedSVD(lsa_number)\n",
    "bow_idf_reduced_lsa = svd.fit_transform(bow_idf_indeed)\n",
    "explained_variance = svd.explained_variance_ratio_.sum()\n",
    "print(\"Explained variance of the SVD step: {}%\".format(int(explained_variance * 100)))\n",
    "\n",
    "hac = AgglomerativeClustering(n_clusters=5, affinity='euclidean', compute_full_tree='auto', linkage='ward')\n",
    "hac.fit(bow_idf_reduced_lsa)\n",
    "labels_hac_lsa = hac.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Label\n",
       "0    386\n",
       "1    381\n",
       "2    134\n",
       "3     46\n",
       "4    246\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(hac.labels_, columns=[\"Label\"]).groupby([\"Label\"])[\"Label\"].size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find topic behind each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict_rand_word_label = dict()\n",
    "for label in np.unique(labels_hac_lsa):\n",
    "    idx_label = np.where(labels_hac_lsa==label)[0]\n",
    "    idx_rand = np.random.choice(idx_label,size=(20))\n",
    "    liste_cluster_word = []\n",
    "    for idx in idx_rand:\n",
    "        text = \" \"\n",
    "        idx_ordered = np.argsort(bow_idf_indeed[idx])[::-1]\n",
    "        for j in range(0, 4):  # nombre de motss\n",
    "            text += vocab_frame_indeed.loc[vocab_liste_indeed[idx_ordered[j]]].values.tolist()[0][0] + \" \"\n",
    "        liste_cluster_word.append(text)\n",
    "    dict_rand_word_label[\"cluster\"+str(label)] = liste_cluster_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' commercial presentation offres detecter ',\n",
       " ' moe personnel gestionnaires alternance ',\n",
       " ' europe responsable vente commercial ',\n",
       " ' humaines prospection ressources niveau ',\n",
       " ' alternance locaux marches expertise ',\n",
       " ' financiers audit specialisation banque ',\n",
       " ' clients plus clientele equipe ',\n",
       " ' marketing analytics performances digital ',\n",
       " ' new digital to media ',\n",
       " ' commercial business directeur partenariat ',\n",
       " ' controle comptabilite suivi comptable ',\n",
       " ' entretien crm suivi reporting ',\n",
       " ' filiale suivi entites budget ',\n",
       " ' marketing erp offres responsable ',\n",
       " ' gestionnaires finance structure ecommerce ',\n",
       " ' economie finance flux comptable ',\n",
       " ' comptabilite responsabilites fournisseur comptable ',\n",
       " ' innovantes bilan economie developpement ',\n",
       " ' achats complets definition principales ',\n",
       " ' marketing strategie direction marques ']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_rand_word_label[\"cluster1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CV TALAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "632\n"
     ]
    }
   ],
   "source": [
    "#download talan CVs \n",
    "liste_cv_talan = []\n",
    "liste_files_talan = []\n",
    "path = '../Data_talan/txt/'\n",
    "filenames = sorted(glob(os.path.join(path,\"*.txt\")))\n",
    "print(len(filenames))\n",
    "for file in filenames:\n",
    "    liste_cv_talan.append(open(file).read())\n",
    "    liste_files_talan.append(file.split('/')[-1].split('.')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Secteurs Télécom & média Activités métier Avant-vente, Développement Intégration & validation. Compétences fonctionnelles CRM : Elaboration de réponses aux appels d’offres, conception des processus métiers, rédaction des spécifications fonctionnelles, préparations des maquettes. Compétences techniques Microsoft Dynamics CRM 2011,2013 Salesforce CRM Architecture et plan de migration (volumétrie importante) Service Director : solution de QOS Oblicore Guarantee : solution SLM Développement : J2EE (EJB3, JMS), Webservice, API, Javascript,C#. Base de données : SQL Server, Oracle, PostgreSQL, MySQL Méthodologie Agile: Scrum, Extreem programing, Sure Step , UML. Atos France Bull France CRM Dynamics 2013, Salesforce CRM Dynamics 2011 Avant-vente CRM : Elaboration de réponses aux appels d’offres, conception des processus métiers, rédaction des spécifications fonctionnelles, préparations des maquettes SLM : Gestion des contrats SLM (Service Level Management) Prise de commande Télécom (SI) Processus métiers transport de fret routier Processus métier collectivité territoriale citoyens 360° Métiers Télécom SI : Provisioning, Médiation, Activation Orange France SLA réseau mobile Gestion du contrat SLM (Service Level Management) sur le produit Oblicore (BSI) Mobistar Covage Service Quality Management évolution sur le modèle Mobistar SI : Interface de prise de commande Orange France Activation Des Terminaux SIP Phones 2015 2015 2013 2012 2012 2012 2011 2008 Formation CRM Salesforce Application Builder 401 Formation CRM Dynamics 2013 Formation Adobe Campaign (Neolane) Certification Microsoft Dynamics CRM 2011 Applications Certification Microsoft Dynamics CRM 2011 Customization and Configuration Certification Microsoft Dynamics CRM 2011 Installation and deployment Formation CRM (théorique et technique), CRM Dynamics 2011, Formation Marketing Formation Méthode de gestion de projet SCRUM Formation Microsoft SURE STEP Formation UML de 5 jours 2009 2008 Master 2 Télécom et Réseau (Orsay) Formation UML 5 jours 2007 Ingénieur Généraliste en Informatique (BAC+5) USTHB (Université des Sciences et de la Technologie HOUARI BOUMEDIENE) Français Bilingue Anglais Lu, Ecrit &Parlé Arabe Langue maternelle Avril 2016 ATOS – Société Générale ALD I -Consultante CRM –International Projet CRM Salesforce. Rôle Consultante Technico-fonctionnelle, Deployment Manager par Intérim, Référente Technique Déploiement Contexte & Mission Projet CRM Salesforce : CRM Salesforce remarketing automation projet en mode agile Réalisations Auditer les Paramétrages sur la solution existante Auditer le code Apex : vérifier l’application des bonnes pratiques Définir le RACI et les prérequis pour chaque étape avec le client, participer à définir le planning de déploiement et la Roadmap pour 13 pays Définir avec le client les documents à utiliser pour le déploiement : Config kit, les Templates, Outils de ticketing..ect Contribuer sur les différentes tâches d’initialisation du projet : Kick-off présentations, animer les workshops déploiements Participer à la récolte des besoins pays (Pre-study) : Slovaquie et république tchèque Animer les workshops pour récupérer les inputs techniques avec les responsables IT des pays Organiser le démarrage de déploiement pays : Chiffrage, Budget, vérifier les prérequis, donner l’OK pour le démarrage Paramétrer (Paramétrage Core) sur Salesforce pour déployer la Slovaquie et la république tchèque : Fields, profils, rôles, Outlook for Salesforce...ect Gérer la migration des données depuis les applications existantes Gérer l’équipe déploiement Environnement technique CRM salesforce,Eclipse, Outlook for salesforce, Migration tool Mars 2015 Mars 2016 ATOS– RENAULT en régie -Consultante CRM –France Consultante Technico-fonctionnelle Projet CRM Salesforce : CRM Salesforce (SFA Sales Force automation, LMT Lead Management tools) : projet en mode agile, Référentiel Prospect/Client déployé internationalement, pour les différents concessionnaires Renault. Interfaçages avec les applications métiers. Consultante technico-Fonctionnelle. Paramétrage sur SalesForce : Profil, Role , Record Type, page layout..ect Développer sur le module de gestion des Leads LMT-Salesforce : Développer un moteur de Fidélisation des clients Développer des Notifications Email en changeant le Template Développer des WebService de Synchronisation des Opportunités/commandes avec les applications métiers. Auditer le code Apex : vérifier l’application des bonnes pratiques Salesforce Participer à la rédaction des contrats d’interface, Implémentation de WebServices, Batch et trigger. Participer à la rédaction des jeux de tests. Qualifier fonctionnellement et effectuer les tests de validation. Support technique. CRM salesforce, Web Services (SOAP), Apex, SOQL,SOAPUI,Eclipse,OAuth2 Avril 2014 au Janvier 2015 ATOS -OBS/Véolia - M2M médiation- Responsable qualité/Recette/Intégration – France IDF M2M : médiation M2O, client OBS/Véolia Responsable qualité/Recette/Intégration Intégration projet M2O Médiation Participer à la rédaction des spécifications générales d’une très importante évolution (V4) (ajout de 13 flux de médiation). Coaching d’une équipe de test de 3 personnes durant les réalisations des évolutions. Préparer les livraisons : Releases, Procédure et manuel d’installation, manuel d’exploitation. Rédiger le cahier de recette. Participer à la rédaction des JEV Qualifier fonctionnellement et effectuer les tests de validation Garantir le support technique. Analyser technico-fonctionnellement les impacts des demandes d’évolution , analyser et auditer à la demande du client les problèmes de performances . Intégration : paramétrage. Effectuer la migration d’une partie de la BDD Oracle de la PF de production vers la PF de pré-production . Animer les instances et workshop du projet avec le client (COPROJ quotidien): présentation, synthèse, état d’avancement. Participer aux réunions quotidiennes d’exploitation point de suivi PRODUCTION animé par le client. Qualifications fonctionnelles et tests de validation. Automatiser les tests, en élaborant et implémentant des scripts Bash entièrement configurables. Participer aux tests de benchemarking. M2M, Scripting Bash, AIX, Platine (mediation), Administration BDD Oracle 11g, Middleware : Tuxedo oracle Mars 2014 au Mai 2014 BULL -INERIS - M2M-BigData– Ingénieur Développeur/testeur– France IDF GEOD’AIR , client INERIS Ingénieur Développeur/testeur Statistiques sur la pollution d’air avec les normes européennes Développement en Java des modules de calcul statistiques de la qualité d'air (collecte des données BD NoSQL). Participation à la rédaction des JEV. Qualifications fonctionnelles et tests de validation. M2M, BigData, MongoDB, BD NoSql, PostgreSQL,Java Déc. 2013 Au févr. 2014 3 mois Bull France –Avant- vente -France SI/CRM Dynamics 2013. Responsable Avant-vente Avant-vente Elaboration de réponses aux appels d’offres clients : Participation à la construction d’offres : CRM, Plate-forme de service ,architecture M2M, Tierce intégration. Etude des différentes solutions techniques pour une architecture SI et plate-forme de service. Elaboration de réponses : plans de migration et stratégie de reprise de données avec une volumétrie importante (opérateurs télécoms) Participation à la rédaction des spécifications fonctionnelles et techniques des propositions. CRM Dynamics 2013,ADFS2.0,OAuth2, BD NoSql, MOM : DDS , AMQP, JMS,Rest, OSGi,OSGi-Me , J2ME Embarquée. Depuis janv 2013 Mobinil Egypt –BULL TMA TMA Mobinil Ingénieur Développeur Support Provisioning pour l’opérateur Telecom Mobinil. Support technique. Différentes activités internes sur le projet : préparation de la migration vers la nouvelle version KPSA 3.7, synchronisation des données entre deux serveurs de développements KPSA3.5, KPSA3.7, SVN, Linux Red Hat, Oracle 10, Oracle11, Scripts shell. juin 2013 à fin juin 2013 Pôle Emploi - BULL – Avant-vente – France Pôle Emploi – Avant-vente – France Avant-vente SLM Pole Emploi Réalisation d’un POC SLM autour de la solution BSI. Mise en place : d’un adaptateur, des contrats, des rapports et un tableau de bord. Présentation du POC au client, (répartie sur deux workshops). BSI 8.1, Oracle 11, IIS7.. Mai 2012 à 2013 Avant-vente : CRM Dynamics 2011 Secteur Public et privé Ingénieur Développement Elaboration de réponses aux appels d’offres clients : Participation à la construction d’offres : CRM, Tierce intégration. Rédaction des spécifications fonctionnelles et techniques de la solution. Préparation de la plate-forme de développement : installation et configuration : Windows Server 2R, Active Directory, SQL Server 2008, SQL Server Reporting Service 2008, CRM Dynamics 2011. Développement de deux maquettes: personnalisation MS Dynamics, développement Java script, développement Web service, plugins c#. CRM Dynamics 2011 , Composants de liste (list component) pour SharePoint, Visual studio 2010, Windows Server 2K,Active Directory,SQL Server 2008, SQL Server Reporting Service 2008, ,IIS7 ,SharePoint. De Juillet 2011 à Avril 2012 Orange –BULL -France SLA réseau mobile Lot 1, Lot2 Gestion du contrat SLM (Service Level Management) sur le produit Oblicore Participation aux Workshops avec le client Intégration et développement sur l’outil de « Service Level management» BSI (anciennement Oblicore) : intégration des données, rapports Développements en .net : Modifications sur l’IHM de prétraitement de données Etude et rédaction de documentation techniques sur les APIs de la solution BSI. Rédaction des rapports associés, rédaction de guide d’utilisation de la solution. Support technique BSI , Visual Basic, Oracle 10, IIS7, Visual studio 2010 De Mars 2011 à Avr 2011 COVAGE –BULL -France COVAGE SI Ingénieur Développement J2EE Interface de prise de commande Covage SI. Développeur. : Évolution de l’interface graphique de l’application web « Prise de Commande ». Environnement technique Eclipse, JOnAS, MySQL ,Hibernate , J2EE (JSP, JSF), MAVEN, spring, SVN. De Juin 2011 à Iuil 2011 ORANGE CAMEROUN –BULL -France P2P ORANGE CAMEROUN Ingénieur Développement J2EE P2P ORANGE CAMEROUN Développeur. Évolution du « Mapping Externe » , en utilisant les bonnes pratiques de développement . Développement du parsing XMl , Servlet Développement des scripts de configuration en Groovy Tests unitaire avec Junit Eclipse, JOnAS, MySQL, J2EE , MAVEN, Spring, Groovy, SVN. De Janv 2011 à Mars 2011 Mobistar –BULL -France Service Quality Management(SQM) évolution sur le modèle Mobistar Projet d’évolution sur produit Service Director de gestion de la QOS Développement des schémas Modélisation Définition des plans de tests Validation de ces schémas Support technique Service Director ,Oracle,XML Scripts Pyton,Sripts Shell De Sept 2010 à Déc 2010 Orange –BULL –France ADT SIP Phones Projet de mise en place d’une plateforme d’activation des terminaux (téléphones VOIP) Développement et Tests des web services (J2EE, SOAP), pour s’interfacer avec des équipements: SIP Phones Validation des web services Conception et modélisation de la partie persistance : base de données J2EE, SOAP, Jonas4, PostgreSQL, SVN, Eclipse De Avr 2010 à Sept 2010 Bull télécom et Média France Stage Conception et Réalisation : Auto Configuration Server ACS Conception et Développement d'un ACS sur la base d'une architecture J2EE et des composants open source. Eclipse, JOnAS, PostgreSQL, J2EE (JSP,JSF, EJB3) SOAP,JMS, SVN. Norme :TR-069. De Fevr 2008 à juil 2009 SNTR Groupe - Algérie ERP Gestion de Fret routier Ingénieur Responsable ERP Responsable ERP Gestion de Fret routier chez le Leader du transport de marchandise et d’hydrocarbure en Algérie . Filiale : Logitrans Maintenance Plus..ect Mise en place et pilotage des évolutions sur la solution. Mise en place des Scripts oracle pour la gestion des recettes. Unification des bases de données des différents sites et unités, le déploiement sur les sites pour les 42 Wilayas de l’ Algérie . Etablir les Testes fonctionnelles 6 Sigma, Oracle,ITIL De Avr 2007 à Sept 2008 Askesis consulting - Algérie Ingénieur Recherche et Développement Etude de plusieurs ERP Open Source, Développement et personnalisation sur des solutions Open source. Etude des fonctionnalités de plusieurs logiciels open source. Conception et implémentation Plug-in d’authentification (c#) sur un projet pour la société Algérienne des Sacs Enduits : Spa (SASACE). Administration réseaux, mise en place serveur Proxy, serveur antiviral Kaspersky Server. OS Linux, JEE, Ajax : GWT, EJB, SOA, BPM, Eclipse, Netbeans, JBoss, C#.net, Scrum, Extreme Programming, ITIL . De Mars 2007 à Sept 2007 Algérie Télécom – Stage Sécurité Réseau – Algérie Stage de fin d’études Stage de Sécurité et réseau à Algérie Télécom (stage de fin d’études) PFE Contribution à la détection d’intrusions, application à Snort sous Linux. L’ajout de deux préprocesseurs de détection à Snort ( langage C ) . Optimisation de la structure des règles Snort (langage C ) . Conception et réalisation d’une interface graphique à Snort (langage JAVA ). Java, JCreator, AWT, Swing , C, Linux Fedora core 4, Snort ,TCP/IP ACH Consultante Senior 9 ans d’expérience Domaines Majeurs de Compétences Références Significatives Formations Langues Expériences professionnelles\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "liste_cv_talan[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#suppression des saut de lignes\n",
    "liste_cv_talan = [del_line_feed(text).lower() for text in liste_cv_talan]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#suppression de la ponctuation\n",
    "liste_cv_talan_no_punc = [del_punct(text) for text in liste_cv_talan]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proportion cv french : 0.9636075949367089\n"
     ]
    }
   ],
   "source": [
    "#selectionner seulement cvs fr\n",
    "liste_cv_talan_fr, liste_files_talan_fr = get_cv_langue(liste_cv_talan_no_punc,'french',liste_files_talan)\n",
    "\n",
    "nb_cv = len(liste_cv_talan_no_punc)\n",
    "nb_cv_fr = len(liste_cv_talan_fr)\n",
    "\n",
    "print(\"proportion cv french :\",1- ((nb_cv-nb_cv_fr)/nb_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#On supprime les caractères étranges, accents et stop words\n",
    "liste_cv_treated_talan = [text_treatment(text) for text in liste_cv_talan_fr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#remove stop word\n",
    "liste_cv_talan_no_stop = [remove_stopwords(text,stop_words_main) for text in liste_cv_treated_talan]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"competences sectorielles finance sante serveurs applications progiciels reuters powerplus pro methodologies xml langages outils developpement sql visual basic html materiel systemes exploitation windows nt bases donnees relationnelles oracle consultant junior formation ecole ingenieur ecole internationale sciences traitement information e s genie mathematique option ingenierie financiere competences experience personelle sejour linguistique famille ecossaise approfondissement anglais decouverte milieu equestre experience professionnelle stage pole bourse etrangere procapital - projet gestion execution operations financieres objet projet controle gestion execution transactions financieres controle trades etrangers depositaires marches rapprochement quotidien clients - marches etrangers collecte transactions passees fortis merrill lynch forme fichiers extraits ifac base donnees controle transactions passees fortis merrill lynch gestion anticipation achat vente cash devises extraction informations cash plates formes fortis custody merrill lynch integration informations comptes reflet representant operations devises controle prevision cash cash devise ajout comptes cash devises ordres achat vente action obligation routes systeme brokers externes anticipation cash devise gestion execution operations titres etrangers passees brokers cash ordre passe instructions reglement livraison depositaire depouillement operation comptes clients brokers reflets gestion execution transferts valeurs etrangeres entrants sortants environnement technique windows nt microsoft office excel vba stage assistant gerant quilvest banque privee - projet assistance equipe gestion actifs objet projet assistance gerants documentation presentation quilvest projet homogeneisation documentation presentation quilvest banque privee futurs investisseurs clients powerpoint preparation presentation morning suivi redaction clients realisation reportings fonds amelioration creation reportings presentations comptes resultats societes suivi synthese comptes resultats passage ordres billets tresorerie fonds analyse ponctuelle entreprises cotees definition limites introduction eventuelle liste valeurs extraction analyses reuters environnement technique windows nt microsoft office excel vba reuters powerplus pro powerpoint stage ministere sante - projet evolution systeme information travers portail web objet projet assistance evolution systeme information plans regionaux sante publique travers portail web briques extractions informations base donnees systeme information biais requetes sql presentation rapports extractions creation tableaux croises dynamiques reporting presentations powerpoint elaboration cahier charges fonctionnel version logiciel elaboration nomenclature version recette fonctionnelle environnement technique windows nt microsoft office excel sql powerpoint langues anglais courant espagnol niveau intermediaire centres interets sports champion de volley ball voile planche voile velo equitation vie associative vice president voieisti course spi dauphine course edhec secretaire club investissement eisti'mat responsable volley ball bureau sports bds \""
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#facultatif add only for talan cv (delete numbers) -> could be use for the preprocessing in general !\n",
    "liste_cv_talan_clean = [re.sub('[0-9 ]+', ' ', text) for text in liste_cv_talan_no_stop]\n",
    "liste_cv_talan_clean[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalvocab_stemmed_talan = []\n",
    "totalvocab_tokenized_talan = []\n",
    "for text in liste_cv_talan_no_stop:\n",
    "    allwords_stemmed = tokenize_and_stem(text) #for each item in 'synopses', tokenize/stem\n",
    "    totalvocab_stemmed_talan.extend(allwords_stemmed) #extend the 'totalvocab_stemmed' list\n",
    "    allwords_tokenized = tokenize_only(text)\n",
    "    totalvocab_tokenized_talan.extend(allwords_tokenized)\n",
    "\n",
    "vocab_frame_talan = pd.DataFrame({'words': totalvocab_tokenized_talan}, index = totalvocab_stemmed_talan)\n",
    "print('there are ' + str(vocab_frame_talan.shape[0]) + ' items in vocab_frame')\n",
    "vocab_frame_talan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HAC Talan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Representation bag of word tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF IDF BOW Representation\n",
    "tf_vect = TfidfVectorizer(stop_words=stop_words_main,max_df=0.7,min_df=0.1,\\\n",
    "                           preprocessor=text_treatment,tokenizer=tokenize_and_stem)\n",
    "bow_idf_talan = tf_vect.fit_transform(liste_cv_talan_clean)\n",
    "\n",
    "\n",
    "vocab_liste_talan = tf_vect.get_feature_names()\n",
    "\n",
    "#Ajout une étape pour supprimer les doublons & shuffle\n",
    "buffer = pd.DataFrame(data=bow_idf_talan.toarray())\n",
    "buffer = shuffle(buffer)\n",
    "buffer.drop_duplicates(inplace=True)\n",
    "\n",
    "new_idx = buffer.index\n",
    "bow_idf_talan = buffer.values\n",
    "liste_files_new = []\n",
    "liste_cv_clean_2 = []\n",
    "for idx in new_idx:\n",
    "    liste_files_new.append(liste_files_talan_fr[idx])\n",
    "    liste_cv_clean_2.append(liste_cv_talan_clean[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hac = AgglomerativeClustering(n_clusters=5, affinity='euclidean', compute_full_tree='auto', linkage='ward')\n",
    "hac.fit(bow_idf_talan)\n",
    "\n",
    "pd.DataFrame(hac.labels_,columns=[\"Label\"]).groupby([\"Label\"])[\"Label\"].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict_rand_word_label = dict()\n",
    "for label in np.unique(hac.labels_):\n",
    "    idx_label = np.where(hac.labels_==label)[0]\n",
    "    idx_rand = np.random.choice(idx_label,size=(12))\n",
    "    liste_cluster_word = []\n",
    "    for idx in idx_rand:\n",
    "        text = \" \"\n",
    "        idx_ordered = np.argsort(bow_idf_talan[idx])[::-1]\n",
    "        for j in range(0, 4):  # nombre de motss\n",
    "            text += vocab_frame_talan.loc[vocab_liste_talan[idx_ordered[j]]].values.tolist()[0][0] + \" \"\n",
    "        liste_cluster_word.append(text)\n",
    "    dict_rand_word_label[\"cluster\"+str(label)] = liste_cluster_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_rand_word_label[\"cluster1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ACP X HAC TALAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement LSA\n",
    "lsa_number = 60\n",
    "svd = TruncatedSVD(lsa_number)\n",
    "bow_idf_reduced_talan = svd.fit_transform(bow_idf_talan)\n",
    "explained_variance = svd.explained_variance_ratio_.sum()\n",
    "print(\"Explained variance of the SVD step: {}%\".format(int(explained_variance * 100)))\n",
    "\n",
    "hac = AgglomerativeClustering(n_clusters=5, affinity='euclidean', compute_full_tree='auto', linkage='ward')\n",
    "hac.fit(bow_idf_reduced_talan)\n",
    "labels_hac_lsa = hac.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(labels_hac_lsa, columns=[\"Label\"]).groupby([\"Label\"])[\"Label\"].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict_rand_word_label = dict()\n",
    "for label in np.unique(labels_hac_lsa):\n",
    "    idx_label = np.where(labels_hac_lsa==label)[0]\n",
    "    idx_rand = np.random.choice(idx_label,size=(12))\n",
    "    liste_cluster_word = []\n",
    "    for idx in idx_rand:\n",
    "        text = \" \"\n",
    "        idx_ordered = np.argsort(bow_idf_talan[idx])[::-1]\n",
    "        for j in range(0, 4):  # nombre de motss\n",
    "            text += vocab_frame_talan.loc[vocab_liste_talan[idx_ordered[j]]].values.tolist()[0][0] + \" \"\n",
    "        liste_cluster_word.append(text)\n",
    "    dict_rand_word_label[\"cluster\"+str(label)] = liste_cluster_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_rand_word_label[\"cluster3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_label = np.where(labels_hac_lsa==3)[0]\n",
    "idx_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liste_cv_clean_2[99]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
