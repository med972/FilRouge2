{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copie de Mehdi - ne pas enregistrer dessus (génération de conflits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "from glob import glob\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "import codecs\n",
    "import unidecode\n",
    "#pip install unidecode\n",
    "import mpld3\n",
    "# pip install mpld3\n",
    "import stop_words\n",
    "# pip install stop-words\n",
    "from nltk import SnowballStemmer, pos_tag, word_tokenize, wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction.text import HashingVectorizer,TfidfTransformer,TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import TruncatedSVD, NMF\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.metrics.pairwise import *\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import *\n",
    "from sklearn.semi_supervised import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' anzoanfr tknernd,o/e;id,d;zo,'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" ANZOANfr tknernd,o/E;id,d;zo,\".lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Lecture des données ** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objectif garder uniquement les parties contenant les compétences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Suppression des sauts de ligne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string, re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def del_line_feed(s):  \n",
    "    \"\"\"Delete \\n in the text\"\"\"\n",
    "    regex = re.compile('[%s]' % '(\\\\n)*(\\\\x0c)*(\\\\uf0a7)*')\n",
    "    return regex.sub(' ', s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Suppression ponctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#le maintient de la ponctuation pertube le stop words, apostrophe gérée dans text_treatment\n",
    "regex = re.compile('[%s]' % re.escape('!\"#$%&\\()*+,-./:;<=>?@[\\\\]^_{|}~')) \n",
    "\n",
    "def del_punct(s):  \n",
    "    \"\"\"Delete punctuation in the text\"\"\"\n",
    "    return regex.sub(' ', s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CV:\n",
    "    def __init__(self, filename, title, cv, doc_type):\n",
    "        self.filename = filename\n",
    "        self.title = title\n",
    "        self.cv = cv\n",
    "        self.doc_type = doc_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ao_delete_context_part(ao):\n",
    "    \"\"\"Supprime la partie de l'appel d'offre consacrée au contexte pour ceux qui suivent le pattern SNCF\"\"\"\n",
    "    split_ao = ao.split('DESCRIPTION DE LA MISSION')\n",
    "    if len(split_ao)>1:\n",
    "        ao_light = split_ao[1] \n",
    "    else :\n",
    "        ao_light = split_ao[0]\n",
    "    return ao_light"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ao_delete_contact_part(ao):\n",
    "    \"\"\"Supprime la partie de l'appel d'offre consacrée au contact et adresse pour \n",
    "    ceux suivant le même pattern\"\"\"\n",
    "    \n",
    "    split1_ao = ao.split('conditions d’execution')\n",
    "    if len(split1_ao)>1:\n",
    "        if 'autres points' in split1_ao[1]:\n",
    "            split2_ao = split1_ao[1].split('autres points')\n",
    "            ao_light = split1_ao[0] + \" \" + split2_ao[1]\n",
    "            \n",
    "        elif 'profils et competences attendus' in split1_ao[1] :\n",
    "            split2_ao = split1_ao[1].split('profils et competences attendus')\n",
    "            ao_light = split1_ao[0] + \" \" + split2_ao[1]\n",
    "            \n",
    "        else:\n",
    "            ao_light = split1_ao[0]\n",
    "    \n",
    "    else :\n",
    "        ao_light = split1_ao[0]\n",
    "        \n",
    "    return ao_light"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ao_delete_element_reponse(ao):\n",
    "    if \"elements de reponse attendus\" in ao:\n",
    "        ao_light = ao.split(\"elements de reponse attendus\")[0]\n",
    "    elif \"methode d'attribution\" in ao:\n",
    "        ao_light = ao.split(\"methode d'attribution\")[0]\n",
    "    else:\n",
    "        ao_light = ao\n",
    "    return ao_light"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cv_keep_comp(cv):\n",
    "    if \"compétences & outils\" in cv:\n",
    "        cv_split1 = cv.split(\"compétences & outils\")[1]\n",
    "        if \"formations & certifications\" in cv_split1:\n",
    "            cv_light = cv_split1.split(\"formations & certifications\")[0]\n",
    "        else:\n",
    "            cv_light = \"\"\n",
    "            \n",
    "    elif \"compétences\" in cv:\n",
    "        cv_split1 = cv.split(\"compétences\")[1]\n",
    "        if \"experience professionnelle\" in cv_split1:\n",
    "            cv_light = cv_split1.split(\"experience professionnelle\")[0]\n",
    "        elif \"expérience professionnelle\" in cv_split1:\n",
    "            cv_light = cv_split1.split(\"expérience professionnelle\")[0]\n",
    "        elif \"parcours professionnel\" in cv_split1:\n",
    "            cv_light = cv_split1.split(\"parcours professionnel\")[0]\n",
    "        elif \"langues\" in cv_split1:\n",
    "            cv_light = cv_split1.split(\"langues\")[0]\n",
    "        elif \"experience\" in cv_split1:\n",
    "            cv_light = cv_split1.split(\"experience\")[0]\n",
    "        elif \"expérience\" in cv_split1:\n",
    "            cv_light = cv_split1.split(\"expérience\")[0]\n",
    "        else:\n",
    "            cv_light = \"\"\n",
    "            \n",
    "    elif \"competences\" in cv:\n",
    "        cv_split1 = cv.split(\"competences\")[1]\n",
    "        if \"experience\" in cv_split1:\n",
    "            cv_light = cv_split1.split(\"experience\")[0]\n",
    "        elif \"expérience\" in cv_split1:\n",
    "            cv_light = cv_split1.split(\"expérience\")[0]\n",
    "        elif \"parcours professionnel\" in cv_split1:\n",
    "            cv_light = cv_split1.split(\"parcours professionnel\")[0]\n",
    "        elif \"formations & certifications\" in cv_split1:\n",
    "            cv_light = cv_split1.split(\"formations & certifications\")[0]\n",
    "        elif \"formation\" in cv_split1:\n",
    "            cv_light = cv_split1.split(\"formations & certifications\")[0]\n",
    "        elif \"langues\" in cv_split1:\n",
    "            cv_light = cv_split1.split(\"langues\")[0]\n",
    "        else:\n",
    "            cv_light = \"\"\n",
    "    \n",
    "    elif \"it skills\" in cv:\n",
    "        cv_split1 = cv.split(\"it skills\")[1]\n",
    "        if \"activities & interests\" in cv_split1:\n",
    "            cv_light = cv_split1.split(\"activities & interests\")[0]\n",
    "        elif \"hobbies\" in cv_split1:\n",
    "            cv_light = cv_split1.split(\"hobbies\")[0]\n",
    "        else:\n",
    "            cv_light = \"\"\n",
    "    \n",
    "    else:\n",
    "        cv_light = \"\"\n",
    "    \n",
    "    return cv_light "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#download cv in a list\n",
    "def load_Doc_list(nombre, path, doc_type):\n",
    "    liste_paths = [path+directory for directory in os.listdir(path)]\n",
    "    #print(liste_paths)\n",
    "    liste_cv = []\n",
    "    dico_cv = {}\n",
    "    for path in liste_paths :\n",
    "        cv = CV(0, 0, 0, 0)\n",
    "        cv.filename = os.path.basename(path)\n",
    "        \n",
    "        try:\n",
    "            if doc_type == 'ao':\n",
    "                cv.cv = open(path).read()\n",
    "                cv.cv = del_punct(del_line_feed(cv.cv.lower()))\n",
    "                cv.cv = ao_delete_element_reponse(ao_delete_context_part(ao_delete_contact_part(cv.cv)))\n",
    "\n",
    "            else :\n",
    "                cv.cv = open(path).read()\n",
    "                cv.cv = cv_keep_comp(del_punct(del_line_feed(cv.cv.lower())))\n",
    "                \n",
    "            cv.doc_type = doc_type\n",
    "            liste_cv.append(cv.cv)\n",
    "            dico_cv[cv.filename.split(\".\")[0]] = cv#.\n",
    "        except UnicodeDecodeError:\n",
    "            print(\"Error !\")\n",
    "            print(path)\n",
    "    return liste_cv, dico_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading titles\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading titles\")\n",
    "def load_title_list(nombre, path):\n",
    "    path = \"../Maha/data/\"\n",
    "    dico_titles = {}\n",
    "    filenames = sorted(glob(os.path.join(path,\"*.csv\")))\n",
    "    for file in filenames[:nombre]:\n",
    "        with open(file, encoding='latin-1') as csvfile:\n",
    "            readCSV = csv.reader(csvfile, delimiter=';')\n",
    "            for row in readCSV:\n",
    "                dico_titles[row[0]] = row[1]\n",
    "    return dico_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_AO = '../Commun/Data_Talan/txt_ao/'\n",
    "path_CV = '../Commun/Data_Talan/txt/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "liste_cv, dico_cv = load_Doc_list(200, path_CV, 'cv')\n",
    "liste_AO, dico_AO = load_Doc_list(200, path_AO, 'ao')\n",
    "\n",
    "dico_global = dict(dico_cv)\n",
    "dico_global.update(dico_AO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  savoir faire  gestion de la relation client   conseil   gestion de portefeuille de clients professionnels   techniques de planification   gestion de projets   marketing informatique logiciels de gestion  erp  crm  bureautique  pack office  word  excel  access  powerpoint   '"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "liste_cv[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "691"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dico_global)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Reconnaissance du langage du CV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _calculate_languages_ratios(text):\n",
    "    \"\"\"\n",
    "    Calculate probability of given text to be written in several languages and\n",
    "    return a dictionary that looks like {'french': 2, 'spanish': 4, 'english': 0}\n",
    "    \"\"\"\n",
    "\n",
    "    languages_ratios = {}\n",
    "\n",
    "    '''\n",
    "    nltk.wordpunct_tokenize() splits all punctuations into separate tokens\n",
    "    \n",
    "    >>> wordpunct_tokenize(\"That's thirty minutes away. I'll be there in ten.\")\n",
    "    ['That', \"'\", 's', 'thirty', 'minutes', 'away', '.', 'I', \"'\", 'll', 'be', 'there', 'in', 'ten', '.']\n",
    "    '''\n",
    "\n",
    "    tokens = wordpunct_tokenize(text)\n",
    "    words = [word.lower() for word in tokens] #from text get list of word in minuscule\n",
    "\n",
    "    \n",
    "    for language in stopwords.fileids(): # pour chaque langue\n",
    "        stopwords_set = set(stopwords.words(language)) #je mets les stop words du langage dans un set\n",
    "        words_set = set(words) #je mets les mots de mon texte dans un set\n",
    "        #je prends l'intersection entre les mots de mon texte et les mots du stopwords dans le langage donné\n",
    "        common_elements = words_set & stopwords_set\n",
    "        \n",
    "        #je compute mon score comme le nombre d'éléments en communs dictionnaire [langage : score]\n",
    "        languages_ratios[language] = len(common_elements) # language \"score\"\n",
    "\n",
    "    return languages_ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/mehdiregina/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['arabic',\n",
       " 'azerbaijani',\n",
       " 'danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'greek',\n",
       " 'hungarian',\n",
       " 'indonesian',\n",
       " 'italian',\n",
       " 'kazakh',\n",
       " 'nepali',\n",
       " 'norwegian',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'spanish',\n",
       " 'swedish',\n",
       " 'turkish']"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "stopwords.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cv_langue(liste_cv) :\n",
    "    \"\"\"Return resume witten in the specified language in parameter\"\"\"\n",
    "    liste_2 = []\n",
    "    for cv in liste_cv:\n",
    "        if max(_calculate_languages_ratios(cv),key =_calculate_languages_ratios(cv).get)=='french':\n",
    "            liste_2.append(cv)\n",
    "    return liste_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "liste_cv_fr = get_cv_langue(liste_cv) \n",
    "liste_ao_fr = get_cv_langue(liste_AO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Preprocessing du text **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_treatment (text):\n",
    "    text = text.lower()\n",
    "    text = text.replace(\"\\x00\", '').replace(\"\\x01\", '').replace(\"\\x02\", '').replace(\"\\x03\", '') \\\n",
    "    .replace(\"\\x04\", '').replace(\"\\x05\", '').replace(\"\\x06\", '').replace(\"\\x07\", '').replace(\"\\x08\", '') \\\n",
    "    .replace(\"\\x0e\", '').replace(\"\\x11\", '').replace(\"\\x12\", '').replace(\"\\x10\", '').replace(\"\\x19\", '') \\\n",
    "    .replace(\"\\x1b\", '').replace(\"\\x14\", '').replace(\"\\x15\", '').replace('/', '').replace('=', '').replace(\"〓\", \"\") \\\n",
    "    .replace(\"»\", \"\").replace(\"«\", \"\").replace(\"¬\", \"\").replace('`', '').replace (\" -\", \"\").replace(\"•\", \"\")\\\n",
    "    .replace(\"l'\", \"\").replace(\"l’\", \"\").replace(\"l´\", \"\").replace(\"d’\", \"\").replace(\"d'\", \"\").replace(\"d´\",\"\")\\\n",
    "    .replace(\"j’\", \"\").replace(\"j'\", \"\").replace(\"j´\",\"\").replace(\"n’\", \"\").replace(\"n'\", \"\").replace(\"n´\",\"\")\\\n",
    "    .replace(\"”\", \"\").replace(\"~\", \"\").replace(\"§\", \"\").replace(\"¨\", \"\").replace(\"©\", \"\").replace(\"›\", \"\")\\\n",
    "    .replace(\"₋\", \"\").replace(\"→\", \"\").replace(\"⇨\", \"\").replace(\"∎\", \"\").replace(\"√\", \"\").replace(\"□\", \"\")\\\n",
    "    .replace(\"*\", \"\").replace(\"&\", \"\").replace(\"►\", \"\").replace(\"◊\", \"\").replace(\"☞\", \"\").replace(\"#\", \"\")\\\n",
    "    .replace(\"%\", \"\").replace(\"❖\", \"\").replace(\"➠\", \"\").replace(\"➢\", \"\").replace(\"\", \"\").replace(\"✓\", \"\") \\\n",
    "    .replace(\"√\", \"\").replace(\"✔\", \"\").replace(\"♦\", \"\").replace(\"◦\", \"\").replace(\"●\", \"\").replace(\"▫\", \"\")\\\n",
    "    .replace(\"▪\", \"\").replace(\"…\", \"\").replace(\"þ\", \"\").replace(\"®\", \"\").replace('', '').replace(\"...\", \"\")\\\n",
    "    .replace(\" o \", \"\")\n",
    "    text = unidecode.unidecode(text) # remove accent\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Gestion des stop words **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taille stop words liste :  611\n"
     ]
    }
   ],
   "source": [
    "#generate stopwords\n",
    "stop_words_py = set(stop_words.get_stop_words('french'))\n",
    "\n",
    "# attention certains stop words pourraient être utiles par la suite\n",
    "stopwords_set_manuel = set([\"an\", \"ans\", 'les', 'moins', 'd\\'un','janvier', 'fevrier', 'février', 'mars', 'avril', \\\n",
    "                 'mai', 'juin', 'juillet', 'aout', 'août', 'septembre', 'octobre', 'novembre', 'décembre', \\\n",
    "                  'decembre', 'moins', 'mise', 'universit\\xc3\\xa9', 'université', 'universite', 'ion','sage', \\\n",
    "                  'o', 'rac', 'vers', 'via', 'p\\xc3\\xa9rim\\xc3\\xa8tre', 'périmètre','et','paris','x',\"\\x00\",\\\n",
    "                          \"\\x01\",\"\\x02\", \"\\x03\",\"\\x04\",\"\\x05\",\"\\x06\",\"\\x07\",\"\\x08\",\"\\x09\",\"\\x0e\",\"\\x0e\",\"\\x11\",\\\n",
    "                           \"\\x12\",\"\\x13\",\"\\x14\",\"\\x15\",\"\\x16\",\"\\x17\",\"\\x18\",\"\\x19\",\"transport\",\"puis\",\"lieu\",\\\n",
    "                           \"adresse\",\"entre\",'dun','dune','chez','boulognebillancourt','bt','etc','recrutement','main',\\\n",
    "                           'and', 'paie','paiement','environ','place','france','paris','mois','mobile','mobiles',\\\n",
    "                           'nanterre','source','sources','concerne','concernant','of','non','notes','rh','minimum',\\\n",
    "                           'maximum','bac','site','sites','actuellement','telephone','telephonique','telephoniques','ca','demenager',\\\n",
    "                           'demenagement','participer','participation','lycee','baccalaureat','lien','liens','in',\\\n",
    "                           'indeed','email','indeedcomrd7e8913ed00d0384','aujourhui','afin','toujours','enterprise',\\\n",
    "                           \"guide\",\"10g\",\"11g\",\"9i\",'ad','v10','v2','v3','v5','v6','v8','v9','pablo','neruda',\\\n",
    "                           'dec','stelsia','cid1','sens','va','24h24', '7j7', 'levalloisperret','louis', 'armand', 'ermont',\\\n",
    "                            'localisation','anne', 'perot',' s ','mission', 'date', 'debut','fin','avenue', 'val', 'oise',\\\n",
    "                           'echeant','bout','division','prestations', 'intellectuelles', 'contexte','descroix','24h', '7j',\\\n",
    "                           'disposition','places','moyens', 'sncf', 'mis','rue','levallois','perret','scnf','ainsi',\\\n",
    "                           \"jusqu'au\",\"jusqu'a\",'ast','exercant','souhaitee','sectorielles','sectorielles','competences',\\\n",
    "                           \"ensemble\",\"phases\",\"phase\",\"technique\",\"garantir\",\"coherence\",\"referentiels\",\"server\",\"langages\",\"langage\"\\\n",
    "                           \"secteurs\",'activite','metier','campaign',\"sup'com\",'collegiate','contest','deuxieme',\\\n",
    "                            'competition','sectorielles','outils','arts','metiers','methodes','methode','trace',\\\n",
    "                           'entity','services','centres','interet','musculation','cuisine','voyage','travail','groupe','realisation',\\\n",
    "                            'team','building','presentation','devant','publique','versions','anterieurs','etats',\\\n",
    "                            'crystal','reports','superviseur','tiers','secteurs','marche','tertiaire','specialite',\\\n",
    "                           'divers','lumiere','lyon','parcours','sise','iup','institut','universitaire','professionnel',\\\n",
    "                           '1ere','annee','3eme','annee','licence','dut','niort','techniques','langues','vivantes','anglais',\\\n",
    "                            'professionnel','espagnol','scolaire','arabe','parle','revue','code','check','list','2008r2',\\\n",
    "                           'biginsight','general','prog','procedurale','parallele','qt','compilation','courant','jour',\\\n",
    "                           'informatiques','suites','active','activites','adaptation','alternance','animation','annees',\\\n",
    "                           'appliquees','autour','aupres','autres','avance','avancees','bien','campus','cartographie','cas',\\\n",
    "                           'center','centrale','centre','certification','certifications','charges','choix','classe','client',\\\n",
    "                            'clients','com','comite','comites','cours','derive','detail','differents','diplomes',\\\n",
    "                           'e','ecrit','ecriture','effets','engagee','enjeux','enquete','entite','entreprise',\\\n",
    "                           'esprit','etant','etoile','europe','evenements','externes','famille','fixes','fortes','fr',\\\n",
    "                           'garantie','generale','grandes','group','hp','humaines','impact','import','investissement',\\\n",
    "                           'langage','lille','livraison','locaux','mises','missions','mode','national','nationale','necessaires',\\\n",
    "                            'niveau','passage','plusieurs','periode','permettra','pertinence','principaux','principes','promotion',\\\n",
    "                            'proposes','rapidement','rencontres','reprise','saint','salle','secteur','sein','selon',\\\n",
    "                           'suite','suivant','tant','tendances','toutes','travaux','tres','type',\\\n",
    "                            'utilisateur','utilisateurs','utilisation','utilises','voix','voyages',])\n",
    "stop_words_main = stop_words_py | stopwords_set_manuel\n",
    "stop_words_main = list(stop_words_main)\n",
    "print(\"taille stop words liste : \", len(stop_words_main))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fus',\n",
       " 'deuxieme',\n",
       " 'donc',\n",
       " 'sommes',\n",
       " '24h',\n",
       " 'lumiere',\n",
       " '7j7',\n",
       " 'o',\n",
       " 'puis',\n",
       " '\\x0e',\n",
       " 'voyage',\n",
       " 'minimum',\n",
       " 'neruda',\n",
       " '10g',\n",
       " 'en',\n",
       " 'votre',\n",
       " 'division',\n",
       " 'concerne',\n",
       " 'pour',\n",
       " 'eût',\n",
       " 'differents',\n",
       " 'v10',\n",
       " 'travaux',\n",
       " '\\x01',\n",
       " 'rapidement',\n",
       " 'eussiez',\n",
       " 'ast',\n",
       " 'levalloisperret',\n",
       " 'participation',\n",
       " 'revue',\n",
       " 'depuis',\n",
       " 'aujourhui',\n",
       " 'ainsi',\n",
       " 'auraient',\n",
       " 'nationale',\n",
       " 'état',\n",
       " 'du',\n",
       " 'ayez',\n",
       " 'qu',\n",
       " 'etats',\n",
       " 'mois',\n",
       " 'deux',\n",
       " 'se',\n",
       " 'armand',\n",
       " 'date',\n",
       " 'national',\n",
       " 'devrait',\n",
       " 'garantir',\n",
       " 'dès',\n",
       " 'val',\n",
       " 'bon',\n",
       " 'devriez',\n",
       " 'novembre',\n",
       " 'arabe',\n",
       " 'ad',\n",
       " 'animation',\n",
       " 'main',\n",
       " 'aussi',\n",
       " 'juin',\n",
       " 'étais',\n",
       " 'cartographie',\n",
       " 'enjeux',\n",
       " 'cid1',\n",
       " 'mes',\n",
       " 'suites',\n",
       " 'périmètre',\n",
       " 't',\n",
       " 'ayant',\n",
       " 'tes',\n",
       " 'il',\n",
       " 'aie',\n",
       " 'cette',\n",
       " '\\x14',\n",
       " 'qt',\n",
       " '11g',\n",
       " 'au',\n",
       " 'esprit',\n",
       " 'concernant',\n",
       " 'avoir',\n",
       " 'ma',\n",
       " 'ne',\n",
       " 'sera',\n",
       " 'v3',\n",
       " '3eme',\n",
       " 'devrez',\n",
       " 'participer',\n",
       " 'v5',\n",
       " 'iup',\n",
       " '24h24',\n",
       " 'eusses',\n",
       " 'nommés',\n",
       " 'principaux',\n",
       " 'ses',\n",
       " 'été',\n",
       " 'd',\n",
       " 'car',\n",
       " 'debut',\n",
       " 'vont',\n",
       " 'intellectuelles',\n",
       " 'vers',\n",
       " 'sa',\n",
       " 'fais',\n",
       " 'mises',\n",
       " 'annees',\n",
       " '\\x17',\n",
       " 'passage',\n",
       " 'auront',\n",
       " 'tres',\n",
       " 'langages',\n",
       " 'group',\n",
       " 'v9',\n",
       " 'ce',\n",
       " 'je',\n",
       " 'tertiaire',\n",
       " 'te',\n",
       " 'sage',\n",
       " 'rue',\n",
       " 'campaign',\n",
       " 'v6',\n",
       " 'niveau',\n",
       " 'etoile',\n",
       " 'à',\n",
       " 'dù',\n",
       " 'valeur',\n",
       " 'lien',\n",
       " 'parcours',\n",
       " 'publique',\n",
       " 'nom',\n",
       " 'via',\n",
       " 'list',\n",
       " 'sectorielles',\n",
       " 'derive',\n",
       " 'voit',\n",
       " 'notes',\n",
       " 'mobiles',\n",
       " 'personne',\n",
       " 'localisation',\n",
       " 'devant',\n",
       " 'devoir',\n",
       " 'reports',\n",
       " \"sup'com\",\n",
       " 'methodes',\n",
       " 'bac',\n",
       " '\\x04',\n",
       " 'eûtes',\n",
       " 'langues',\n",
       " 'avais',\n",
       " 'fût',\n",
       " 'campus',\n",
       " 'aies',\n",
       " 'perot',\n",
       " 'levallois',\n",
       " 'centre',\n",
       " '\\x06',\n",
       " 'eûmes',\n",
       " 'methode',\n",
       " 'engagee',\n",
       " 'chez',\n",
       " 'telephoniques',\n",
       " 'arts',\n",
       " 'ceux',\n",
       " 'appliquees',\n",
       " 'centres',\n",
       " 'lyon',\n",
       " 'ermont',\n",
       " 'parole',\n",
       " 'espagnol',\n",
       " 'fusses',\n",
       " 'fin',\n",
       " 'enterprise',\n",
       " 'elle',\n",
       " 'plupart',\n",
       " 'dans',\n",
       " 'aurions',\n",
       " '\\t',\n",
       " 'aurez',\n",
       " 'la',\n",
       " 'étiez',\n",
       " 'leur',\n",
       " 'places',\n",
       " '\\x03',\n",
       " 'hors',\n",
       " 'dut',\n",
       " 'permettra',\n",
       " 'faisez',\n",
       " 'nouveaux',\n",
       " 'certification',\n",
       " 'plusieurs',\n",
       " 'seriez',\n",
       " 'metier',\n",
       " 'source',\n",
       " 'cuisine',\n",
       " 'cas',\n",
       " 'eue',\n",
       " 'certifications',\n",
       " 'tant',\n",
       " 'contest',\n",
       " 'eux',\n",
       " 'étions',\n",
       " 'activites',\n",
       " 'elles',\n",
       " 'auriez',\n",
       " 'prog',\n",
       " 'ici',\n",
       " 'descroix',\n",
       " 'quelle',\n",
       " 'louis',\n",
       " 'devrions',\n",
       " 'saint',\n",
       " 'aucun',\n",
       " '\\x07',\n",
       " '\\x05',\n",
       " 'quel',\n",
       " 'generale',\n",
       " 'aient',\n",
       " 'juste',\n",
       " 'n',\n",
       " 'quelles',\n",
       " 'v2',\n",
       " 'sein',\n",
       " 'moins',\n",
       " 'choix',\n",
       " 'maintenant',\n",
       " 'rh',\n",
       " 'superviseur',\n",
       " 'utilises',\n",
       " 'avril',\n",
       " 'faire',\n",
       " 'ils',\n",
       " 'ecrit',\n",
       " 'a',\n",
       " 'in',\n",
       " 'as',\n",
       " 'bt',\n",
       " 'humaines',\n",
       " 'trace',\n",
       " 'interet',\n",
       " 'stelsia',\n",
       " 'fut',\n",
       " 'courant',\n",
       " 'lille',\n",
       " 'des',\n",
       " 'dehors',\n",
       " '\\x15',\n",
       " 'an',\n",
       " 'building',\n",
       " 'non',\n",
       " 'fixes',\n",
       " 'auras',\n",
       " 'charges',\n",
       " 'etant',\n",
       " 'grandes',\n",
       " 'même',\n",
       " '\\x12',\n",
       " 'on',\n",
       " 'nommée',\n",
       " 'voix',\n",
       " 'notre',\n",
       " 'souhaitee',\n",
       " 'quels',\n",
       " 'sites',\n",
       " 'exercant',\n",
       " 'secteurs',\n",
       " 'check',\n",
       " 'fr',\n",
       " 'fortes',\n",
       " 'services',\n",
       " '7j',\n",
       " 'parallele',\n",
       " 'avant',\n",
       " 'eu',\n",
       " 'actuellement',\n",
       " 'aout',\n",
       " 'impact',\n",
       " '\\x18',\n",
       " 'un',\n",
       " 'avons',\n",
       " 'y',\n",
       " 'collegiate',\n",
       " 'là',\n",
       " 'force',\n",
       " 'ça',\n",
       " 'europe',\n",
       " 'tandis',\n",
       " 'sources',\n",
       " 'comme',\n",
       " 'soi',\n",
       " 'ion',\n",
       " 'centrale',\n",
       " 'aurons',\n",
       " 'nos',\n",
       " 'livraison',\n",
       " 'dune',\n",
       " 'classe',\n",
       " 'peu',\n",
       " 'indeed',\n",
       " 'transport',\n",
       " 'environ',\n",
       " 'langagesecteurs',\n",
       " 'eurent',\n",
       " 'anglais',\n",
       " 'phase',\n",
       " 'realisation',\n",
       " 'septembre',\n",
       " 'vu',\n",
       " 'rac',\n",
       " 'garantie',\n",
       " 'avait',\n",
       " 'est',\n",
       " 'une',\n",
       " 'décembre',\n",
       " 'sncf',\n",
       " 'disposition',\n",
       " 'x',\n",
       " 'début',\n",
       " 'demenager',\n",
       " 'selon',\n",
       " 'alors',\n",
       " 'mon',\n",
       " 'ni',\n",
       " 'aurait',\n",
       " 'serai',\n",
       " 'trop',\n",
       " 'dois',\n",
       " 'ait',\n",
       " 'v8',\n",
       " 'famille',\n",
       " 'team',\n",
       " 'suis',\n",
       " 'soyons',\n",
       " \"jusqu'a\",\n",
       " 'ayons',\n",
       " 'tu',\n",
       " 'personnes',\n",
       " 'octobre',\n",
       " 'informatiques',\n",
       " 'était',\n",
       " 'proposes',\n",
       " 'mission',\n",
       " 'universitÃ©',\n",
       " 'nommé',\n",
       " 'entity',\n",
       " 'serez',\n",
       " 'soit',\n",
       " 'entite',\n",
       " 'ca',\n",
       " 'boulognebillancourt',\n",
       " 'toujours',\n",
       " 'entre',\n",
       " 'que',\n",
       " 'tels',\n",
       " 'chaque',\n",
       " 'maximum',\n",
       " 'juillet',\n",
       " 'licence',\n",
       " 'encore',\n",
       " 'es',\n",
       " 'devront',\n",
       " 'client',\n",
       " 'aviez',\n",
       " 'place',\n",
       " 'crystal',\n",
       " 'autres',\n",
       " 'comment',\n",
       " 'dedans',\n",
       " 'moi',\n",
       " 'and',\n",
       " 'de',\n",
       " 'comite',\n",
       " 'aux',\n",
       " 'serions',\n",
       " 'recrutement',\n",
       " 'université',\n",
       " 'furent',\n",
       " 'fusse',\n",
       " 'ta',\n",
       " 'm',\n",
       " 'investissement',\n",
       " 'mai',\n",
       " 'seraient',\n",
       " 'lui',\n",
       " 'droite',\n",
       " 'janvier',\n",
       " 'annee',\n",
       " 'active',\n",
       " 'avancees',\n",
       " 'general',\n",
       " 'salle',\n",
       " 'telephone',\n",
       " 'competences',\n",
       " 'of',\n",
       " 'mot',\n",
       " 'missions',\n",
       " 'nouveau',\n",
       " 'parce',\n",
       " 'site',\n",
       " 'tout',\n",
       " 'être',\n",
       " 'toutes',\n",
       " 'competition',\n",
       " 'ces',\n",
       " 'eusse',\n",
       " 'quand',\n",
       " 'cela',\n",
       " 'fûmes',\n",
       " 'soient',\n",
       " 'demenagement',\n",
       " 'secteur',\n",
       " 'fussent',\n",
       " 'leurs',\n",
       " 'contexte',\n",
       " '\\x08',\n",
       " 'êtes',\n",
       " 'seulement',\n",
       " 'scolaire',\n",
       " 'haut',\n",
       " 'j',\n",
       " 'et',\n",
       " 'ou',\n",
       " 'e',\n",
       " 'seras',\n",
       " 'referentiels',\n",
       " 'avaient',\n",
       " 'dos',\n",
       " 'professionnel',\n",
       " 'toi',\n",
       " 'paris',\n",
       " 'eut',\n",
       " 'promotion',\n",
       " 'necessaires',\n",
       " 'serais',\n",
       " 'groupe',\n",
       " 'metiers',\n",
       " 'vous',\n",
       " 'me',\n",
       " 'pourquoi',\n",
       " 'import',\n",
       " 'externes',\n",
       " 'suite',\n",
       " 'travail',\n",
       " 'l',\n",
       " 'biginsight',\n",
       " 'anne',\n",
       " 'aurais',\n",
       " 'universite',\n",
       " 'sise',\n",
       " 'serait',\n",
       " 'ton',\n",
       " 'cours',\n",
       " 'août',\n",
       " 'reprise',\n",
       " 'com',\n",
       " '\\x16',\n",
       " 'bout',\n",
       " 'faites',\n",
       " 'center',\n",
       " 'vivantes',\n",
       " 'phases',\n",
       " 'musculation',\n",
       " 'presentation',\n",
       " 'eues',\n",
       " 'ai',\n",
       " 'type',\n",
       " 'technique',\n",
       " 'sien',\n",
       " 'sous',\n",
       " 'tous',\n",
       " 'seront',\n",
       " '\\x00',\n",
       " 'ensemble',\n",
       " \"jusqu'au\",\n",
       " 'très',\n",
       " 'evenements',\n",
       " 'mobile',\n",
       " 'techniques',\n",
       " 'pertinence',\n",
       " 'lycee',\n",
       " 'mis',\n",
       " 'institut',\n",
       " 'compilation',\n",
       " 'mais',\n",
       " 'france',\n",
       " 'versions',\n",
       " 'telephonique',\n",
       " 'mode',\n",
       " 'tendances',\n",
       " 'son',\n",
       " 'clients',\n",
       " 'avec',\n",
       " '\\x02',\n",
       " 'enquete',\n",
       " 'principes',\n",
       " 'qui',\n",
       " 'aupres',\n",
       " 'étant',\n",
       " '\\x13',\n",
       " 'pas',\n",
       " 'dun',\n",
       " 'sens',\n",
       " 'fûtes',\n",
       " 'jour',\n",
       " 'avenue',\n",
       " 'sont',\n",
       " '1ere',\n",
       " 'indeedcomrd7e8913ed00d0384',\n",
       " 'afin',\n",
       " 'adaptation',\n",
       " 'tiers',\n",
       " 'serons',\n",
       " '2008r2',\n",
       " 'effets',\n",
       " 'février',\n",
       " 'parle',\n",
       " 'fait',\n",
       " 'aura',\n",
       " 'etc',\n",
       " 'scnf',\n",
       " 'nanterre',\n",
       " 'sois',\n",
       " 'anterieurs',\n",
       " 'ci',\n",
       " 'outils',\n",
       " 'utilisateur',\n",
       " 'email',\n",
       " 'server',\n",
       " 'hp',\n",
       " 'ceci',\n",
       " 'pÃ©rimÃ¨tre',\n",
       " 'utilisateurs',\n",
       " 'mars',\n",
       " 'locaux',\n",
       " 'decembre',\n",
       " 'diplomes',\n",
       " 'fussions',\n",
       " 'dec',\n",
       " 'avance',\n",
       " 'ans',\n",
       " 'fevrier',\n",
       " 'procedurale',\n",
       " 'paie',\n",
       " 'specialite',\n",
       " 'paiement',\n",
       " 'fussiez',\n",
       " 'langage',\n",
       " 'comites',\n",
       " 'les',\n",
       " 'periode',\n",
       " 'eus',\n",
       " 'si',\n",
       " 'bien',\n",
       " 'vois',\n",
       " 'coherence',\n",
       " 'divers',\n",
       " 'mise',\n",
       " 'aurai',\n",
       " 'devrons',\n",
       " 'par',\n",
       " ' s ',\n",
       " 'activite',\n",
       " 'liens',\n",
       " '\\x11',\n",
       " 'nous',\n",
       " 'étés',\n",
       " 'utilisation',\n",
       " 'adresse',\n",
       " 'baccalaureat',\n",
       " 'vos',\n",
       " 'guide',\n",
       " 'niort',\n",
       " 'universitaire',\n",
       " 'voient',\n",
       " 'marche',\n",
       " 'peut',\n",
       " 'prestations',\n",
       " 'echeant',\n",
       " 'detail',\n",
       " \"d'un\",\n",
       " 'pablo',\n",
       " 'soyez',\n",
       " 'eussions',\n",
       " 'ecriture',\n",
       " 'avez',\n",
       " 'avions',\n",
       " 'cet',\n",
       " 'va',\n",
       " 'perret',\n",
       " 'eussent',\n",
       " 'entreprise',\n",
       " 'autre',\n",
       " 'font',\n",
       " 'sujet',\n",
       " 'le',\n",
       " '\\x19',\n",
       " 'ont',\n",
       " 'doit',\n",
       " 'moyens',\n",
       " '9i',\n",
       " 'code',\n",
       " 'oise',\n",
       " 'sans',\n",
       " 'autour',\n",
       " 'fois',\n",
       " 'rencontres',\n",
       " 'voyages',\n",
       " 'étaient',\n",
       " 'où',\n",
       " 'sur',\n",
       " 'lieu',\n",
       " 'suivant',\n",
       " 'alternance',\n",
       " 'tellement']"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#voir si utile\n",
    "def remove_stopwords(text,stopwords_list):\n",
    "    text_temp = \" \".join(text.split())+\" \"\n",
    "    for word in stopwords_list:\n",
    "        text_temp = text_temp.replace(\" \"+word+\" \", \" \")\n",
    "    return text_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "liste_cv_clean = [remove_stopwords(text_treatment(cv),stop_words_main) for cv in liste_cv_fr]\n",
    "liste_ao_clean = [remove_stopwords(text_treatment(ao),stop_words_main) for ao in liste_ao_fr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#SnowballStemmer \n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"french\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_and_stem(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters \n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems\n",
    "\n",
    "\n",
    "def tokenize_only(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters \n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "liste_cv_tokenized = [tokenize_only(cv) for cv in liste_cv_clean]\n",
    "liste_ao_tokenized = [tokenize_only(ao) for ao in liste_ao_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "longueur set cv 4784\n",
      "longueur set ao 2227\n",
      "longueur set intersection 1045\n"
     ]
    }
   ],
   "source": [
    "#get set\n",
    "liste_flat = []\n",
    "for row in liste_cv_tokenized:\n",
    "    liste_flat += row\n",
    "set_cv = set(liste_flat)\n",
    "print(\"longueur set cv {}\".format(len(set_cv)))\n",
    "\n",
    "#get set\n",
    "liste_flat = []\n",
    "for row in liste_ao_tokenized:\n",
    "    liste_flat += row\n",
    "set_ao = set(liste_flat)\n",
    "print(\"longueur set ao {}\".format(len(set_ao)))\n",
    "\n",
    "#get set\n",
    "set_intersect = set_cv & set_ao\n",
    "print(\"longueur set intersection {}\".format(len(set_intersect)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
