{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "from glob import glob\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "import codecs\n",
    "import unidecode\n",
    "#pip install unidecode\n",
    "import mpld3\n",
    "# pip install mpld3\n",
    "import stop_words\n",
    "# pip install stop-words\n",
    "from nltk import SnowballStemmer, pos_tag, word_tokenize, wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction.text import HashingVectorizer,TfidfTransformer,TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.decomposition import TruncatedSVD, NMF\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.metrics.pairwise import *\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import *\n",
    "from sklearn.semi_supervised import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Lecture des données **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_cv_list(nombre):\n",
    "    path = '../data_indeed/Txt/'\n",
    "    liste_paths = [path+directory for directory in os.listdir(path)]\n",
    "    liste_cv = []\n",
    "    liste_files = []\n",
    "    for path in liste_paths :\n",
    "        if \"informaticien\" and \"dba\" and \"chef_de_projet_informatique\" not in path:\n",
    "            filenames = sorted(glob(os.path.join(path,\"*.txt\")))\n",
    "           \n",
    "            for file in filenames[:nombre]:\n",
    "                liste_cv.append(open(file).read())\n",
    "                liste_files.append(file.split('/')[-1].split('.')[0])\n",
    "    return liste_cv,liste_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "liste_cv_indeed,liste_files = load_cv_list(200)\n",
    "len(liste_cv_indeed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning fonctions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Suppression des sauts de ligne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string,re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regex = re.compile('[%s]' % '(\\\\n)*(\\\\x0c)*')\n",
    "def del_line_feed(s):  \n",
    "    \"\"\"Delete \\n in the text\"\"\"\n",
    "    return regex.sub(' ', s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"informaticien développement et réseaux  développeur intégrateur web  éragny  95  - email me on indeed: indeed.com/r/d7e8913ed00d0384  aujourd’hui, je suis en recherche d'une opportunité sur un poste de développeur ou d’intégrateur web afin de monter toujours plus en compétence et d’approfondir les bases solide que j’ai acquis en formation.  expérience  informaticien développement et réseaux  legrandcercle95  -  éragny  95  -  novembre 2016 - juin 2017  informaticien de l'entreprise, mes missions était de gérer les différent problème dans l'entreprise. mise en place d'un antivirus serveur, sauvegarde nas... créé et gérer les droits sur l'active directory. paramétrer des clients léger ainsi que du matériel informatique comme des imprimantes ou des étiqueteuse en ip fixe.  réajustement du code html et css sur le site grand public selon les normes w3c. création d'une source odbc création d'un code en php - sql afin de récupéré des données librairie sur un site fournisseur pour les enregistré en fiche xml. création de bannières pour les différents évènements avec photoshop et formation d'une personne sur place au logiciel photoshop.  charge de clientele  europcar france commercial  -  saint-ouen-l'aumône  95  -  novembre 2008 - novembre 2016  mes missions principales :   - qualité de service : assurer l'accueil des clients et le respect de la charte en agence.  - gestion des clients : traiter l'ensemble des appels téléphoniques. analyser les besoins du client. assurer le suivi de la clientèle.  - logistique et administratif : s'assurer de la disponibilité des véhicules. gérer l'administratif de l'agence. gestion de la caisse.  - polyvalence   formation développeur intégrateur web  -  septembre 2015 - juin 2016  c.i.f  centre de formation ifocop  8mois   - création de différents sites.  portfolio  - présentation d'un site e-commerce créé de a à z pour l'examen  développeur web  stage   le club des formateurs  -  éragny  95  -  février 2016 - mai 2016  développement d'un site sous wordpress de a à z.  commercial stagiaire  lamy assurance -  novembre 2007 - décembre 2007  stagiaire  port marly accessoires  -  le port-marly  78  -  mai 2007 - juin 2007  les galeries lafayette vendeur  -  2005 - 2006  confection homme  job étudiant  brice vendeur  pap job -  2004 - 2004  étudiant  formation  niveau ii bac+3 bac+4 en développeur intégrateur web  ifocop  -  éragny  95   2015 - 2016   \""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "liste_cv_indeed = [del_line_feed(text).lower() for text in liste_cv_indeed]\n",
    "liste_cv_indeed[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Suppression ponctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"informaticien développement et réseaux  développeur intégrateur web  éragny  95   email me on indeed indeedcomrd7e8913ed00d0384  aujourd’hui je suis en recherche d'une opportunité sur un poste de développeur ou d’intégrateur web afin de monter toujours plus en compétence et d’approfondir les bases solide que j’ai acquis en formation  expérience  informaticien développement et réseaux  legrandcercle95    éragny  95    novembre 2016  juin 2017  informaticien de l'entreprise mes missions était de gérer les différent problème dans l'entreprise mise en place d'un antivirus serveur sauvegarde nas créé et gérer les droits sur l'active directory paramétrer des clients léger ainsi que du matériel informatique comme des imprimantes ou des étiqueteuse en ip fixe  réajustement du code html et css sur le site grand public selon les normes w3c création d'une source odbc création d'un code en php  sql afin de récupéré des données librairie sur un site fournisseur pour les enregistré en fiche xml création de bannières pour les différents évènements avec photoshop et formation d'une personne sur place au logiciel photoshop  charge de clientele  europcar france commercial    saintouenl'aumône  95    novembre 2008  novembre 2016  mes missions principales     qualité de service  assurer l'accueil des clients et le respect de la charte en agence   gestion des clients  traiter l'ensemble des appels téléphoniques analyser les besoins du client assurer le suivi de la clientèle   logistique et administratif  s'assurer de la disponibilité des véhicules gérer l'administratif de l'agence gestion de la caisse   polyvalence   formation développeur intégrateur web    septembre 2015  juin 2016  cif  centre de formation ifocop  8mois    création de différents sites  portfolio   présentation d'un site ecommerce créé de a à z pour l'examen  développeur web  stage   le club des formateurs    éragny  95    février 2016  mai 2016  développement d'un site sous wordpress de a à z  commercial stagiaire  lamy assurance   novembre 2007  décembre 2007  stagiaire  port marly accessoires    le portmarly  78    mai 2007  juin 2007  les galeries lafayette vendeur    2005  2006  confection homme  job étudiant  brice vendeur  pap job   2004  2004  étudiant  formation  niveau ii bac3 bac4 en développeur intégrateur web  ifocop    éragny  95   2015  2016   \""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#le maintient de la ponctuation pertube le stop words, apostrophe gérée dans text_treatment\n",
    "regex = re.compile('[%s]' % re.escape('!\"#$%&\\()*+,-./:;<=>?@[\\\\]^_{|}~')) \n",
    "\n",
    "def del_punct(s):  \n",
    "    \"\"\"Delete punctuation in the text\"\"\"\n",
    "    return regex.sub('', s)\n",
    "\n",
    "#test \n",
    "liste_cv_indeed_no_punc = [del_punct(text) for text in liste_cv_indeed]\n",
    "\n",
    "liste_cv_indeed_no_punc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Reconnaissance du langage du CV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _calculate_languages_ratios(text):\n",
    "    \"\"\"\n",
    "    Calculate probability of given text to be written in several languages and\n",
    "    return a dictionary that looks like {'french': 2, 'spanish': 4, 'english': 0}\n",
    "    \"\"\"\n",
    "\n",
    "    languages_ratios = {}\n",
    "\n",
    "    '''\n",
    "    nltk.wordpunct_tokenize() splits all punctuations into separate tokens\n",
    "    \n",
    "    >>> wordpunct_tokenize(\"That's thirty minutes away. I'll be there in ten.\")\n",
    "    ['That', \"'\", 's', 'thirty', 'minutes', 'away', '.', 'I', \"'\", 'll', 'be', 'there', 'in', 'ten', '.']\n",
    "    '''\n",
    "\n",
    "    tokens = wordpunct_tokenize(text)\n",
    "    words = [word.lower() for word in tokens] #from text get list of word in minuscule\n",
    "\n",
    "    \n",
    "    for language in stopwords.fileids(): # pour chaque langue\n",
    "        stopwords_set = set(stopwords.words(language)) #je mets les stop words du langage dans un set\n",
    "        words_set = set(words) #je mets les mots de mon texte dans un set\n",
    "        #je prends l'intersection entre les mots de mon texte et les mots du stopwords dans le langage donné\n",
    "        common_elements = words_set & stopwords_set\n",
    "        \n",
    "        #je compute mon score comme le nombre d'éléments en communs dictionnaire [langage : score]\n",
    "        languages_ratios[language] = len(common_elements) # language \"score\"\n",
    "\n",
    "    return languages_ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/mehdiregina/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['arabic',\n",
       " 'azerbaijani',\n",
       " 'danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'greek',\n",
       " 'hungarian',\n",
       " 'indonesian',\n",
       " 'italian',\n",
       " 'kazakh',\n",
       " 'nepali',\n",
       " 'norwegian',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'spanish',\n",
       " 'swedish',\n",
       " 'turkish']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "stopwords.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cv_langue(liste_cv, language,cv_names) :\n",
    "    \"\"\"Return resume witten in the specified language in parameter\"\"\"\n",
    "    liste_2 = []\n",
    "    french_cv_names = []\n",
    "    i=0\n",
    "    for cv in liste_cv:\n",
    "        if max(_calculate_languages_ratios(cv),key =_calculate_languages_ratios(cv).get)=='french':\n",
    "            liste_2.append(cv)\n",
    "            french_cv_names.append(cv_names[i])\n",
    "        i+=1\n",
    "    return liste_2,french_cv_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "liste_cv_indeed_fr, liste_files_fr = get_cv_langue(liste_cv_indeed_no_punc,'french',liste_files)\n",
    "nb_cv = len(liste_cv_indeed_no_punc)\n",
    "nb_cv_fr = len(liste_cv_indeed_fr)\n",
    "\n",
    "print(\"proportion cv french :\",1- ((nb_cv-nb_cv_fr)/nb_cv))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(liste_cv_indeed_fr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Preprocessing du text **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_treatment (text):\n",
    "    text = text.lower()\n",
    "    text = text.replace(\"\\x00\", '').replace(\"\\x01\", '').replace(\"\\x02\", '').replace(\"\\x03\", '') \\\n",
    "    .replace(\"\\x04\", '').replace(\"\\x05\", '').replace(\"\\x06\", '').replace(\"\\x07\", '').replace(\"\\x08\", '') \\\n",
    "    .replace(\"\\x0e\", '').replace(\"\\x11\", '').replace(\"\\x12\", '').replace(\"\\x10\", '').replace(\"\\x19\", '') \\\n",
    "    .replace(\"\\x1b\", '').replace(\"\\x14\", '').replace(\"\\x15\", '').replace('/', '').replace('=', '').replace(\"〓\", \"\") \\\n",
    "    .replace(\"»\", \"\").replace(\"«\", \"\").replace(\"¬\", \"\").replace('`', '').replace (\" -\", \"\").replace(\"•\", \"\")\\\n",
    "    .replace(\"l'\", \"\").replace(\"l’\", \"\").replace(\"l´\", \"\").replace(\"d’\", \"\").replace(\"d'\", \"\").replace(\"d´\",\"\")\\\n",
    "    .replace(\"j’\", \"\").replace(\"j'\", \"\").replace(\"j´\",\"\").replace(\"n’\", \"\").replace(\"n'\", \"\").replace(\"n´\",\"\")\\\n",
    "    .replace(\"”\", \"\").replace(\"~\", \"\").replace(\"§\", \"\").replace(\"¨\", \"\").replace(\"©\", \"\").replace(\"›\", \"\")\\\n",
    "    .replace(\"₋\", \"\").replace(\"→\", \"\").replace(\"⇨\", \"\").replace(\"∎\", \"\").replace(\"√\", \"\").replace(\"□\", \"\")\\\n",
    "    .replace(\"*\", \"\").replace(\"&\", \"\").replace(\"►\", \"\").replace(\"◊\", \"\").replace(\"☞\", \"\").replace(\"#\", \"\")\\\n",
    "    .replace(\"%\", \"\").replace(\"❖\", \"\").replace(\"➠\", \"\").replace(\"➢\", \"\").replace(\"\", \"\").replace(\"✓\", \"\") \\\n",
    "    .replace(\"√\", \"\").replace(\"✔\", \"\").replace(\"♦\", \"\").replace(\"◦\", \"\").replace(\"●\", \"\").replace(\"▫\", \"\")\\\n",
    "    .replace(\"▪\", \"\").replace(\"…\", \"\").replace(\"þ\", \"\").replace(\"®\", \"\").replace('', '').replace(\"...\", \"\")\n",
    "    text = unidecode.unidecode(text) # remove accent\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#On supprime les caractères étranges, accents et stop words\n",
    "liste_cv_indeed_treated = [text_treatment(text) for text in liste_cv_indeed_fr]\n",
    "#test\n",
    "liste_cv_indeed_treated[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Gestion des stop words **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate stopwords\n",
    "stop_words_py = set(stop_words.get_stop_words('french'))\n",
    "\n",
    "# attention certains stop words pourraient être utiles par la suite\n",
    "stopwords_set_manuel = set([\"an\", \"ans\", 'les', 'moins', 'd\\'un','janvier', 'fevrier', 'février', 'mars', 'avril', \\\n",
    "                 'mai', 'juin', 'juillet', 'aout', 'août', 'septembre', 'octobre', 'novembre', 'décembre', \\\n",
    "                  'decembre', 'moins', 'mise', 'universit\\xc3\\xa9', 'université', 'universite', 'ion','sage', \\\n",
    "                  'o', 'rac', 'vers', 'via', 'p\\xc3\\xa9rim\\xc3\\xa8tre', 'périmètre','et','paris','x',\"\\x00\",\\\n",
    "                          \"\\x01\",\"\\x02\", \"\\x03\",\"\\x04\",\"\\x05\",\"\\x06\",\"\\x07\",\"\\x08\",\"\\x09\",\"\\x0e\",\"\\x0e\",\"\\x11\",\\\n",
    "                           \"\\x12\",\"\\x13\",\"\\x14\",\"\\x15\",\"\\x16\",\"\\x17\",\"\\x18\",\"\\x19\",\"transport\",\"puis\",\"lieu\",\\\n",
    "                           \"adresse\",\"entre\",'dun','dune','chez','boulognebillancourt','bt','etc','recrutement','main',\\\n",
    "                           'and', 'paie','paiement','environ','place','france','paris','mois','mobile','mobiles',\\\n",
    "                           'nanterre','source','sources','concerne','concernant','of','non','notes','rh','minimum',\\\n",
    "                           'maximum','bac','site','sites','actuellement','telephone','telephonique','telephoniques','ca','demenager',\\\n",
    "                           'demenagement','participer','participation','lycee','baccalaureat','lien','liens','in',\\\n",
    "                           'indeed','email','indeedcomrd7e8913ed00d0384','aujourhui','afin','toujours','enterprise',\\\n",
    "                           \"guide\",\"10g\",\"11g\",\"9i\",'ad','v10','v2','v3','v5','v6','v8','v9','talan','talansolutions',\\\n",
    "                           \"aide\",\"ainsi\",'aix','aupres','autour','autres','b','bonne','campagnes','cas','chaine',\\\n",
    "                           'choix','coherence','departement','differentes','differents','divers','fin','for','grandes',\\\n",
    "                           'i','ii','jour','lies','lors','lu','mettre','necessaires','national','nationale','nouvelle',\\\n",
    "                           'nouvelles','parle','partir','partie','permettant','permettte','plusieurs','reel','selon',\\\n",
    "                           'temps','toutes','v'])\n",
    "stop_words_main = stop_words_py | stopwords_set_manuel\n",
    "stop_words_main = list(stop_words_main)\n",
    "print(\"taille stop words liste : \", len(stop_words_main))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#voir si utile\n",
    "def remove_stopwords(text,stopwords_list):\n",
    "    text_temp = \" \".join(text.split())+\" \"\n",
    "    for word in stopwords_list:\n",
    "        text_temp = text_temp.replace(\" \"+word+\" \", \" \")\n",
    "    return text_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test \n",
    "liste_cv_indeed_no_stop = [remove_stopwords(text,stop_words_main) for text in liste_cv_indeed_treated]\n",
    "liste_cv_indeed_no_stop[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#SnowballStemmer \n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"french\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_and_stem(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters \n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems\n",
    "\n",
    "\n",
    "def tokenize_only(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters \n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "totalvocab_indeed_stemmed = []\n",
    "totalvocab_indeed_tokenized = []\n",
    "for text in liste_cv_indeed_no_stop:\n",
    "    allwords_stemmed = tokenize_and_stem(text) #for each item in 'synopses', tokenize/stem\n",
    "    totalvocab_indeed_stemmed.extend(allwords_stemmed) #extend the 'totalvocab_stemmed' list\n",
    "    allwords_tokenized = tokenize_only(text)\n",
    "    totalvocab_indeed_tokenized.extend(allwords_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_frame_indeed = pd.DataFrame({'words': totalvocab_indeed_tokenized}, index = totalvocab_indeed_stemmed)\n",
    "print('there are ' + str(vocab_frame_indeed.shape[0]) + ' items in vocab_frame')\n",
    "vocab_frame_indeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_frame_indeed.loc['in']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HAC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag of word tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vect = TfidfVectorizer(stop_words=stop_words_main,max_df=0.8,min_df=0.05,\\\n",
    "                           preprocessor=text_treatment,tokenizer=tokenize_and_stem)\n",
    "bow_idf_indeed = tf_vect.fit_transform(liste_cv_indeed_no_stop)\n",
    "l2_norm = make_pipeline(Normalizer(copy=False))\n",
    "bow_idf_indeed = l2_norm.fit_transform(bow_idf_indeed)\n",
    "\n",
    "vocab_liste_indeed = tf_vect.get_feature_names()\n",
    "\n",
    "#Ajout une étape pour supprimer les doublons & shuffle\n",
    "buffer = pd.DataFrame(data=bow_idf_indeed.toarray())\n",
    "buffer = shuffle(buffer)\n",
    "buffer.drop_duplicates(inplace=True)\n",
    "\n",
    "new_idx = buffer.index\n",
    "bow_idf_indeed = buffer.values\n",
    "liste_files_new = []\n",
    "for idx in new_idx:\n",
    "    liste_files_new.append(liste_files_fr[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hac = AgglomerativeClustering(n_clusters=5, affinity='euclidean', compute_full_tree='auto', linkage='ward')\n",
    "hac.fit(bow_idf_indeed)\n",
    "\n",
    "hac.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(hac.labels_, columns=[\"Label\"]).groupby([\"Label\"])[\"Label\"].size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find topics behind each label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#select randomly n doc idx in each cluster\n",
    "#for each doc return n strongest tf-idf\n",
    "dict_rand_word_label = dict()\n",
    "for label in np.unique(hac.labels_):\n",
    "    idx_label = np.where(hac.labels_==label)[0]\n",
    "    idx_rand = np.random.choice(idx_label,size=(20))\n",
    "    liste_cluster_word = []\n",
    "    for idx in idx_rand:\n",
    "        text = \" \"\n",
    "        idx_ordered = np.argsort(bow_idf_indeed[idx])[::-1]\n",
    "        for j in range(0, 4):  # nombre de motss\n",
    "            text += vocab_frame_indeed.loc[vocab_liste_indeed[idx_ordered[j]]].values.tolist()[0][0] + \" \"\n",
    "        liste_cluster_word.append(text)\n",
    "    dict_rand_word_label[\"cluster\"+str(label)] = liste_cluster_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_core_word_label[\"cluster0\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ACP X HAC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimension reduction before applying HAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement LSA\n",
    "lsa_number = 170\n",
    "svd = TruncatedSVD(lsa_number)\n",
    "bow_idf_reduced_lsa = svd.fit_transform(bow_idf_indeed)\n",
    "explained_variance = svd.explained_variance_ratio_.sum()\n",
    "print(\"Explained variance of the SVD step: {}%\".format(int(explained_variance * 100)))\n",
    "\n",
    "hac = AgglomerativeClustering(n_clusters=5, affinity='euclidean', compute_full_tree='auto', linkage='ward')\n",
    "hac.fit(bow_idf_reduced_lsa)\n",
    "labels_hac_lsa = hac.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(hac.labels_, columns=[\"Label\"]).groupby([\"Label\"])[\"Label\"].size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find topic behind each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict_rand_word_label = dict()\n",
    "for label in np.unique(labels_hac_lsa):\n",
    "    idx_label = np.where(labels_hac_lsa==label)[0]\n",
    "    idx_rand = np.random.choice(idx_label,size=(20))\n",
    "    liste_cluster_word = []\n",
    "    for idx in idx_rand:\n",
    "        text = \" \"\n",
    "        idx_ordered = np.argsort(bow_idf_indeed[idx])[::-1]\n",
    "        for j in range(0, 4):  # nombre de motss\n",
    "            text += vocab_frame_indeed.loc[vocab_liste_indeed[idx_ordered[j]]].values.tolist()[0][0] + \" \"\n",
    "        liste_cluster_word.append(text)\n",
    "    dict_rand_word_label[\"cluster\"+str(label)] = liste_cluster_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_rand_word_label[\"cluster1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CV TALAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download talan CVs \n",
    "liste_cv_talan = []\n",
    "liste_files_talan = []\n",
    "path = '../Data_talan/txt/'\n",
    "filenames = sorted(glob(os.path.join(path,\"*.txt\")))\n",
    "print(len(filenames))\n",
    "for file in filenames:\n",
    "    liste_cv_talan.append(open(file).read())\n",
    "    liste_files_talan.append(file.split('/')[-1].split('.')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liste_cv_talan[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#suppression des saut de lignes\n",
    "liste_cv_talan = [del_line_feed(text).lower() for text in liste_cv_talan]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#suppression de la ponctuation\n",
    "liste_cv_talan_no_punc = [del_punct(text) for text in liste_cv_talan]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selectionner seulement cvs fr\n",
    "liste_cv_talan_fr, liste_files_talan_fr = get_cv_langue(liste_cv_talan_no_punc,'french',liste_files_talan)\n",
    "\n",
    "nb_cv = len(liste_cv_talan_no_punc)\n",
    "nb_cv_fr = len(liste_cv_talan_fr)\n",
    "\n",
    "print(\"proportion cv french :\",1- ((nb_cv-nb_cv_fr)/nb_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#On supprime les caractères étranges, accents et stop words\n",
    "liste_cv_treated_talan = [text_treatment(text) for text in liste_cv_talan_fr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#remove stop word\n",
    "liste_cv_talan_no_stop = [remove_stopwords(text,stop_words_main) for text in liste_cv_treated_talan]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#facultatif add only for talan cv (delete numbers) -> could be use for the preprocessing in general !\n",
    "liste_cv_talan_clean = [re.sub('[0-9 ]+', ' ', text) for text in liste_cv_talan_no_stop]\n",
    "liste_cv_talan_clean[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalvocab_stemmed_talan = []\n",
    "totalvocab_tokenized_talan = []\n",
    "for text in liste_cv_talan_no_stop:\n",
    "    allwords_stemmed = tokenize_and_stem(text) #for each item in 'synopses', tokenize/stem\n",
    "    totalvocab_stemmed_talan.extend(allwords_stemmed) #extend the 'totalvocab_stemmed' list\n",
    "    allwords_tokenized = tokenize_only(text)\n",
    "    totalvocab_tokenized_talan.extend(allwords_tokenized)\n",
    "\n",
    "vocab_frame_talan = pd.DataFrame({'words': totalvocab_tokenized_talan}, index = totalvocab_stemmed_talan)\n",
    "print('there are ' + str(vocab_frame_talan.shape[0]) + ' items in vocab_frame')\n",
    "vocab_frame_talan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HAC Talan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Representation bag of word tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF IDF BOW Representation\n",
    "tf_vect = TfidfVectorizer(stop_words=stop_words_main,max_df=0.7,min_df=0.1,\\\n",
    "                           preprocessor=text_treatment,tokenizer=tokenize_and_stem)\n",
    "bow_idf_talan = tf_vect.fit_transform(liste_cv_talan_clean)\n",
    "\n",
    "\n",
    "vocab_liste_talan = tf_vect.get_feature_names()\n",
    "\n",
    "#Ajout une étape pour supprimer les doublons & shuffle\n",
    "buffer = pd.DataFrame(data=bow_idf_talan.toarray())\n",
    "buffer = shuffle(buffer)\n",
    "buffer.drop_duplicates(inplace=True)\n",
    "\n",
    "new_idx = buffer.index\n",
    "bow_idf_talan = buffer.values\n",
    "liste_files_new = []\n",
    "liste_cv_clean_2 = []\n",
    "for idx in new_idx:\n",
    "    liste_files_new.append(liste_files_talan_fr[idx])\n",
    "    liste_cv_clean_2.append(liste_cv_talan_clean[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hac = AgglomerativeClustering(n_clusters=5, affinity='euclidean', compute_full_tree='auto', linkage='ward')\n",
    "hac.fit(bow_idf_talan)\n",
    "\n",
    "pd.DataFrame(hac.labels_,columns=[\"Label\"]).groupby([\"Label\"])[\"Label\"].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict_rand_word_label = dict()\n",
    "for label in np.unique(hac.labels_):\n",
    "    idx_label = np.where(hac.labels_==label)[0]\n",
    "    idx_rand = np.random.choice(idx_label,size=(12))\n",
    "    liste_cluster_word = []\n",
    "    for idx in idx_rand:\n",
    "        text = \" \"\n",
    "        idx_ordered = np.argsort(bow_idf_talan[idx])[::-1]\n",
    "        for j in range(0, 4):  # nombre de motss\n",
    "            text += vocab_frame_talan.loc[vocab_liste_talan[idx_ordered[j]]].values.tolist()[0][0] + \" \"\n",
    "        liste_cluster_word.append(text)\n",
    "    dict_rand_word_label[\"cluster\"+str(label)] = liste_cluster_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_rand_word_label[\"cluster1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ACP X HAC TALAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement LSA\n",
    "lsa_number = 60\n",
    "svd = TruncatedSVD(lsa_number)\n",
    "bow_idf_reduced_talan = svd.fit_transform(bow_idf_talan)\n",
    "explained_variance = svd.explained_variance_ratio_.sum()\n",
    "print(\"Explained variance of the SVD step: {}%\".format(int(explained_variance * 100)))\n",
    "\n",
    "hac = AgglomerativeClustering(n_clusters=5, affinity='euclidean', compute_full_tree='auto', linkage='ward')\n",
    "hac.fit(bow_idf_reduced_talan)\n",
    "labels_hac_lsa = hac.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(labels_hac_lsa, columns=[\"Label\"]).groupby([\"Label\"])[\"Label\"].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict_rand_word_label = dict()\n",
    "for label in np.unique(labels_hac_lsa):\n",
    "    idx_label = np.where(labels_hac_lsa==label)[0]\n",
    "    idx_rand = np.random.choice(idx_label,size=(12))\n",
    "    liste_cluster_word = []\n",
    "    for idx in idx_rand:\n",
    "        text = \" \"\n",
    "        idx_ordered = np.argsort(bow_idf_talan[idx])[::-1]\n",
    "        for j in range(0, 4):  # nombre de motss\n",
    "            text += vocab_frame_talan.loc[vocab_liste_talan[idx_ordered[j]]].values.tolist()[0][0] + \" \"\n",
    "        liste_cluster_word.append(text)\n",
    "    dict_rand_word_label[\"cluster\"+str(label)] = liste_cluster_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_rand_word_label[\"cluster3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_label = np.where(labels_hac_lsa==3)[0]\n",
    "idx_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liste_cv_clean_2[99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
